(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$ ssh ignacys@recs.man.poznan.pl
Last login: Thu Sep 23 15:43:35 2021 from 83.8.24.17.ipv4.supernova.orange.pl
===============================================================================
24.01.2016

                                 IMPORTANT
Due to disk limitations quota has been introduced to all users. Soft quota 80G
for one day, hard quota 100G.

==============================================================================
04.06.2019

Core allocation policy has changed. Users should specify required number of
cores with --cpus-per-task option.

===============================================================================
18.02.2020

In order to log in into openshift or grafana, user should:

    1. Set an ldap password for their account. In order to do that, one should
       run "request_ldap_passwd_reset" and follow the instructions.
    2. Then send an email to marqs@man.poznan.pl and mzdunek@man.poznan.pl
        specifying a group you want to be added to (e.g. openshift or grafana).


Aby zalogować się do usługi openshift lub grafana należy ustawić hasło do konta
w ldapie. W tym celu należy:

    1. Wywołać skrypt "request_ldap_passwd_reset", a następnie postępować
       zgodnie z poleceniami.
    2. Po ustawieniu hasła należy napisać emaila do marqs@man.poznan.pl oraz
       mzdunek@man.poznan.pl, w którym należy podać grupę, do której chce się
       być dodanym (np. openshift lub grafana).

===============================================================================
squeue(base) [ignacys@ui ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             29789       all     semi kubaraps  R      59:59      1 xeon-e-p40
             29790       all     semi kubaraps  R      59:55      1 xeon-e-v100
(base) [ignacys@ui ~]$ salloc -N1 -n1 -p all -w xeon-10
salloc: Granted job allocation 29791
srun: Step created for job 29791
(base) [ignacys@xeon-10 ~]$ sinfo --Node -o "%24N %.6D %10P %.11T %.4c %.8z %.6m %.8d %.6w %.8f %20E %G"
NODELIST                  NODES PARTITION        STATE CPUS    S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON               GRES
i5-0-02                       1 testing          down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-02                       1 standard*        down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-02                       1 long             down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-02                       1 all              down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-03                       1 testing          down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-03                       1 standard*        down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-03                       1 long             down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-03                       1 all              down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-04                       1 testing          down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-04                       1 standard*        down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-04                       1 long             down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i5-0-04                       1 all              down*    4    1:2:2      1        0      1       i5 Not responding       gpu:0
i7-k40-01                     1 testing           idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-01                     1 standard*         idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-01                     1 long              idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-01                     1 all               idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-02                     1 testing           idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-02                     1 standard*         idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-02                     1 long              idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
i7-k40-02                     1 all               idle    8    1:4:2      1        0      1   i7_k40 none                 gpu:1
taishan-2180-01               1 standard*        down*   32   32:1:1      1        0      1    arm64 Not responding       (null)
taishan-2180-01               1 testing          down*   32   32:1:1      1        0      1    arm64 Not responding       (null)
taishan-2180-01               1 long             down*   32   32:1:1      1        0      1    arm64 Not responding       (null)
taishan-2180-01               1 all              down*   32   32:1:1      1        0      1    arm64 Not responding       (null)
taishan-2280-01               1 standard*        down*   64   64:1:1      1        0      1    arm64 shutdown             (null)
taishan-2280-01               1 testing          down*   64   64:1:1      1        0      1    arm64 shutdown             (null)
taishan-2280-01               1 long             down*   64   64:1:1      1        0      1    arm64 shutdown             (null)
taishan-2280-01               1 all              down*   64   64:1:1      1        0      1    arm64 shutdown             (null)
xeon-09                       1 xeon              idle   16    2:8:2      1        0      1     v100 none                 gpu:2
xeon-09                       1 standard*         idle   16    2:8:2      1        0      1     v100 none                 gpu:2
xeon-09                       1 testing           idle   16    2:8:2      1        0      1     v100 none                 gpu:2
xeon-09                       1 all               idle   16    2:8:2      1        0      1     v100 none                 gpu:2
xeon-10                       1 standard*        mixed   16    2:8:2      1        0      1 xeon_K80 none                 gpu:2
xeon-10                       1 testing          mixed   16    2:8:2      1        0      1 xeon_K80 none                 gpu:2
xeon-10                       1 xeon             mixed   16    2:8:2      1        0      1 xeon_K80 none                 gpu:2
xeon-10                       1 long             mixed   16    2:8:2      1        0      1 xeon_K80 none                 gpu:2
xeon-10                       1 all              mixed   16    2:8:2      1        0      1 xeon_K80 none                 gpu:2
xeon-12                       1 xeon              idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-12                       1 all               idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-13                       1 xeon              idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-13                       1 all               idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-14                       1 xeon              idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-14                       1 all               idle   32    2:8:2      1        0      1     xeon none                 gpu:0
xeon-d-01                     1 standard*         idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-01                     1 testing           idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-01                     1 long              idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-01                     1 all               idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-02                     1 standard*         idle   32   1:16:2      1        0      1   xeon-d none                 gpu:0
xeon-d-02                     1 testing           idle   32   1:16:2      1        0      1   xeon-d none                 gpu:0
xeon-d-02                     1 long              idle   32   1:16:2      1        0      1   xeon-d none                 gpu:0
xeon-d-02                     1 all               idle   32   1:16:2      1        0      1   xeon-d none                 gpu:0
xeon-d-03                     1 standard*        down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-03                     1 testing          down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-03                     1 long             down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-03                     1 all              down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-04                     1 standard*        down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-04                     1 testing          down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-04                     1 long             down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-04                     1 all              down*   32   1:16:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-05                     1 standard*        down*   16    1:8:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-05                     1 testing          down*   16    1:8:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-05                     1 long             down*   16    1:8:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-05                     1 all              down*   16    1:8:2      1        0      1   xeon-d Not responding       gpu:0
xeon-d-06                     1 standard*         idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-06                     1 testing           idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-06                     1 long              idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-d-06                     1 all               idle   16    1:8:2      1        0      1   xeon-d none                 gpu:0
xeon-e-p4                     1 standard*         idle    8    1:4:2      1        0      1  xeon_p4 none                 gpu:1
xeon-e-p4                     1 testing           idle    8    1:4:2      1        0      1  xeon_p4 none                 gpu:1
xeon-e-p4                     1 long              idle    8    1:4:2      1        0      1  xeon_p4 none                 gpu:1
xeon-e-p4                     1 all               idle    8    1:4:2      1        0      1  xeon_p4 none                 gpu:1
xeon-e-p40                    1 standard*    allocated    8    1:4:2      1        0      1 xeon_p40 none                 gpu:1
xeon-e-p40                    1 testing      allocated    8    1:4:2      1        0      1 xeon_p40 none                 gpu:1
xeon-e-p40                    1 long         allocated    8    1:4:2      1        0      1 xeon_p40 none                 gpu:1
xeon-e-p40                    1 all          allocated    8    1:4:2      1        0      1 xeon_p40 none                 gpu:1
xeon-e-v100                   1 standard*    allocated    8    1:4:2      1        0      1     v100 none                 gpu:1
xeon-e-v100                   1 testing      allocated    8    1:4:2      1        0      1     v100 none                 gpu:1
xeon-e-v100                   1 long         allocated    8    1:4:2      1        0      1     v100 none                 gpu:1
xeon-e-v100                   1 all          allocated    8    1:4:2      1        0      1     v100 none                 gpu:1
zynq7045-01                   1 standard*        down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-01                   1 testing          down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-01                   1 long             down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-01                   1 all              down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-02                   1 standard*        down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-02                   1 testing          down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-02                   1 long             down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
zynq7045-02                   1 all              down*    2    1:2:1      1        0      1 zynqfpga Not responding       gpu:0
(base) [ignacys@xeon-10 ~]$ cd segmentation
(base) [ignacys@xeon-10 segmentation]$ ls
benchmark.csv      experiment_controller.py  mlruns         __pycache__       runs.json       test_gpu.py   utils.py
data_generator.py  jaccard_eval.py           npy_datasets   runs_config.json  simplified.csv  train_gpu.py
data_loader.py     logs                      preprocessing  runs.csv          src             UNet.py
(base) [ignacys@xeon-10 segmentation]$ conda activate tfenv
(tfenv) [ignacys@xeon-10 segmentation]$ python experiment_controller.py
2021-09-24 12:36:40.477787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1

-------------------------
RUN: Baseline - lesion, PARAMS: {}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 12:36:47.699221: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 12:36:47.701424: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-09-24 12:36:47.744979: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 2 GPUs
2021-09-24 12:36:47.746182: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory
2021-09-24 12:36:47.746831: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory
2021-09-24 12:36:47.746866: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 12:36:47.755727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:83:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.92GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:36:47.756620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:36:47.756658: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:36:48.140271: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:36:48.533562: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:36:49.105290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:36:49.428883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:36:49.677355: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:36:50.364366: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:36:50.368093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-09-24 12:36:50.368503: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-24 12:36:50.377050: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2594125000 Hz
2021-09-24 12:36:50.377183: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55788be49a10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-24 12:36:50.377208: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-09-24 12:36:50.472652: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55788be5d4f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-24 12:36:50.472695: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2021-09-24 12:36:50.472706: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7
2021-09-24 12:36:50.474027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:83:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.92GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:36:50.474865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:36:50.474910: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:36:50.474949: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:36:50.474977: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:36:50.475004: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:36:50.475031: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:36:50.475057: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:36:50.475085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:36:50.478360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-09-24 12:36:50.478403: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:36:53.283836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-24 12:36:53.284605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1
2021-09-24 12:36:53.284621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N Y
2021-09-24 12:36:53.284631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   Y N
2021-09-24 12:36:53.287578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11345 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:83:00.0, compute capability: 3.7)
2021-09-24 12:36:53.288822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10626 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:84:00.0, compute capability: 3.7)
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           conv2d_9[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 768)  0           up_sampling2d[0][0]
                                                                 conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_10[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_1[0][0]
                                                                 conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_12[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_2[0][0]
                                                                 conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 96) 0           up_sampling2d_3[0][0]
                                                                 conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 1)  33          conv2d_17[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
2021-09-24 12:37:04.533406: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:37:08.211835: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256
2021-09-24 12:37:08.378318: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output:
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-24 12:37:10.281429: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
  1/254 [..............................] - ETA: 0s - loss: 0.6894 - acc: 0.8151 - precision: 0.0818 - auc: 0.7173 - recall: 0.01222021-09-24 12:38:35.832886: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 12:38:35.833410: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.stop` instead.
2021-09-24 12:38:36.315966: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 12:38:36.325265: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36
2021-09-24 12:38:36.329171: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.trace.json.gz
2021-09-24 12:38:36.353254: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36
2021-09-24 12:38:36.364667: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.memory_profile.json.gz
2021-09-24 12:38:36.383301: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36Dumped tool data for xplane.pb to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-123647/train/plugins/profile/2021_09_24_12_38_36/xeon-10.kernel_stats.pb

 75/254 [=======>......................] - ETA: 1:07 - loss: 0.5617 - acc: 0.7817 - precision: 0.6046 - auc: 0.6366 - recall: 0.0021^CTraceback (most recent call last):
  File "experiment_controller.py", line 57, in <module>
    trainer.train()
  File "/home/ignacys/segmentation/train_gpu.py", line 282, in train
    self.model.fit(
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 523, in safe_patch_function
    patch_function.call(call_original, *args, **kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 156, in call
    return cls().__call__(original, *args, **kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 167, in __call__
    raise e
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 160, in __call__
    return self._patch_implementation(original, *args, **kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 218, in _patch_implementation
    result = super(PatchWithManagedRun, self)._patch_implementation(
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/tensorflow.py", line 1153, in _patch_implementation
    history = original(inst, *args, **kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py", line 481, in call_original
    original_result = original(*og_args, **og_kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit
    tmp_logs = train_function(iterator)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__
    result = self._call(*args, **kwds)
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call
    return self._call_flat(
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call
    outputs = execute.execute(
  File "/home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
KeyboardInterrupt
2021-09-24 12:39:04.563578: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
         [[{{node PyFunc}}]]
^C
(tfenv) [ignacys@xeon-10 segmentation]$ ls
benchmark.csv      experiment_controller.py  mlruns         __pycache__       runs.json       test_gpu.py   utils.py
data_generator.py  jaccard_eval.py           npy_datasets   runs_config.json  simplified.csv  train_gpu.py
data_loader.py     logs                      preprocessing  runs.csv          src             UNet.py
(tfenv) [ignacys@xeon-10 segmentation]$ python test_gpu.py
2021-09-24 12:39:16.204762: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:39:18.624335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-09-24 12:39:18.666957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:83:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.92GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:39:18.667850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:84:00.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2021-09-24 12:39:18.667886: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:39:18.670734: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:39:18.673260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:39:18.673828: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:39:18.676573: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:39:18.678032: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:39:18.683386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:39:18.686874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
Num GPUs Available:  2
(tfenv) [ignacys@xeon-10 segmentation]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
             29789       all     semi kubaraps  R    1:04:29      1 xeon-e-p40
             29790       all     semi kubaraps  R    1:04:25      1 xeon-e-v100
             29791       all       sh  ignacys  R       3:34      1 xeon-10
(tfenv) [ignacys@xeon-10 segmentation]$ scancel 29791
salloc: Job allocation 29791 has been revoked.
(tfenv) [ignacys@xeon-10 segmentation]$ srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: xeon-10: task 0: Killed
(base) [ignacys@ui ~]$ salloc -N1 -n1 -p all -w xeon-09
salloc: Granted job allocation 29792
srun: Step created for job 29792
(base) [ignacys@xeon-09 ~]$ conda activate tfenv
(tfenv) [ignacys@xeon-09 ~]$ cd segmentation
(tfenv) [ignacys@xeon-09 segmentation]$ ls
benchmark.csv      experiment_controller.py  mlruns         __pycache__       runs.json       test_gpu.py   utils.py
data_generator.py  jaccard_eval.py           npy_datasets   runs_config.json  simplified.csv  train_gpu.py
data_loader.py     logs                      preprocessing  runs.csv          src             UNet.py
(tfenv) [ignacys@xeon-09 segmentation]$ python test_gpu.py
2021-09-24 12:40:27.002512: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:40:30.935667: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-09-24 12:40:31.023323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:04:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:40:31.025149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:81:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:40:31.025185: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:40:31.396485: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:40:31.770702: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:40:32.339432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:40:32.683847: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:40:32.904855: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:40:33.587706: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:40:33.595138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
Num GPUs Available:  2
(tfenv) [ignacys@xeon-09 segmentation]$ salloc -N1 -n1 -p all -w xeon-09
salloc: Granted job allocation 29793
srun: Step created for job 29793
(base) [ignacys@xeon-09 segmentation]$ python experiment_controller.py
Traceback (most recent call last):
  File "experiment_controller.py", line 2, in <module>
    from train_gpu import Trainer
  File "/home/ignacys/segmentation/train_gpu.py", line 2, in <module>
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'
(base) [ignacys@xeon-09 segmentation]$ conda activate tfenv
(tfenv) [ignacys@xeon-09 segmentation]$ python experiment_controller.py
2021-09-24 12:41:25.486135: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1

-------------------------
RUN: Baseline - lesion, PARAMS: {}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 12:41:29.303708: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 12:41:29.304911: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-09-24 12:41:29.359932: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 2 GPUs
2021-09-24 12:41:29.360796: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory
2021-09-24 12:41:29.361367: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcupti.so'; dlerror: libcupti.so: cannot open shared object file: No such file or directory
2021-09-24 12:41:29.361401: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 12:41:29.370595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:04:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:41:29.372427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:81:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:41:29.372463: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:41:29.375281: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:41:29.377901: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:41:29.378479: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:41:29.381290: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:41:29.382869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:41:29.388335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:41:29.395536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-09-24 12:41:29.395914: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-24 12:41:29.405217: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2593915000 Hz
2021-09-24 12:41:29.405360: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bc785ce5e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-09-24 12:41:29.405386: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-09-24 12:41:29.604712: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55bc785e20c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-09-24 12:41:29.604756: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2021-09-24 12:41:29.604768: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2021-09-24 12:41:29.610213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:04:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:41:29.612002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties:
pciBusID: 0000:81:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 836.37GiB/s
2021-09-24 12:41:29.612047: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:41:29.612085: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-09-24 12:41:29.612113: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-09-24 12:41:29.612140: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-09-24 12:41:29.612168: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-09-24 12:41:29.612196: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-09-24 12:41:29.612224: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:41:29.625814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0, 1
2021-09-24 12:41:29.625865: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-09-24 12:41:32.366090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-09-24 12:41:32.366157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1
2021-09-24 12:41:32.366171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N
2021-09-24 12:41:32.366181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N
2021-09-24 12:41:32.372003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30132 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:04:00.0, compute capability: 7.0)
2021-09-24 12:41:32.374220: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30132 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:81:00.0, compute capability: 7.0)
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 256, 256, 32) 320         input_1[0][0]
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 256, 256, 32) 9248        conv2d[0][0]
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 128, 128, 64) 18496       max_pooling2d[0][0]
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 128, 128, 64) 36928       conv2d_2[0][0]
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 64, 64, 128)  147584      conv2d_4[0][0]
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 32, 32, 256)  590080      conv2d_6[0][0]
__________________________________________________________________________________________________
max_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 16, 16, 512)  2359808     conv2d_8[0][0]
__________________________________________________________________________________________________
up_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           conv2d_9[0][0]
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 32, 32, 768)  0           up_sampling2d[0][0]
                                                                 conv2d_7[0][0]
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate[0][0]
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_10[0][0]
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_11[0][0]
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_1[0][0]
                                                                 conv2d_5[0][0]
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_1[0][0]
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_12[0][0]
__________________________________________________________________________________________________
up_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_13[0][0]
__________________________________________________________________________________________________
concatenate_2 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_2[0][0]
                                                                 conv2d_3[0][0]
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_2[0][0]
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_14[0][0]
__________________________________________________________________________________________________
up_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_15[0][0]
__________________________________________________________________________________________________
concatenate_3 (Concatenate)     (None, 256, 256, 96) 0           up_sampling2d_3[0][0]
                                                                 conv2d_1[0][0]
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_3[0][0]
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_16[0][0]
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 256, 256, 1)  33          conv2d_17[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
2021-09-24 12:41:43.783093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-09-24 12:41:48.225950: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256
2021-09-24 12:41:48.414069: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output:
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-09-24 12:41:50.512673: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
  1/254 [..............................] - ETA: 0s - loss: 0.6894 - acc: 0.8151 - precision: 0.0818 - auc: 0.7173 - recall: 0.01222021-09-24 12:43:23.305858: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 12:43:23.306458: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.stop` instead.
2021-09-24 12:43:23.524646: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 12:43:23.539296: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23
2021-09-24 12:43:23.542800: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.trace.json.gz
2021-09-24 12:43:23.568144: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23
2021-09-24 12:43:23.579709: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.memory_profile.json.gz
2021-09-24 12:43:23.601764: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23Dumped tool data for xplane.pb to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-124129/train/plugins/profile/2021_09_24_12_43_23/xeon-09.kernel_stats.pb

254/254 [==============================] - 188s 740ms/step - loss: 0.4598 - acc: 0.8081 - precision: 0.7592 - auc: 0.7610 - recall: 0.1661 - val_loss: 0.4603 - val_acc: 0.7876 - val_precision: 0.0000e+00 - val_auc: 0.8141 - val_recall: 0.0000e+00
Epoch 2/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3637 - acc: 0.8522 - precision: 0.8108 - auc: 0.8502 - recall: 0.4139 - val_loss: 0.3388 - val_acc: 0.8630 - val_precision: 0.9515 - val_auc: 0.8731 - val_recall: 0.3729
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3452 - acc: 0.8604 - precision: 0.8042 - auc: 0.8632 - recall: 0.4694 - val_loss: 0.3515 - val_acc: 0.8513 - val_precision: 0.9632 - val_auc: 0.8700 - val_recall: 0.3172
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3353 - acc: 0.8664 - precision: 0.8118 - auc: 0.8704 - recall: 0.4984 - val_loss: 0.3023 - val_acc: 0.8799 - val_precision: 0.8989 - val_auc: 0.8896 - val_recall: 0.4859
Epoch 5/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3312 - acc: 0.8695 - precision: 0.8250 - auc: 0.8719 - recall: 0.5040 - val_loss: 0.3069 - val_acc: 0.8766 - val_precision: 0.8225 - val_auc: 0.8860 - val_recall: 0.5105
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3140 - acc: 0.8780 - precision: 0.8236 - auc: 0.8839 - recall: 0.5554 - val_loss: 0.2809 - val_acc: 0.8934 - val_precision: 0.8611 - val_auc: 0.9056 - val_recall: 0.6060
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2905 - acc: 0.8904 - precision: 0.8353 - auc: 0.8987 - recall: 0.6152 - val_loss: 0.2617 - val_acc: 0.9047 - val_precision: 0.9162 - val_auc: 0.9113 - val_recall: 0.5996
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2593 - acc: 0.9030 - precision: 0.8497 - auc: 0.9204 - recall: 0.6707 - val_loss: 0.2274 - val_acc: 0.9146 - val_precision: 0.8601 - val_auc: 0.9413 - val_recall: 0.7291
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2357 - acc: 0.9108 - precision: 0.8515 - auc: 0.9369 - recall: 0.7123 - val_loss: 0.2678 - val_acc: 0.8954 - val_precision: 0.9735 - val_auc: 0.9314 - val_recall: 0.5137
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2254 - acc: 0.9147 - precision: 0.8606 - auc: 0.9431 - recall: 0.7232 - val_loss: 0.2023 - val_acc: 0.9217 - val_precision: 0.9114 - val_auc: 0.9554 - val_recall: 0.7016
Epoch 11/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2069 - acc: 0.9202 - precision: 0.8641 - auc: 0.9532 - recall: 0.7492 - val_loss: 0.1768 - val_acc: 0.9297 - val_precision: 0.8675 - val_auc: 0.9658 - val_recall: 0.7875
Epoch 12/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1923 - acc: 0.9258 - precision: 0.8703 - auc: 0.9597 - recall: 0.7723 - val_loss: 0.1628 - val_acc: 0.9385 - val_precision: 0.8649 - val_auc: 0.9727 - val_recall: 0.8397
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1779 - acc: 0.9307 - precision: 0.8778 - auc: 0.9663 - recall: 0.7897 - val_loss: 0.1520 - val_acc: 0.9402 - val_precision: 0.8662 - val_auc: 0.9770 - val_recall: 0.8530
Epoch 14/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1739 - acc: 0.9318 - precision: 0.8784 - auc: 0.9674 - recall: 0.7952 - val_loss: 0.1611 - val_acc: 0.9375 - val_precision: 0.9077 - val_auc: 0.9697 - val_recall: 0.7760
Epoch 15/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1737 - acc: 0.9322 - precision: 0.8843 - auc: 0.9682 - recall: 0.7904 - val_loss: 0.1668 - val_acc: 0.9357 - val_precision: 0.8560 - val_auc: 0.9742 - val_recall: 0.8550
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1582 - acc: 0.9365 - precision: 0.8826 - auc: 0.9742 - recall: 0.8153 - val_loss: 0.1391 - val_acc: 0.9458 - val_precision: 0.9183 - val_auc: 0.9761 - val_recall: 0.7975
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1668 - acc: 0.9351 - precision: 0.8842 - auc: 0.9707 - recall: 0.8060 - val_loss: 0.1488 - val_acc: 0.9409 - val_precision: 0.8861 - val_auc: 0.9765 - val_recall: 0.8392
Epoch 18/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1589 - acc: 0.9379 - precision: 0.8865 - auc: 0.9735 - recall: 0.8177 - val_loss: 0.1372 - val_acc: 0.9490 - val_precision: 0.9320 - val_auc: 0.9785 - val_recall: 0.8125
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1543 - acc: 0.9392 - precision: 0.8855 - auc: 0.9756 - recall: 0.8260 - val_loss: 0.1456 - val_acc: 0.9421 - val_precision: 0.9415 - val_auc: 0.9792 - val_recall: 0.7857
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1454 - acc: 0.9429 - precision: 0.8955 - auc: 0.9783 - recall: 0.8335 - val_loss: 0.1259 - val_acc: 0.9527 - val_precision: 0.8994 - val_auc: 0.9792 - val_recall: 0.8476
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1482 - acc: 0.9417 - precision: 0.8911 - auc: 0.9772 - recall: 0.8322 - val_loss: 0.1366 - val_acc: 0.9455 - val_precision: 0.8799 - val_auc: 0.9831 - val_recall: 0.8865
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1374 - acc: 0.9453 - precision: 0.8992 - auc: 0.9803 - recall: 0.8418 - val_loss: 0.1420 - val_acc: 0.9449 - val_precision: 0.8563 - val_auc: 0.9793 - val_recall: 0.8758
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1371 - acc: 0.9454 - precision: 0.9002 - auc: 0.9806 - recall: 0.8411 - val_loss: 0.1341 - val_acc: 0.9469 - val_precision: 0.8705 - val_auc: 0.9820 - val_recall: 0.8795
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1296 - acc: 0.9486 - precision: 0.9043 - auc: 0.9827 - recall: 0.8529 - val_loss: 0.1448 - val_acc: 0.9427 - val_precision: 0.8448 - val_auc: 0.9802 - val_recall: 0.8999
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1241 - acc: 0.9505 - precision: 0.9032 - auc: 0.9841 - recall: 0.8641 - val_loss: 0.1375 - val_acc: 0.9453 - val_precision: 0.8415 - val_auc: 0.9841 - val_recall: 0.9076
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1232 - acc: 0.9502 - precision: 0.9040 - auc: 0.9844 - recall: 0.8614 - val_loss: 0.1411 - val_acc: 0.9473 - val_precision: 0.9218 - val_auc: 0.9771 - val_recall: 0.8192
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1246 - acc: 0.9507 - precision: 0.9083 - auc: 0.9835 - recall: 0.8588 - val_loss: 0.1304 - val_acc: 0.9479 - val_precision: 0.8783 - val_auc: 0.9820 - val_recall: 0.8765
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1135 - acc: 0.9545 - precision: 0.9113 - auc: 0.9867 - recall: 0.8751 - val_loss: 0.1609 - val_acc: 0.9367 - val_precision: 0.8099 - val_auc: 0.9793 - val_recall: 0.9174
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1161 - acc: 0.9544 - precision: 0.9109 - auc: 0.9860 - recall: 0.8747 - val_loss: 0.1492 - val_acc: 0.9411 - val_precision: 0.8614 - val_auc: 0.9778 - val_recall: 0.8807
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1028 - acc: 0.9589 - precision: 0.9199 - auc: 0.9891 - recall: 0.8872 - val_loss: 0.1279 - val_acc: 0.9501 - val_precision: 0.9076 - val_auc: 0.9815 - val_recall: 0.8509
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1011 - acc: 0.9592 - precision: 0.9201 - auc: 0.9895 - recall: 0.8887 - val_loss: 0.1576 - val_acc: 0.9333 - val_precision: 0.7968 - val_auc: 0.9794 - val_recall: 0.9156
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0957 - acc: 0.9618 - precision: 0.9229 - auc: 0.9902 - recall: 0.8988 - val_loss: 0.1341 - val_acc: 0.9504 - val_precision: 0.8993 - val_auc: 0.9790 - val_recall: 0.8648
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0898 - acc: 0.9636 - precision: 0.9258 - auc: 0.9917 - recall: 0.9044 - val_loss: 0.1357 - val_acc: 0.9514 - val_precision: 0.9158 - val_auc: 0.9786 - val_recall: 0.8477
Epoch 34/100
254/254 [==============================] - ETA: 0s - loss: 0.0793 - acc: 0.9676 - precision: 0.9343 - auc: 0.9935 - recall: 0.9146Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0793 - acc: 0.9676 - precision: 0.9343 - auc: 0.9935 - recall: 0.9146 - val_loss: 0.1586 - val_acc: 0.9472 - val_precision: 0.8734 - val_auc: 0.9703 - val_recall: 0.8547
Epoch 00034: early stopping
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2021-09-24 13:16:42.506698: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /home/ignacys/anaconda3/envs/tfenv/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
36/36 [==============================] - 42s 1s/step - loss: 0.1472 - acc: 0.9455 - precision: 0.9090 - auc: 0.9780 - recall: 0.8402
---------------TEST METRICS----------------------
jaccard_index 0.7802364347819745
test_sensitivity 0.8481279581687503
test_specifitivity 0.973312916208177
test_accuracy 0.9453213739056959
test_precision 0.9015042411518669
test_jaccard_score 0.7802364347819745
test_dicecoef 0.8740019206613738
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-131914.h5
[0. 0. 0. 0. 0.] [0.94532137 0.78023643 0.90150424 0.84812796 0.97331292]

-------------------------
Rep: 1
-------------------------

2021-09-24 13:19:15.114692: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 13:19:15.115329: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_3"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 256, 256, 32) 320         input_2[0][0]
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_19[0][0]
__________________________________________________________________________________________________
max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_21 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_4[0][0]
__________________________________________________________________________________________________
conv2d_22 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_21[0][0]
__________________________________________________________________________________________________
max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_22[0][0]
__________________________________________________________________________________________________
conv2d_23 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_5[0][0]
__________________________________________________________________________________________________
conv2d_24 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_23[0][0]
__________________________________________________________________________________________________
max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 128)  0           conv2d_24[0][0]
__________________________________________________________________________________________________
conv2d_25 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_6[0][0]
__________________________________________________________________________________________________
conv2d_26 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_25[0][0]
__________________________________________________________________________________________________
max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2d_26[0][0]
__________________________________________________________________________________________________
conv2d_27 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_7[0][0]
__________________________________________________________________________________________________
conv2d_28 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_27[0][0]
__________________________________________________________________________________________________
up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_28[0][0]
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_4[0][0]
                                                                 conv2d_26[0][0]
__________________________________________________________________________________________________
conv2d_29 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_4[0][0]
__________________________________________________________________________________________________
conv2d_30 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_29[0][0]
__________________________________________________________________________________________________
up_sampling2d_5 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_30[0][0]
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_5[0][0]
                                                                 conv2d_24[0][0]
__________________________________________________________________________________________________
conv2d_31 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_5[0][0]
__________________________________________________________________________________________________
conv2d_32 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_31[0][0]
__________________________________________________________________________________________________
up_sampling2d_6 (UpSampling2D)  (None, 128, 128, 128 0           conv2d_32[0][0]
__________________________________________________________________________________________________
concatenate_6 (Concatenate)     (None, 128, 128, 192 0           up_sampling2d_6[0][0]
                                                                 conv2d_22[0][0]
__________________________________________________________________________________________________
conv2d_33 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_6[0][0]
__________________________________________________________________________________________________
conv2d_34 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_33[0][0]
__________________________________________________________________________________________________
up_sampling2d_7 (UpSampling2D)  (None, 256, 256, 64) 0           conv2d_34[0][0]
__________________________________________________________________________________________________
concatenate_7 (Concatenate)     (None, 256, 256, 96) 0           up_sampling2d_7[0][0]
                                                                 conv2d_20[0][0]
__________________________________________________________________________________________________
conv2d_35 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_7[0][0]
__________________________________________________________________________________________________
conv2d_36 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_35[0][0]
__________________________________________________________________________________________________
conv2d_37 (Conv2D)              (None, 256, 256, 1)  33          conv2d_36[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.7146 - acc: 0.1644 - precision_1: 0.1644 - auc_1: 0.2401 - recall_1: 1.00002021-09-24 13:19:23.145568: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 13:19:23.145651: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 13:19:23.627576: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 13:19:23.638268: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23
2021-09-24 13:19:23.641014: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.trace.json.gz
2021-09-24 13:19:23.667115: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23
2021-09-24 13:19:23.675337: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.memory_profile.json.gz
2021-09-24 13:19:23.690587: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23Dumped tool data for xplane.pb to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-131915/train/plugins/profile/2021_09_24_13_19_23/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:08 - loss: 0.6986 - acc: 0.4022 - precision_1: 0.1644 - auc_1: 0.4042 - recall_1: 0.3135WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1994s vs `on_train_batch_end` time: 0.3473s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4332 - acc: 0.8171 - precision_1: 0.7154 - auc_1: 0.7810 - recall_1: 0.2576 - val_loss: 0.3425 - val_acc: 0.8634 - val_precision_1: 0.7937 - val_auc_1: 0.8630 - val_recall_1: 0.4820
Epoch 2/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3531 - acc: 0.8534 - precision_1: 0.7772 - auc_1: 0.8581 - recall_1: 0.4524 - val_loss: 0.3409 - val_acc: 0.8593 - val_precision_1: 0.9550 - val_auc_1: 0.8716 - val_recall_1: 0.3531
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3471 - acc: 0.8585 - precision_1: 0.7966 - auc_1: 0.8624 - recall_1: 0.4651 - val_loss: 0.3373 - val_acc: 0.8649 - val_precision_1: 0.9055 - val_auc_1: 0.8718 - val_recall_1: 0.4120
Epoch 4/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3457 - acc: 0.8612 - precision_1: 0.8094 - auc_1: 0.8625 - recall_1: 0.4692 - val_loss: 0.3290 - val_acc: 0.8680 - val_precision_1: 0.9496 - val_auc_1: 0.8783 - val_recall_1: 0.3961
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3287 - acc: 0.8732 - precision_1: 0.8175 - auc_1: 0.8719 - recall_1: 0.5333 - val_loss: 0.3417 - val_acc: 0.8725 - val_precision_1: 0.8769 - val_auc_1: 0.8629 - val_recall_1: 0.4431
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3176 - acc: 0.8779 - precision_1: 0.8398 - auc_1: 0.8805 - recall_1: 0.5389 - val_loss: 0.2824 - val_acc: 0.8881 - val_precision_1: 0.7753 - val_auc_1: 0.9137 - val_recall_1: 0.6811
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2761 - acc: 0.8975 - precision_1: 0.8406 - auc_1: 0.9095 - recall_1: 0.6496 - val_loss: 0.2246 - val_acc: 0.9165 - val_precision_1: 0.9007 - val_auc_1: 0.9410 - val_recall_1: 0.6759
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2357 - acc: 0.9131 - precision_1: 0.8522 - auc_1: 0.9360 - recall_1: 0.7245 - val_loss: 0.2167 - val_acc: 0.9199 - val_precision_1: 0.9011 - val_auc_1: 0.9506 - val_recall_1: 0.7131
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2122 - acc: 0.9202 - precision_1: 0.8647 - auc_1: 0.9499 - recall_1: 0.7485 - val_loss: 0.2471 - val_acc: 0.8948 - val_precision_1: 0.9832 - val_auc_1: 0.9556 - val_recall_1: 0.5058
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2049 - acc: 0.9222 - precision_1: 0.8667 - auc_1: 0.9547 - recall_1: 0.7569 - val_loss: 0.2020 - val_acc: 0.9268 - val_precision_1: 0.9464 - val_auc_1: 0.9575 - val_recall_1: 0.6967
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1954 - acc: 0.9252 - precision_1: 0.8716 - auc_1: 0.9591 - recall_1: 0.7674 - val_loss: 0.1922 - val_acc: 0.9252 - val_precision_1: 0.8252 - val_auc_1: 0.9626 - val_recall_1: 0.8199
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1915 - acc: 0.9268 - precision_1: 0.8724 - auc_1: 0.9607 - recall_1: 0.7754 - val_loss: 0.1498 - val_acc: 0.9419 - val_precision_1: 0.9053 - val_auc_1: 0.9759 - val_recall_1: 0.8095
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1809 - acc: 0.9296 - precision_1: 0.8727 - auc_1: 0.9663 - recall_1: 0.7899 - val_loss: 0.1494 - val_acc: 0.9408 - val_precision_1: 0.8695 - val_auc_1: 0.9785 - val_recall_1: 0.8518
Epoch 14/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1764 - acc: 0.9309 - precision_1: 0.8730 - auc_1: 0.9682 - recall_1: 0.7966 - val_loss: 0.1736 - val_acc: 0.9331 - val_precision_1: 0.9084 - val_auc_1: 0.9674 - val_recall_1: 0.7515
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1706 - acc: 0.9337 - precision_1: 0.8789 - auc_1: 0.9697 - recall_1: 0.8046 - val_loss: 0.1547 - val_acc: 0.9385 - val_precision_1: 0.8667 - val_auc_1: 0.9761 - val_recall_1: 0.8555
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1619 - acc: 0.9360 - precision_1: 0.8816 - auc_1: 0.9734 - recall_1: 0.8134 - val_loss: 0.1515 - val_acc: 0.9436 - val_precision_1: 0.9264 - val_auc_1: 0.9720 - val_recall_1: 0.7768
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1616 - acc: 0.9362 - precision_1: 0.8827 - auc_1: 0.9732 - recall_1: 0.8134 - val_loss: 0.1512 - val_acc: 0.9380 - val_precision_1: 0.8634 - val_auc_1: 0.9774 - val_recall_1: 0.8529
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1577 - acc: 0.9375 - precision_1: 0.8853 - auc_1: 0.9744 - recall_1: 0.8169 - val_loss: 0.1429 - val_acc: 0.9473 - val_precision_1: 0.9256 - val_auc_1: 0.9758 - val_recall_1: 0.8105
Epoch 19/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1517 - acc: 0.9397 - precision_1: 0.8878 - auc_1: 0.9766 - recall_1: 0.8259 - val_loss: 0.1609 - val_acc: 0.9381 - val_precision_1: 0.9435 - val_auc_1: 0.9768 - val_recall_1: 0.7647
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1472 - acc: 0.9414 - precision_1: 0.8916 - auc_1: 0.9778 - recall_1: 0.8303 - val_loss: 0.1360 - val_acc: 0.9509 - val_precision_1: 0.8953 - val_auc_1: 0.9771 - val_recall_1: 0.8419
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1534 - acc: 0.9396 - precision_1: 0.8878 - auc_1: 0.9759 - recall_1: 0.8254 - val_loss: 0.1470 - val_acc: 0.9407 - val_precision_1: 0.9232 - val_auc_1: 0.9791 - val_recall_1: 0.8125
Epoch 22/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1404 - acc: 0.9445 - precision_1: 0.8978 - auc_1: 0.9794 - recall_1: 0.8389 - val_loss: 0.1355 - val_acc: 0.9468 - val_precision_1: 0.8836 - val_auc_1: 0.9803 - val_recall_1: 0.8504
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1364 - acc: 0.9455 - precision_1: 0.8994 - auc_1: 0.9810 - recall_1: 0.8427 - val_loss: 0.1626 - val_acc: 0.9343 - val_precision_1: 0.8162 - val_auc_1: 0.9772 - val_recall_1: 0.8892
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1352 - acc: 0.9465 - precision_1: 0.9006 - auc_1: 0.9813 - recall_1: 0.8464 - val_loss: 0.1636 - val_acc: 0.9340 - val_precision_1: 0.8105 - val_auc_1: 0.9780 - val_recall_1: 0.9063
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1271 - acc: 0.9488 - precision_1: 0.9043 - auc_1: 0.9838 - recall_1: 0.8539 - val_loss: 0.1497 - val_acc: 0.9423 - val_precision_1: 0.8341 - val_auc_1: 0.9829 - val_recall_1: 0.9017
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1269 - acc: 0.9491 - precision_1: 0.9024 - auc_1: 0.9839 - recall_1: 0.8576 - val_loss: 0.1552 - val_acc: 0.9403 - val_precision_1: 0.8994 - val_auc_1: 0.9737 - val_recall_1: 0.8065
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1201 - acc: 0.9518 - precision_1: 0.9092 - auc_1: 0.9852 - recall_1: 0.8637 - val_loss: 0.1426 - val_acc: 0.9449 - val_precision_1: 0.9009 - val_auc_1: 0.9790 - val_recall_1: 0.8326
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1077 - acc: 0.9565 - precision_1: 0.9152 - auc_1: 0.9883 - recall_1: 0.8809 - val_loss: 0.1613 - val_acc: 0.9374 - val_precision_1: 0.8281 - val_auc_1: 0.9747 - val_recall_1: 0.8902
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1083 - acc: 0.9566 - precision_1: 0.9134 - auc_1: 0.9883 - recall_1: 0.8835 - val_loss: 0.1631 - val_acc: 0.9360 - val_precision_1: 0.8514 - val_auc_1: 0.9748 - val_recall_1: 0.8680
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1030 - acc: 0.9585 - precision_1: 0.9168 - auc_1: 0.9894 - recall_1: 0.8891 - val_loss: 0.1430 - val_acc: 0.9488 - val_precision_1: 0.9321 - val_auc_1: 0.9782 - val_recall_1: 0.8181
Epoch 31/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0894 - acc: 0.9637 - precision_1: 0.9276 - auc_1: 0.9918 - recall_1: 0.9027 - val_loss: 0.1631 - val_acc: 0.9414 - val_precision_1: 0.8582 - val_auc_1: 0.9717 - val_recall_1: 0.8632
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0860 - acc: 0.9650 - precision_1: 0.9287 - auc_1: 0.9925 - recall_1: 0.9083 - val_loss: 0.1674 - val_acc: 0.9437 - val_precision_1: 0.8892 - val_auc_1: 0.9711 - val_recall_1: 0.8415
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0845 - acc: 0.9661 - precision_1: 0.9295 - auc_1: 0.9928 - recall_1: 0.9124 - val_loss: 0.1582 - val_acc: 0.9443 - val_precision_1: 0.8736 - val_auc_1: 0.9752 - val_recall_1: 0.8604
Epoch 34/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0713 - acc: 0.9706 - precision_1: 0.9392 - auc_1: 0.9948 - recall_1: 0.9238 - val_loss: 0.1966 - val_acc: 0.9424 - val_precision_1: 0.8629 - val_auc_1: 0.9633 - val_recall_1: 0.8392
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0646 - acc: 0.9733 - precision_1: 0.9447 - auc_1: 0.9958 - recall_1: 0.9312 - val_loss: 0.1884 - val_acc: 0.9380 - val_precision_1: 0.8585 - val_auc_1: 0.9681 - val_recall_1: 0.8672
Epoch 36/100
254/254 [==============================] - ETA: 0s - loss: 0.0614 - acc: 0.9745 - precision_1: 0.9468 - auc_1: 0.9961 - recall_1: 0.9348Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 216ms/step - loss: 0.0614 - acc: 0.9745 - precision_1: 0.9468 - auc_1: 0.9961 - recall_1: 0.9348 - val_loss: 0.2178 - val_acc: 0.9414 - val_precision_1: 0.8980 - val_auc_1: 0.9587 - val_recall_1: 0.8175
Epoch 00036: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1385 - acc: 0.9445 - precision_1: 0.8943 - auc_1: 0.9812 - recall_1: 0.8527
---------------TEST METRICS----------------------
jaccard_index 0.7772905925416126
test_sensitivity 0.8534643076575357
test_specifitivity 0.9679413640459974
test_accuracy 0.9423441244355331
test_precision 0.8846209606136948
test_jaccard_score 0.7772905925416126
test_dicecoef 0.8687633794174632
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-135251.h5
[0.94532137 0.78023643 0.90150424 0.84812796 0.97331292] [0.94234412 0.77729059 0.88462096 0.85346431 0.96794136]

-------------------------
Rep: 2
-------------------------

2021-09-24 13:52:52.311450: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 13:52:52.311564: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_5"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_38 (Conv2D)              (None, 256, 256, 32) 320         input_3[0][0]
__________________________________________________________________________________________________
conv2d_39 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_38[0][0]
__________________________________________________________________________________________________
max_pooling2d_8 (MaxPooling2D)  (None, 128, 128, 32) 0           conv2d_39[0][0]
__________________________________________________________________________________________________
conv2d_40 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_8[0][0]
__________________________________________________________________________________________________
conv2d_41 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_40[0][0]
__________________________________________________________________________________________________
max_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 64)   0           conv2d_41[0][0]
__________________________________________________________________________________________________
conv2d_42 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_9[0][0]
__________________________________________________________________________________________________
conv2d_43 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_42[0][0]
__________________________________________________________________________________________________
max_pooling2d_10 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_43[0][0]
__________________________________________________________________________________________________
conv2d_44 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_10[0][0]
__________________________________________________________________________________________________
conv2d_45 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_44[0][0]
__________________________________________________________________________________________________
max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_45[0][0]
__________________________________________________________________________________________________
conv2d_46 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_11[0][0]
__________________________________________________________________________________________________
conv2d_47 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_46[0][0]
__________________________________________________________________________________________________
up_sampling2d_8 (UpSampling2D)  (None, 32, 32, 512)  0           conv2d_47[0][0]
__________________________________________________________________________________________________
concatenate_8 (Concatenate)     (None, 32, 32, 768)  0           up_sampling2d_8[0][0]
                                                                 conv2d_45[0][0]
__________________________________________________________________________________________________
conv2d_48 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_8[0][0]
__________________________________________________________________________________________________
conv2d_49 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_48[0][0]
__________________________________________________________________________________________________
up_sampling2d_9 (UpSampling2D)  (None, 64, 64, 256)  0           conv2d_49[0][0]
__________________________________________________________________________________________________
concatenate_9 (Concatenate)     (None, 64, 64, 384)  0           up_sampling2d_9[0][0]
                                                                 conv2d_43[0][0]
__________________________________________________________________________________________________
conv2d_50 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_9[0][0]
__________________________________________________________________________________________________
conv2d_51 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_50[0][0]
__________________________________________________________________________________________________
up_sampling2d_10 (UpSampling2D) (None, 128, 128, 128 0           conv2d_51[0][0]
__________________________________________________________________________________________________
concatenate_10 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_10[0][0]
                                                                 conv2d_41[0][0]
__________________________________________________________________________________________________
conv2d_52 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_10[0][0]
__________________________________________________________________________________________________
conv2d_53 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_52[0][0]
__________________________________________________________________________________________________
up_sampling2d_11 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_53[0][0]
__________________________________________________________________________________________________
concatenate_11 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_11[0][0]
                                                                 conv2d_39[0][0]
__________________________________________________________________________________________________
conv2d_54 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_11[0][0]
__________________________________________________________________________________________________
conv2d_55 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_54[0][0]
__________________________________________________________________________________________________
conv2d_56 (Conv2D)              (None, 256, 256, 1)  33          conv2d_55[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.7129 - acc: 0.1644 - precision_2: 0.1644 - auc_2: 0.2327 - recall_2: 1.00002021-09-24 13:53:00.043691: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 13:53:00.043782: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 13:53:00.531775: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 13:53:00.542456: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00
2021-09-24 13:53:00.545806: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.trace.json.gz
2021-09-24 13:53:00.570971: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00
2021-09-24 13:53:00.579579: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.memory_profile.json.gz
2021-09-24 13:53:00.593505: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00Dumped tool data for xplane.pb to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-135252/train/plugins/profile/2021_09_24_13_53_00/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:09 - loss: 0.7047 - acc: 0.2660 - precision_2: 0.2624 - auc_2: 0.2678 - recall_2: 0.9942WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2031s vs `on_train_batch_end` time: 0.3478s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4563 - acc: 0.8042 - precision_2: 0.6683 - auc_2: 0.7481 - recall_2: 0.1903 - val_loss: 0.3582 - val_acc: 0.8600 - val_precision_2: 0.9303 - val_auc_2: 0.8584 - val_recall_2: 0.3687
Epoch 2/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3640 - acc: 0.8532 - precision_2: 0.7740 - auc_2: 0.8460 - recall_2: 0.4550 - val_loss: 0.3386 - val_acc: 0.8652 - val_precision_2: 0.9205 - val_auc_2: 0.8691 - val_recall_2: 0.3988
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3452 - acc: 0.8603 - precision_2: 0.8000 - auc_2: 0.8625 - recall_2: 0.4728 - val_loss: 0.3324 - val_acc: 0.8667 - val_precision_2: 0.8650 - val_auc_2: 0.8666 - val_recall_2: 0.4470
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3375 - acc: 0.8648 - precision_2: 0.8130 - auc_2: 0.8658 - recall_2: 0.4876 - val_loss: 0.3043 - val_acc: 0.8795 - val_precision_2: 0.8903 - val_auc_2: 0.8868 - val_recall_2: 0.4896
Epoch 5/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3266 - acc: 0.8690 - precision_2: 0.8261 - auc_2: 0.8748 - recall_2: 0.5004 - val_loss: 0.3054 - val_acc: 0.8795 - val_precision_2: 0.8881 - val_auc_2: 0.8841 - val_recall_2: 0.4743
Epoch 6/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3188 - acc: 0.8737 - precision_2: 0.8265 - auc_2: 0.8803 - recall_2: 0.5274 - val_loss: 0.2915 - val_acc: 0.8896 - val_precision_2: 0.8371 - val_auc_2: 0.8987 - val_recall_2: 0.6093
Epoch 7/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3063 - acc: 0.8814 - precision_2: 0.8267 - auc_2: 0.8882 - recall_2: 0.5719 - val_loss: 0.2895 - val_acc: 0.8869 - val_precision_2: 0.9039 - val_auc_2: 0.8999 - val_recall_2: 0.5143
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2966 - acc: 0.8871 - precision_2: 0.8423 - auc_2: 0.8940 - recall_2: 0.5886 - val_loss: 0.3014 - val_acc: 0.8840 - val_precision_2: 0.8666 - val_auc_2: 0.8955 - val_recall_2: 0.5569
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2880 - acc: 0.8905 - precision_2: 0.8324 - auc_2: 0.9014 - recall_2: 0.6187 - val_loss: 0.3015 - val_acc: 0.8833 - val_precision_2: 0.9480 - val_auc_2: 0.9034 - val_recall_2: 0.4673
Epoch 10/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2903 - acc: 0.8911 - precision_2: 0.8425 - auc_2: 0.8983 - recall_2: 0.6113 - val_loss: 0.2812 - val_acc: 0.8912 - val_precision_2: 0.8806 - val_auc_2: 0.9110 - val_recall_2: 0.5675
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2694 - acc: 0.8987 - precision_2: 0.8401 - auc_2: 0.9139 - recall_2: 0.6574 - val_loss: 0.2998 - val_acc: 0.8859 - val_precision_2: 0.9055 - val_auc_2: 0.8968 - val_recall_2: 0.5137
Epoch 12/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2680 - acc: 0.8994 - precision_2: 0.8438 - auc_2: 0.9144 - recall_2: 0.6570 - val_loss: 0.2368 - val_acc: 0.9129 - val_precision_2: 0.8603 - val_auc_2: 0.9421 - val_recall_2: 0.7008
Epoch 13/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2517 - acc: 0.9047 - precision_2: 0.8479 - auc_2: 0.9258 - recall_2: 0.6824 - val_loss: 0.2361 - val_acc: 0.9100 - val_precision_2: 0.8015 - val_auc_2: 0.9428 - val_recall_2: 0.7718
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2490 - acc: 0.9057 - precision_2: 0.8523 - auc_2: 0.9271 - recall_2: 0.6827 - val_loss: 0.2776 - val_acc: 0.9079 - val_precision_2: 0.9318 - val_auc_2: 0.9246 - val_recall_2: 0.5972
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2398 - acc: 0.9092 - precision_2: 0.8583 - auc_2: 0.9346 - recall_2: 0.6952 - val_loss: 0.2088 - val_acc: 0.9197 - val_precision_2: 0.8676 - val_auc_2: 0.9542 - val_recall_2: 0.7543
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2248 - acc: 0.9137 - precision_2: 0.8630 - auc_2: 0.9437 - recall_2: 0.7151 - val_loss: 0.1891 - val_acc: 0.9270 - val_precision_2: 0.8966 - val_auc_2: 0.9560 - val_recall_2: 0.7134
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2158 - acc: 0.9180 - precision_2: 0.8706 - auc_2: 0.9476 - recall_2: 0.7297 - val_loss: 0.1952 - val_acc: 0.9232 - val_precision_2: 0.8962 - val_auc_2: 0.9579 - val_recall_2: 0.7358
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2112 - acc: 0.9197 - precision_2: 0.8743 - auc_2: 0.9497 - recall_2: 0.7349 - val_loss: 0.1804 - val_acc: 0.9314 - val_precision_2: 0.9025 - val_auc_2: 0.9618 - val_recall_2: 0.7495
Epoch 19/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2042 - acc: 0.9231 - precision_2: 0.8700 - auc_2: 0.9544 - recall_2: 0.7583 - val_loss: 0.1860 - val_acc: 0.9274 - val_precision_2: 0.9021 - val_auc_2: 0.9621 - val_recall_2: 0.7522
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1814 - acc: 0.9303 - precision_2: 0.8810 - auc_2: 0.9637 - recall_2: 0.7838 - val_loss: 0.1449 - val_acc: 0.9444 - val_precision_2: 0.8756 - val_auc_2: 0.9730 - val_recall_2: 0.8269
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1808 - acc: 0.9313 - precision_2: 0.8816 - auc_2: 0.9637 - recall_2: 0.7885 - val_loss: 0.1667 - val_acc: 0.9335 - val_precision_2: 0.8738 - val_auc_2: 0.9726 - val_recall_2: 0.8347
Epoch 22/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1734 - acc: 0.9337 - precision_2: 0.8868 - auc_2: 0.9669 - recall_2: 0.7954 - val_loss: 0.1489 - val_acc: 0.9412 - val_precision_2: 0.8890 - val_auc_2: 0.9746 - val_recall_2: 0.8120
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1642 - acc: 0.9369 - precision_2: 0.8886 - auc_2: 0.9705 - recall_2: 0.8103 - val_loss: 0.1417 - val_acc: 0.9443 - val_precision_2: 0.8756 - val_auc_2: 0.9783 - val_recall_2: 0.8586
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1700 - acc: 0.9342 - precision_2: 0.8862 - auc_2: 0.9695 - recall_2: 0.7984 - val_loss: 0.1541 - val_acc: 0.9392 - val_precision_2: 0.8435 - val_auc_2: 0.9776 - val_recall_2: 0.8823
Epoch 25/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1520 - acc: 0.9414 - precision_2: 0.8955 - auc_2: 0.9749 - recall_2: 0.8254 - val_loss: 0.1410 - val_acc: 0.9452 - val_precision_2: 0.8478 - val_auc_2: 0.9829 - val_recall_2: 0.8971
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1459 - acc: 0.9428 - precision_2: 0.8983 - auc_2: 0.9772 - recall_2: 0.8298 - val_loss: 0.1417 - val_acc: 0.9462 - val_precision_2: 0.9276 - val_auc_2: 0.9770 - val_recall_2: 0.8075
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1432 - acc: 0.9446 - precision_2: 0.9030 - auc_2: 0.9775 - recall_2: 0.8339 - val_loss: 0.1366 - val_acc: 0.9470 - val_precision_2: 0.8992 - val_auc_2: 0.9797 - val_recall_2: 0.8456
Epoch 28/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1348 - acc: 0.9472 - precision_2: 0.9066 - auc_2: 0.9806 - recall_2: 0.8431 - val_loss: 0.1587 - val_acc: 0.9372 - val_precision_2: 0.8199 - val_auc_2: 0.9780 - val_recall_2: 0.9029
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1473 - acc: 0.9424 - precision_2: 0.8953 - auc_2: 0.9767 - recall_2: 0.8310 - val_loss: 0.1531 - val_acc: 0.9385 - val_precision_2: 0.8507 - val_auc_2: 0.9776 - val_recall_2: 0.8825
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1312 - acc: 0.9481 - precision_2: 0.9050 - auc_2: 0.9818 - recall_2: 0.8496 - val_loss: 0.1249 - val_acc: 0.9498 - val_precision_2: 0.9259 - val_auc_2: 0.9829 - val_recall_2: 0.8294
Epoch 31/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1296 - acc: 0.9484 - precision_2: 0.9070 - auc_2: 0.9823 - recall_2: 0.8487 - val_loss: 0.1495 - val_acc: 0.9407 - val_precision_2: 0.8489 - val_auc_2: 0.9773 - val_recall_2: 0.8724
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1248 - acc: 0.9514 - precision_2: 0.9117 - auc_2: 0.9830 - recall_2: 0.8587 - val_loss: 0.1332 - val_acc: 0.9480 - val_precision_2: 0.8855 - val_auc_2: 0.9808 - val_recall_2: 0.8693
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1211 - acc: 0.9515 - precision_2: 0.9100 - auc_2: 0.9848 - recall_2: 0.8613 - val_loss: 0.1282 - val_acc: 0.9495 - val_precision_2: 0.8953 - val_auc_2: 0.9816 - val_recall_2: 0.8616
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1130 - acc: 0.9551 - precision_2: 0.9185 - auc_2: 0.9867 - recall_2: 0.8696 - val_loss: 0.1344 - val_acc: 0.9478 - val_precision_2: 0.8652 - val_auc_2: 0.9797 - val_recall_2: 0.8689
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1065 - acc: 0.9572 - precision_2: 0.9191 - auc_2: 0.9881 - recall_2: 0.8798 - val_loss: 0.1374 - val_acc: 0.9472 - val_precision_2: 0.8832 - val_auc_2: 0.9811 - val_recall_2: 0.8819
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1038 - acc: 0.9583 - precision_2: 0.9211 - auc_2: 0.9888 - recall_2: 0.8832 - val_loss: 0.1564 - val_acc: 0.9463 - val_precision_2: 0.9219 - val_auc_2: 0.9747 - val_recall_2: 0.8168
Epoch 37/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1122 - acc: 0.9556 - precision_2: 0.9153 - auc_2: 0.9868 - recall_2: 0.8758 - val_loss: 0.1387 - val_acc: 0.9469 - val_precision_2: 0.9443 - val_auc_2: 0.9799 - val_recall_2: 0.8012
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0987 - acc: 0.9600 - precision_2: 0.9241 - auc_2: 0.9899 - recall_2: 0.8881 - val_loss: 0.1399 - val_acc: 0.9483 - val_precision_2: 0.8926 - val_auc_2: 0.9787 - val_recall_2: 0.8563
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0950 - acc: 0.9617 - precision_2: 0.9266 - auc_2: 0.9907 - recall_2: 0.8939 - val_loss: 0.1451 - val_acc: 0.9476 - val_precision_2: 0.9068 - val_auc_2: 0.9774 - val_recall_2: 0.8481
Epoch 40/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0902 - acc: 0.9636 - precision_2: 0.9304 - auc_2: 0.9914 - recall_2: 0.8989 - val_loss: 0.1689 - val_acc: 0.9510 - val_precision_2: 0.9279 - val_auc_2: 0.9654 - val_recall_2: 0.8167
Epoch 41/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0818 - acc: 0.9668 - precision_2: 0.9358 - auc_2: 0.9929 - recall_2: 0.9089 - val_loss: 0.1521 - val_acc: 0.9461 - val_precision_2: 0.8742 - val_auc_2: 0.9778 - val_recall_2: 0.8807
Epoch 42/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0780 - acc: 0.9680 - precision_2: 0.9367 - auc_2: 0.9936 - recall_2: 0.9140 - val_loss: 0.1568 - val_acc: 0.9482 - val_precision_2: 0.8796 - val_auc_2: 0.9756 - val_recall_2: 0.8732
Epoch 43/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0717 - acc: 0.9706 - precision_2: 0.9396 - auc_2: 0.9945 - recall_2: 0.9235 - val_loss: 0.1467 - val_acc: 0.9434 - val_precision_2: 0.9009 - val_auc_2: 0.9758 - val_recall_2: 0.8309
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.0896 - acc: 0.9640 - precision_2: 0.9289 - auc_2: 0.9915 - recall_2: 0.9025Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0896 - acc: 0.9640 - precision_2: 0.9289 - auc_2: 0.9915 - recall_2: 0.9025 - val_loss: 0.1817 - val_acc: 0.9472 - val_precision_2: 0.9547 - val_auc_2: 0.9671 - val_recall_2: 0.7886
Epoch 00044: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1356 - acc: 0.9476 - precision_2: 0.9184 - auc_2: 0.9812 - recall_2: 0.8402
---------------TEST METRICS----------------------
jaccard_index 0.7817960989817996
test_sensitivity 0.8486513811664033
test_specifitivity 0.9736645847415581
test_accuracy 0.945711446992049
test_precision 0.9027300354350146
test_jaccard_score 0.7817960989817996
test_dicecoef 0.874855795694071
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-143349.h5
[1.8876655  1.55752703 1.7861252  1.70159227 1.94125428] [0.94571145 0.7817961  0.90273004 0.84865138 0.97366458]

-------------------------
Averaged metrics for Baseline - lesion: [0.94445898 0.77977438 0.89628508 0.85008122 0.97163962]
-------------------------


-------------------------
RUN: Baseline + Augumentations - lesion, PARAMS: {'augumentation': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 14:33:50.500196: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 14:33:50.500292: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_7"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_4 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_57 (Conv2D)              (None, 256, 256, 32) 320         input_4[0][0]
__________________________________________________________________________________________________
conv2d_58 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_57[0][0]
__________________________________________________________________________________________________
max_pooling2d_12 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_58[0][0]
__________________________________________________________________________________________________
conv2d_59 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_12[0][0]
__________________________________________________________________________________________________
conv2d_60 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_59[0][0]
__________________________________________________________________________________________________
max_pooling2d_13 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_60[0][0]
__________________________________________________________________________________________________
conv2d_61 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_13[0][0]
__________________________________________________________________________________________________
conv2d_62 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_61[0][0]
__________________________________________________________________________________________________
max_pooling2d_14 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_62[0][0]
__________________________________________________________________________________________________
conv2d_63 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_14[0][0]
__________________________________________________________________________________________________
conv2d_64 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_63[0][0]
__________________________________________________________________________________________________
max_pooling2d_15 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_64[0][0]
__________________________________________________________________________________________________
conv2d_65 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_15[0][0]
__________________________________________________________________________________________________
conv2d_66 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_65[0][0]
__________________________________________________________________________________________________
up_sampling2d_12 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_66[0][0]
__________________________________________________________________________________________________
concatenate_12 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_12[0][0]
                                                                 conv2d_64[0][0]
__________________________________________________________________________________________________
conv2d_67 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_12[0][0]
__________________________________________________________________________________________________
conv2d_68 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_67[0][0]
__________________________________________________________________________________________________
up_sampling2d_13 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_68[0][0]
__________________________________________________________________________________________________
concatenate_13 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_13[0][0]
                                                                 conv2d_62[0][0]
__________________________________________________________________________________________________
conv2d_69 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_13[0][0]
__________________________________________________________________________________________________
conv2d_70 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_69[0][0]
__________________________________________________________________________________________________
up_sampling2d_14 (UpSampling2D) (None, 128, 128, 128 0           conv2d_70[0][0]
__________________________________________________________________________________________________
concatenate_14 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_14[0][0]
                                                                 conv2d_60[0][0]
__________________________________________________________________________________________________
conv2d_71 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_14[0][0]
__________________________________________________________________________________________________
conv2d_72 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_71[0][0]
__________________________________________________________________________________________________
up_sampling2d_15 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_72[0][0]
__________________________________________________________________________________________________
concatenate_15 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_15[0][0]
                                                                 conv2d_58[0][0]
__________________________________________________________________________________________________
conv2d_73 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_15[0][0]
__________________________________________________________________________________________________
conv2d_74 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_73[0][0]
__________________________________________________________________________________________________
conv2d_75 (Conv2D)              (None, 256, 256, 1)  33          conv2d_74[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6911 - acc: 0.7177 - precision_3: 0.0670 - auc_3: 0.5010 - recall_3: 0.05332021-09-24 14:33:58.413921: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 14:33:58.414019: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 14:33:58.905583: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 14:33:58.916011: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58
2021-09-24 14:33:58.919179: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.trace.json.gz
2021-09-24 14:33:58.944552: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58
2021-09-24 14:33:58.953297: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.memory_profile.json.gz
2021-09-24 14:33:58.974596: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58Dumped tool data for xplane.pb to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-143350/train/plugins/profile/2021_09_24_14_33_58/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:10 - loss: 0.6851 - acc: 0.6759 - precision_3: 0.0670 - auc_3: 0.4170 - recall_3: 0.0167WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2072s vs `on_train_batch_end` time: 0.3551s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.3982 - acc: 0.8218 - precision_3: 0.7602 - auc_3: 0.8145 - recall_3: 0.2646 - val_loss: 0.3932 - val_acc: 0.8565 - val_precision_3: 0.8119 - val_auc_3: 0.8093 - val_recall_3: 0.4219
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3378 - acc: 0.8548 - precision_3: 0.8047 - auc_3: 0.8667 - recall_3: 0.4435 - val_loss: 0.3418 - val_acc: 0.8728 - val_precision_3: 0.8347 - val_auc_3: 0.8630 - val_recall_3: 0.4985
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3316 - acc: 0.8587 - precision_3: 0.8157 - auc_3: 0.8716 - recall_3: 0.4567 - val_loss: 0.3599 - val_acc: 0.8614 - val_precision_3: 0.8386 - val_auc_3: 0.8406 - val_recall_3: 0.4363
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3291 - acc: 0.8615 - precision_3: 0.8083 - auc_3: 0.8728 - recall_3: 0.4835 - val_loss: 0.3794 - val_acc: 0.8559 - val_precision_3: 0.6943 - val_auc_3: 0.8321 - val_recall_3: 0.5676
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3407 - acc: 0.8637 - precision_3: 0.8048 - auc_3: 0.8712 - recall_3: 0.5026 - val_loss: 0.5089 - val_acc: 0.8094 - val_precision_3: 0.5291 - val_auc_3: 0.8416 - val_recall_3: 0.6731
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3267 - acc: 0.8624 - precision_3: 0.8171 - auc_3: 0.8742 - recall_3: 0.4789 - val_loss: 0.3277 - val_acc: 0.8732 - val_precision_3: 0.7723 - val_auc_3: 0.8795 - val_recall_3: 0.5884
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2832 - acc: 0.8855 - precision_3: 0.8298 - auc_3: 0.9062 - recall_3: 0.6080 - val_loss: 0.2882 - val_acc: 0.8859 - val_precision_3: 0.7344 - val_auc_3: 0.9075 - val_recall_3: 0.7131
Epoch 8/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2479 - acc: 0.9013 - precision_3: 0.8412 - auc_3: 0.9292 - recall_3: 0.6891 - val_loss: 0.2455 - val_acc: 0.9098 - val_precision_3: 0.8121 - val_auc_3: 0.9370 - val_recall_3: 0.7661
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2234 - acc: 0.9100 - precision_3: 0.8568 - auc_3: 0.9443 - recall_3: 0.7217 - val_loss: 0.2187 - val_acc: 0.9177 - val_precision_3: 0.8781 - val_auc_3: 0.9480 - val_recall_3: 0.7042
Epoch 10/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2016 - acc: 0.9179 - precision_3: 0.8746 - auc_3: 0.9546 - recall_3: 0.7436 - val_loss: 0.1945 - val_acc: 0.9261 - val_precision_3: 0.8546 - val_auc_3: 0.9556 - val_recall_3: 0.7883
Epoch 11/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1944 - acc: 0.9203 - precision_3: 0.8826 - auc_3: 0.9589 - recall_3: 0.7501 - val_loss: 0.1785 - val_acc: 0.9298 - val_precision_3: 0.9433 - val_auc_3: 0.9672 - val_recall_3: 0.7108
Epoch 12/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1883 - acc: 0.9236 - precision_3: 0.8834 - auc_3: 0.9612 - recall_3: 0.7662 - val_loss: 0.2056 - val_acc: 0.9161 - val_precision_3: 0.7534 - val_auc_3: 0.9690 - val_recall_3: 0.8952
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1787 - acc: 0.9268 - precision_3: 0.8871 - auc_3: 0.9660 - recall_3: 0.7787 - val_loss: 0.1871 - val_acc: 0.9270 - val_precision_3: 0.7930 - val_auc_3: 0.9752 - val_recall_3: 0.8924
Epoch 14/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1745 - acc: 0.9274 - precision_3: 0.8885 - auc_3: 0.9671 - recall_3: 0.7807 - val_loss: 0.1636 - val_acc: 0.9362 - val_precision_3: 0.8529 - val_auc_3: 0.9709 - val_recall_3: 0.8346
Epoch 15/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1664 - acc: 0.9311 - precision_3: 0.8918 - auc_3: 0.9708 - recall_3: 0.7953 - val_loss: 0.1674 - val_acc: 0.9355 - val_precision_3: 0.8475 - val_auc_3: 0.9751 - val_recall_3: 0.8663
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1641 - acc: 0.9307 - precision_3: 0.8883 - auc_3: 0.9718 - recall_3: 0.7987 - val_loss: 0.1399 - val_acc: 0.9462 - val_precision_3: 0.9403 - val_auc_3: 0.9774 - val_recall_3: 0.7778
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1652 - acc: 0.9309 - precision_3: 0.8951 - auc_3: 0.9711 - recall_3: 0.7921 - val_loss: 0.1475 - val_acc: 0.9402 - val_precision_3: 0.8866 - val_auc_3: 0.9777 - val_recall_3: 0.8345
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1647 - acc: 0.9317 - precision_3: 0.8924 - auc_3: 0.9713 - recall_3: 0.7992 - val_loss: 0.1349 - val_acc: 0.9476 - val_precision_3: 0.9012 - val_auc_3: 0.9796 - val_recall_3: 0.8385
Epoch 19/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1588 - acc: 0.9329 - precision_3: 0.8918 - auc_3: 0.9735 - recall_3: 0.8050 - val_loss: 0.1512 - val_acc: 0.9388 - val_precision_3: 0.9273 - val_auc_3: 0.9783 - val_recall_3: 0.7834
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1585 - acc: 0.9327 - precision_3: 0.8951 - auc_3: 0.9742 - recall_3: 0.8019 - val_loss: 0.1323 - val_acc: 0.9475 - val_precision_3: 0.8671 - val_auc_3: 0.9800 - val_recall_3: 0.8573
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1596 - acc: 0.9332 - precision_3: 0.8924 - auc_3: 0.9733 - recall_3: 0.8077 - val_loss: 0.1450 - val_acc: 0.9444 - val_precision_3: 0.8807 - val_auc_3: 0.9820 - val_recall_3: 0.8801
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1530 - acc: 0.9353 - precision_3: 0.8994 - auc_3: 0.9755 - recall_3: 0.8103 - val_loss: 0.1687 - val_acc: 0.9347 - val_precision_3: 0.8213 - val_auc_3: 0.9762 - val_recall_3: 0.8673
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1520 - acc: 0.9365 - precision_3: 0.9043 - auc_3: 0.9750 - recall_3: 0.8102 - val_loss: 0.1490 - val_acc: 0.9391 - val_precision_3: 0.8391 - val_auc_3: 0.9788 - val_recall_3: 0.8804
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1509 - acc: 0.9361 - precision_3: 0.9011 - auc_3: 0.9764 - recall_3: 0.8124 - val_loss: 0.1498 - val_acc: 0.9414 - val_precision_3: 0.8431 - val_auc_3: 0.9814 - val_recall_3: 0.8951
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1541 - acc: 0.9348 - precision_3: 0.8973 - auc_3: 0.9753 - recall_3: 0.8105 - val_loss: 0.1827 - val_acc: 0.9392 - val_precision_3: 0.8399 - val_auc_3: 0.9777 - val_recall_3: 0.8741
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1490 - acc: 0.9360 - precision_3: 0.9018 - auc_3: 0.9770 - recall_3: 0.8104 - val_loss: 0.1399 - val_acc: 0.9436 - val_precision_3: 0.9147 - val_auc_3: 0.9792 - val_recall_3: 0.8078
Epoch 27/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1513 - acc: 0.9362 - precision_3: 0.9014 - auc_3: 0.9758 - recall_3: 0.8135 - val_loss: 0.1411 - val_acc: 0.9455 - val_precision_3: 0.8950 - val_auc_3: 0.9792 - val_recall_3: 0.8426
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1445 - acc: 0.9387 - precision_3: 0.9045 - auc_3: 0.9785 - recall_3: 0.8225 - val_loss: 0.1702 - val_acc: 0.9393 - val_precision_3: 0.8315 - val_auc_3: 0.9808 - val_recall_3: 0.8959
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1494 - acc: 0.9374 - precision_3: 0.8988 - auc_3: 0.9766 - recall_3: 0.8228 - val_loss: 0.1790 - val_acc: 0.9220 - val_precision_3: 0.7783 - val_auc_3: 0.9776 - val_recall_3: 0.9153
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1484 - acc: 0.9370 - precision_3: 0.8988 - auc_3: 0.9773 - recall_3: 0.8215 - val_loss: 0.1230 - val_acc: 0.9520 - val_precision_3: 0.9190 - val_auc_3: 0.9847 - val_recall_3: 0.8483
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1430 - acc: 0.9392 - precision_3: 0.9056 - auc_3: 0.9788 - recall_3: 0.8240 - val_loss: 0.1570 - val_acc: 0.9410 - val_precision_3: 0.8408 - val_auc_3: 0.9785 - val_recall_3: 0.8867
Epoch 32/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1450 - acc: 0.9386 - precision_3: 0.9025 - auc_3: 0.9781 - recall_3: 0.8250 - val_loss: 0.1302 - val_acc: 0.9513 - val_precision_3: 0.9039 - val_auc_3: 0.9839 - val_recall_3: 0.8642
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1474 - acc: 0.9378 - precision_3: 0.9030 - auc_3: 0.9774 - recall_3: 0.8190 - val_loss: 0.1375 - val_acc: 0.9421 - val_precision_3: 0.8481 - val_auc_3: 0.9810 - val_recall_3: 0.8839
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1463 - acc: 0.9384 - precision_3: 0.9006 - auc_3: 0.9777 - recall_3: 0.8248 - val_loss: 0.1511 - val_acc: 0.9442 - val_precision_3: 0.8451 - val_auc_3: 0.9787 - val_recall_3: 0.8761
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1424 - acc: 0.9395 - precision_3: 0.9056 - auc_3: 0.9790 - recall_3: 0.8248 - val_loss: 0.1371 - val_acc: 0.9445 - val_precision_3: 0.8645 - val_auc_3: 0.9831 - val_recall_3: 0.8935
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1402 - acc: 0.9404 - precision_3: 0.9072 - auc_3: 0.9795 - recall_3: 0.8272 - val_loss: 0.1301 - val_acc: 0.9474 - val_precision_3: 0.8854 - val_auc_3: 0.9819 - val_recall_3: 0.8650
Epoch 37/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1439 - acc: 0.9388 - precision_3: 0.9029 - auc_3: 0.9785 - recall_3: 0.8248 - val_loss: 0.1623 - val_acc: 0.9416 - val_precision_3: 0.9458 - val_auc_3: 0.9760 - val_recall_3: 0.7740
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1384 - acc: 0.9409 - precision_3: 0.9052 - auc_3: 0.9804 - recall_3: 0.8347 - val_loss: 0.1408 - val_acc: 0.9464 - val_precision_3: 0.8848 - val_auc_3: 0.9791 - val_recall_3: 0.8559
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1408 - acc: 0.9396 - precision_3: 0.9061 - auc_3: 0.9795 - recall_3: 0.8250 - val_loss: 0.1394 - val_acc: 0.9433 - val_precision_3: 0.8955 - val_auc_3: 0.9805 - val_recall_3: 0.8394
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1337 - acc: 0.9425 - precision_3: 0.9098 - auc_3: 0.9818 - recall_3: 0.8372 - val_loss: 0.1258 - val_acc: 0.9530 - val_precision_3: 0.9164 - val_auc_3: 0.9808 - val_recall_3: 0.8401
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1353 - acc: 0.9419 - precision_3: 0.9087 - auc_3: 0.9810 - recall_3: 0.8351 - val_loss: 0.1512 - val_acc: 0.9396 - val_precision_3: 0.8528 - val_auc_3: 0.9800 - val_recall_3: 0.8757
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1382 - acc: 0.9402 - precision_3: 0.9080 - auc_3: 0.9801 - recall_3: 0.8281 - val_loss: 0.1320 - val_acc: 0.9473 - val_precision_3: 0.8856 - val_auc_3: 0.9825 - val_recall_3: 0.8603
Epoch 43/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1360 - acc: 0.9412 - precision_3: 0.9094 - auc_3: 0.9814 - recall_3: 0.8318 - val_loss: 0.1477 - val_acc: 0.9429 - val_precision_3: 0.9075 - val_auc_3: 0.9770 - val_recall_3: 0.8211
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1315 - acc: 0.9430 - precision_3: 0.9079 - auc_3: 0.9823 - recall_3: 0.8409Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1315 - acc: 0.9430 - precision_3: 0.9079 - auc_3: 0.9823 - recall_3: 0.8409 - val_loss: 0.1234 - val_acc: 0.9521 - val_precision_3: 0.9434 - val_auc_3: 0.9831 - val_recall_3: 0.8236
Epoch 00044: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1311 - acc: 0.9499 - precision_3: 0.9118 - auc_3: 0.9833 - recall_3: 0.8592
---------------TEST METRICS----------------------
jaccard_index 0.7983843980108809
test_sensitivity 0.8660448507929871
test_specifitivity 0.9714717091167384
test_accuracy 0.9478981072175587
test_precision 0.8973609648462966
test_jaccard_score 0.7983843980108809
test_dicecoef 0.8814248382480485
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-151528.h5
[0. 0. 0. 0. 0.] [0.94789811 0.7983844  0.89736096 0.86604485 0.97147171]

-------------------------
Rep: 1
-------------------------

2021-09-24 15:15:29.153216: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 15:15:29.153342: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_9"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_5 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_76 (Conv2D)              (None, 256, 256, 32) 320         input_5[0][0]
__________________________________________________________________________________________________
conv2d_77 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_76[0][0]
__________________________________________________________________________________________________
max_pooling2d_16 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_77[0][0]
__________________________________________________________________________________________________
conv2d_78 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_16[0][0]
__________________________________________________________________________________________________
conv2d_79 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_78[0][0]
__________________________________________________________________________________________________
max_pooling2d_17 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_79[0][0]
__________________________________________________________________________________________________
conv2d_80 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_17[0][0]
__________________________________________________________________________________________________
conv2d_81 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_80[0][0]
__________________________________________________________________________________________________
max_pooling2d_18 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_81[0][0]
__________________________________________________________________________________________________
conv2d_82 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_18[0][0]
__________________________________________________________________________________________________
conv2d_83 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_82[0][0]
__________________________________________________________________________________________________
max_pooling2d_19 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_83[0][0]
__________________________________________________________________________________________________
conv2d_84 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_19[0][0]
__________________________________________________________________________________________________
conv2d_85 (Conv2D)              (None, 16, 16, 512)  2359808     conv2d_84[0][0]
__________________________________________________________________________________________________
up_sampling2d_16 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_85[0][0]
__________________________________________________________________________________________________
concatenate_16 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_16[0][0]
                                                                 conv2d_83[0][0]
__________________________________________________________________________________________________
conv2d_86 (Conv2D)              (None, 32, 32, 256)  1769728     concatenate_16[0][0]
__________________________________________________________________________________________________
conv2d_87 (Conv2D)              (None, 32, 32, 256)  590080      conv2d_86[0][0]
__________________________________________________________________________________________________
up_sampling2d_17 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_87[0][0]
__________________________________________________________________________________________________
concatenate_17 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_17[0][0]
                                                                 conv2d_81[0][0]
__________________________________________________________________________________________________
conv2d_88 (Conv2D)              (None, 64, 64, 128)  442496      concatenate_17[0][0]
__________________________________________________________________________________________________
conv2d_89 (Conv2D)              (None, 64, 64, 128)  147584      conv2d_88[0][0]
__________________________________________________________________________________________________
up_sampling2d_18 (UpSampling2D) (None, 128, 128, 128 0           conv2d_89[0][0]
__________________________________________________________________________________________________
concatenate_18 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_18[0][0]
                                                                 conv2d_79[0][0]
__________________________________________________________________________________________________
conv2d_90 (Conv2D)              (None, 128, 128, 64) 110656      concatenate_18[0][0]
__________________________________________________________________________________________________
conv2d_91 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_90[0][0]
__________________________________________________________________________________________________
up_sampling2d_19 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_91[0][0]
__________________________________________________________________________________________________
concatenate_19 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_19[0][0]
                                                                 conv2d_77[0][0]
__________________________________________________________________________________________________
conv2d_92 (Conv2D)              (None, 256, 256, 32) 27680       concatenate_19[0][0]
__________________________________________________________________________________________________
conv2d_93 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_92[0][0]
__________________________________________________________________________________________________
conv2d_94 (Conv2D)              (None, 256, 256, 1)  33          conv2d_93[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6917 - acc: 0.7546 - precision_4: 0.1661 - auc_4: 0.4816 - recall_4: 0.11532021-09-24 15:15:37.305758: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 15:15:37.305843: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 15:15:37.815860: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 15:15:37.826434: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37
2021-09-24 15:15:37.830041: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.trace.json.gz
2021-09-24 15:15:37.856627: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37
2021-09-24 15:15:37.866313: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.memory_profile.json.gz
2021-09-24 15:15:37.880922: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37Dumped tool data for xplane.pb to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-151529/train/plugins/profile/2021_09_24_15_15_37/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:12 - loss: 0.6848 - acc: 0.6944 - precision_4: 0.1661 - auc_4: 0.4125 - recall_4: 0.0361WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2062s vs `on_train_batch_end` time: 0.3704s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4151 - acc: 0.8090 - precision_4: 0.7316 - auc_4: 0.7983 - recall_4: 0.1923 - val_loss: 0.3851 - val_acc: 0.8598 - val_precision_4: 0.7559 - val_auc_4: 0.8434 - val_recall_4: 0.5019
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3420 - acc: 0.8534 - precision_4: 0.7904 - auc_4: 0.8641 - recall_4: 0.4489 - val_loss: 0.4062 - val_acc: 0.8594 - val_precision_4: 0.8294 - val_auc_4: 0.8242 - val_recall_4: 0.4241
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3255 - acc: 0.8615 - precision_4: 0.8086 - auc_4: 0.8754 - recall_4: 0.4811 - val_loss: 0.3525 - val_acc: 0.8649 - val_precision_4: 0.7569 - val_auc_4: 0.8671 - val_recall_4: 0.5429
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3147 - acc: 0.8706 - precision_4: 0.8225 - auc_4: 0.8833 - recall_4: 0.5254 - val_loss: 0.3375 - val_acc: 0.8450 - val_precision_4: 0.6037 - val_auc_4: 0.8961 - val_recall_4: 0.7750
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3193 - acc: 0.8705 - precision_4: 0.8150 - auc_4: 0.8835 - recall_4: 0.5336 - val_loss: 0.3091 - val_acc: 0.8833 - val_precision_4: 0.8372 - val_auc_4: 0.8850 - val_recall_4: 0.5379
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2926 - acc: 0.8798 - precision_4: 0.8263 - auc_4: 0.9017 - recall_4: 0.5766 - val_loss: 0.2785 - val_acc: 0.8825 - val_precision_4: 0.7309 - val_auc_4: 0.9256 - val_recall_4: 0.7250
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2347 - acc: 0.9055 - precision_4: 0.8479 - auc_4: 0.9372 - recall_4: 0.7063 - val_loss: 0.1988 - val_acc: 0.9274 - val_precision_4: 0.8917 - val_auc_4: 0.9526 - val_recall_4: 0.7438
Epoch 8/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2129 - acc: 0.9136 - precision_4: 0.8608 - auc_4: 0.9495 - recall_4: 0.7358 - val_loss: 0.2011 - val_acc: 0.9214 - val_precision_4: 0.8068 - val_auc_4: 0.9657 - val_recall_4: 0.8435
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1945 - acc: 0.9193 - precision_4: 0.8678 - auc_4: 0.9588 - recall_4: 0.7611 - val_loss: 0.2115 - val_acc: 0.9230 - val_precision_4: 0.9482 - val_auc_4: 0.9625 - val_recall_4: 0.6681
Epoch 10/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1868 - acc: 0.9222 - precision_4: 0.8736 - auc_4: 0.9623 - recall_4: 0.7685 - val_loss: 0.1830 - val_acc: 0.9267 - val_precision_4: 0.8235 - val_auc_4: 0.9651 - val_recall_4: 0.8358
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1825 - acc: 0.9245 - precision_4: 0.8811 - auc_4: 0.9639 - recall_4: 0.7747 - val_loss: 0.1691 - val_acc: 0.9303 - val_precision_4: 0.8516 - val_auc_4: 0.9694 - val_recall_4: 0.8118
Epoch 12/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1739 - acc: 0.9270 - precision_4: 0.8804 - auc_4: 0.9676 - recall_4: 0.7879 - val_loss: 0.1754 - val_acc: 0.9274 - val_precision_4: 0.7994 - val_auc_4: 0.9720 - val_recall_4: 0.8757
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1673 - acc: 0.9301 - precision_4: 0.8868 - auc_4: 0.9707 - recall_4: 0.7971 - val_loss: 0.1545 - val_acc: 0.9379 - val_precision_4: 0.8243 - val_auc_4: 0.9808 - val_recall_4: 0.9029
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1635 - acc: 0.9306 - precision_4: 0.8867 - auc_4: 0.9715 - recall_4: 0.8002 - val_loss: 0.1528 - val_acc: 0.9378 - val_precision_4: 0.8565 - val_auc_4: 0.9746 - val_recall_4: 0.8393
Epoch 15/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1562 - acc: 0.9331 - precision_4: 0.8937 - auc_4: 0.9748 - recall_4: 0.8037 - val_loss: 0.1855 - val_acc: 0.9208 - val_precision_4: 0.7674 - val_auc_4: 0.9786 - val_recall_4: 0.9244
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1580 - acc: 0.9323 - precision_4: 0.8880 - auc_4: 0.9740 - recall_4: 0.8073 - val_loss: 0.1444 - val_acc: 0.9444 - val_precision_4: 0.9390 - val_auc_4: 0.9769 - val_recall_4: 0.7689
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1636 - acc: 0.9304 - precision_4: 0.8891 - auc_4: 0.9723 - recall_4: 0.7959 - val_loss: 0.1611 - val_acc: 0.9327 - val_precision_4: 0.8510 - val_auc_4: 0.9751 - val_recall_4: 0.8412
Epoch 18/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1539 - acc: 0.9349 - precision_4: 0.8939 - auc_4: 0.9750 - recall_4: 0.8146 - val_loss: 0.1401 - val_acc: 0.9493 - val_precision_4: 0.9234 - val_auc_4: 0.9783 - val_recall_4: 0.8232
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1509 - acc: 0.9360 - precision_4: 0.8925 - auc_4: 0.9769 - recall_4: 0.8207 - val_loss: 0.1468 - val_acc: 0.9421 - val_precision_4: 0.9259 - val_auc_4: 0.9788 - val_recall_4: 0.8012
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1500 - acc: 0.9353 - precision_4: 0.8965 - auc_4: 0.9769 - recall_4: 0.8144 - val_loss: 0.1340 - val_acc: 0.9459 - val_precision_4: 0.8534 - val_auc_4: 0.9800 - val_recall_4: 0.8660
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1529 - acc: 0.9348 - precision_4: 0.8880 - auc_4: 0.9757 - recall_4: 0.8219 - val_loss: 0.1416 - val_acc: 0.9444 - val_precision_4: 0.8753 - val_auc_4: 0.9829 - val_recall_4: 0.8873
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1437 - acc: 0.9383 - precision_4: 0.8973 - auc_4: 0.9783 - recall_4: 0.8288 - val_loss: 0.1447 - val_acc: 0.9436 - val_precision_4: 0.8570 - val_auc_4: 0.9803 - val_recall_4: 0.8670
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1443 - acc: 0.9381 - precision_4: 0.8988 - auc_4: 0.9782 - recall_4: 0.8253 - val_loss: 0.1692 - val_acc: 0.9304 - val_precision_4: 0.7962 - val_auc_4: 0.9780 - val_recall_4: 0.9013
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1438 - acc: 0.9383 - precision_4: 0.8998 - auc_4: 0.9784 - recall_4: 0.8252 - val_loss: 0.1468 - val_acc: 0.9395 - val_precision_4: 0.8326 - val_auc_4: 0.9807 - val_recall_4: 0.9011
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1484 - acc: 0.9365 - precision_4: 0.8941 - auc_4: 0.9771 - recall_4: 0.8233 - val_loss: 0.1534 - val_acc: 0.9377 - val_precision_4: 0.8055 - val_auc_4: 0.9845 - val_recall_4: 0.9227
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1404 - acc: 0.9389 - precision_4: 0.8996 - auc_4: 0.9798 - recall_4: 0.8287 - val_loss: 0.1325 - val_acc: 0.9470 - val_precision_4: 0.8964 - val_auc_4: 0.9808 - val_recall_4: 0.8462
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1417 - acc: 0.9394 - precision_4: 0.9049 - auc_4: 0.9791 - recall_4: 0.8262 - val_loss: 0.1326 - val_acc: 0.9487 - val_precision_4: 0.8898 - val_auc_4: 0.9833 - val_recall_4: 0.8662
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1398 - acc: 0.9398 - precision_4: 0.9034 - auc_4: 0.9801 - recall_4: 0.8298 - val_loss: 0.1862 - val_acc: 0.9309 - val_precision_4: 0.7842 - val_auc_4: 0.9819 - val_recall_4: 0.9310
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1434 - acc: 0.9387 - precision_4: 0.8962 - auc_4: 0.9786 - recall_4: 0.8330 - val_loss: 0.1934 - val_acc: 0.9138 - val_precision_4: 0.7450 - val_auc_4: 0.9802 - val_recall_4: 0.9398
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1384 - acc: 0.9399 - precision_4: 0.8969 - auc_4: 0.9800 - recall_4: 0.8398 - val_loss: 0.1184 - val_acc: 0.9538 - val_precision_4: 0.9328 - val_auc_4: 0.9859 - val_recall_4: 0.8424
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1382 - acc: 0.9398 - precision_4: 0.8991 - auc_4: 0.9803 - recall_4: 0.8354 - val_loss: 0.1561 - val_acc: 0.9360 - val_precision_4: 0.8064 - val_auc_4: 0.9813 - val_recall_4: 0.9142
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1403 - acc: 0.9400 - precision_4: 0.8978 - auc_4: 0.9798 - recall_4: 0.8390 - val_loss: 0.1464 - val_acc: 0.9425 - val_precision_4: 0.8385 - val_auc_4: 0.9819 - val_recall_4: 0.9052
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1405 - acc: 0.9393 - precision_4: 0.9000 - auc_4: 0.9797 - recall_4: 0.8301 - val_loss: 0.1264 - val_acc: 0.9509 - val_precision_4: 0.8870 - val_auc_4: 0.9833 - val_recall_4: 0.8792
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1396 - acc: 0.9403 - precision_4: 0.9040 - auc_4: 0.9800 - recall_4: 0.8308 - val_loss: 0.1434 - val_acc: 0.9445 - val_precision_4: 0.8385 - val_auc_4: 0.9816 - val_recall_4: 0.8878
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1329 - acc: 0.9420 - precision_4: 0.9030 - auc_4: 0.9817 - recall_4: 0.8414 - val_loss: 0.1397 - val_acc: 0.9439 - val_precision_4: 0.8529 - val_auc_4: 0.9832 - val_recall_4: 0.9072
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1341 - acc: 0.9422 - precision_4: 0.9042 - auc_4: 0.9814 - recall_4: 0.8400 - val_loss: 0.1373 - val_acc: 0.9445 - val_precision_4: 0.8702 - val_auc_4: 0.9789 - val_recall_4: 0.8688
Epoch 37/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1346 - acc: 0.9419 - precision_4: 0.9007 - auc_4: 0.9813 - recall_4: 0.8442 - val_loss: 0.1518 - val_acc: 0.9460 - val_precision_4: 0.9577 - val_auc_4: 0.9808 - val_recall_4: 0.7844
Epoch 38/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1306 - acc: 0.9433 - precision_4: 0.9061 - auc_4: 0.9824 - recall_4: 0.8461 - val_loss: 0.1325 - val_acc: 0.9475 - val_precision_4: 0.8657 - val_auc_4: 0.9814 - val_recall_4: 0.8869
Epoch 39/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1334 - acc: 0.9422 - precision_4: 0.9051 - auc_4: 0.9815 - recall_4: 0.8404 - val_loss: 0.1380 - val_acc: 0.9451 - val_precision_4: 0.8975 - val_auc_4: 0.9795 - val_recall_4: 0.8463
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1584 - acc: 0.9351 - precision_4: 0.8915 - auc_4: 0.9756 - recall_4: 0.8205 - val_loss: 0.2091 - val_acc: 0.9227 - val_precision_4: 0.9569 - val_auc_4: 0.9477 - val_recall_4: 0.6399
Epoch 41/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1729 - acc: 0.9270 - precision_4: 0.8859 - auc_4: 0.9686 - recall_4: 0.7822 - val_loss: 0.1911 - val_acc: 0.9256 - val_precision_4: 0.8140 - val_auc_4: 0.9675 - val_recall_4: 0.8561
Epoch 42/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1506 - acc: 0.9356 - precision_4: 0.8973 - auc_4: 0.9763 - recall_4: 0.8161 - val_loss: 0.1413 - val_acc: 0.9463 - val_precision_4: 0.8856 - val_auc_4: 0.9798 - val_recall_4: 0.8551
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1441 - acc: 0.9380 - precision_4: 0.8951 - auc_4: 0.9787 - recall_4: 0.8324 - val_loss: 0.1653 - val_acc: 0.9359 - val_precision_4: 0.8585 - val_auc_4: 0.9733 - val_recall_4: 0.8441
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1359 - acc: 0.9415 - precision_4: 0.9023 - auc_4: 0.9810 - recall_4: 0.8400Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1359 - acc: 0.9415 - precision_4: 0.9023 - auc_4: 0.9810 - recall_4: 0.8400 - val_loss: 0.1249 - val_acc: 0.9508 - val_precision_4: 0.9052 - val_auc_4: 0.9818 - val_recall_4: 0.8578
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1304 - acc: 0.9490 - precision_4: 0.9176 - auc_4: 0.9829 - recall_4: 0.8482
---------------TEST METRICS----------------------
jaccard_index 0.798216455096388
test_sensitivity 0.8558203161203878
test_specifitivity 0.9750477724906698
test_accuracy 0.9483883363980774
test_precision 0.9080702278723866
test_jaccard_score 0.798216455096388
test_dicecoef 0.8811713993523648
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-155707.h5
[0.94789811 0.7983844  0.89736096 0.86604485 0.97147171] [0.94838834 0.79821646 0.90807023 0.85582032 0.97504777]

-------------------------
Rep: 2
-------------------------

2021-09-24 15:57:08.592003: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 15:57:08.592082: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810149085967636,
Validation samples: 383, channel mean: 0.5863431984931646
Model built.
Model: "functional_11"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_6 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_95 (Conv2D)              (None, 256, 256, 32) 320         input_6[0][0]
__________________________________________________________________________________________________
conv2d_96 (Conv2D)              (None, 256, 256, 32) 9248        conv2d_95[0][0]
__________________________________________________________________________________________________
max_pooling2d_20 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_96[0][0]
__________________________________________________________________________________________________
conv2d_97 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_20[0][0]
__________________________________________________________________________________________________
conv2d_98 (Conv2D)              (None, 128, 128, 64) 36928       conv2d_97[0][0]
__________________________________________________________________________________________________
max_pooling2d_21 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_98[0][0]
__________________________________________________________________________________________________
conv2d_99 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_21[0][0]
__________________________________________________________________________________________________
conv2d_100 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_99[0][0]
__________________________________________________________________________________________________
max_pooling2d_22 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_100[0][0]
__________________________________________________________________________________________________
conv2d_101 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_22[0][0]
__________________________________________________________________________________________________
conv2d_102 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_101[0][0]
__________________________________________________________________________________________________
max_pooling2d_23 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_102[0][0]
__________________________________________________________________________________________________
conv2d_103 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_23[0][0]
__________________________________________________________________________________________________
conv2d_104 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_103[0][0]
__________________________________________________________________________________________________
up_sampling2d_20 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_104[0][0]
__________________________________________________________________________________________________
concatenate_20 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_20[0][0]
                                                                 conv2d_102[0][0]
__________________________________________________________________________________________________
conv2d_105 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_20[0][0]
__________________________________________________________________________________________________
conv2d_106 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_105[0][0]
__________________________________________________________________________________________________
up_sampling2d_21 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_106[0][0]
__________________________________________________________________________________________________
concatenate_21 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_21[0][0]
                                                                 conv2d_100[0][0]
__________________________________________________________________________________________________
conv2d_107 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_21[0][0]
__________________________________________________________________________________________________
conv2d_108 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_107[0][0]
__________________________________________________________________________________________________
up_sampling2d_22 (UpSampling2D) (None, 128, 128, 128 0           conv2d_108[0][0]
__________________________________________________________________________________________________
concatenate_22 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_22[0][0]
                                                                 conv2d_98[0][0]
__________________________________________________________________________________________________
conv2d_109 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_22[0][0]
__________________________________________________________________________________________________
conv2d_110 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_109[0][0]
__________________________________________________________________________________________________
up_sampling2d_23 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_110[0][0]
__________________________________________________________________________________________________
concatenate_23 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_23[0][0]
                                                                 conv2d_96[0][0]
__________________________________________________________________________________________________
conv2d_111 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_23[0][0]
__________________________________________________________________________________________________
conv2d_112 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_111[0][0]
__________________________________________________________________________________________________
conv2d_113 (Conv2D)             (None, 256, 256, 1)  33          conv2d_112[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6985 - acc: 0.1584 - precision_5: 0.1678 - auc_5: 0.5264 - recall_5: 1.00002021-09-24 15:57:16.286230: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 15:57:16.286328: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 15:57:16.782090: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 15:57:16.793118: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16
2021-09-24 15:57:16.796157: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.trace.json.gz
2021-09-24 15:57:16.821525: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16
2021-09-24 15:57:16.829546: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.memory_profile.json.gz
2021-09-24 15:57:16.846537: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16Dumped tool data for xplane.pb to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-155708/train/plugins/profile/2021_09_24_15_57_16/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:10 - loss: 0.6947 - acc: 0.3959 - precision_5: 0.1676 - auc_5: 0.4187 - recall_5: 0.3133WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2094s vs `on_train_batch_end` time: 0.3526s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4494 - acc: 0.7876 - precision_5: 0.6275 - auc_5: 0.7602 - recall_5: 0.0514 - val_loss: 0.4505 - val_acc: 0.8118 - val_precision_5: 0.5564 - val_auc_5: 0.7923 - val_recall_5: 0.5613
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3647 - acc: 0.8473 - precision_5: 0.7873 - auc_5: 0.8415 - recall_5: 0.4112 - val_loss: 0.3584 - val_acc: 0.8619 - val_precision_5: 0.8687 - val_auc_5: 0.8403 - val_recall_5: 0.4107
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3388 - acc: 0.8566 - precision_5: 0.7828 - auc_5: 0.8665 - recall_5: 0.4774 - val_loss: 0.3460 - val_acc: 0.8656 - val_precision_5: 0.7800 - val_auc_5: 0.8606 - val_recall_5: 0.5182
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3191 - acc: 0.8661 - precision_5: 0.8175 - auc_5: 0.8796 - recall_5: 0.5024 - val_loss: 0.3181 - val_acc: 0.8797 - val_precision_5: 0.8422 - val_auc_5: 0.8773 - val_recall_5: 0.5297
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3074 - acc: 0.8752 - precision_5: 0.8492 - auc_5: 0.8862 - recall_5: 0.5272 - val_loss: 0.3228 - val_acc: 0.8738 - val_precision_5: 0.7556 - val_auc_5: 0.8776 - val_recall_5: 0.5720
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2964 - acc: 0.8798 - precision_5: 0.8465 - auc_5: 0.8947 - recall_5: 0.5533 - val_loss: 0.3110 - val_acc: 0.8789 - val_precision_5: 0.7723 - val_auc_5: 0.8920 - val_recall_5: 0.6262
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2845 - acc: 0.8871 - precision_5: 0.8524 - auc_5: 0.9017 - recall_5: 0.5908 - val_loss: 0.2888 - val_acc: 0.8944 - val_precision_5: 0.8328 - val_auc_5: 0.8904 - val_recall_5: 0.6201
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2821 - acc: 0.8897 - precision_5: 0.8600 - auc_5: 0.9014 - recall_5: 0.5975 - val_loss: 0.3167 - val_acc: 0.8823 - val_precision_5: 0.8374 - val_auc_5: 0.8761 - val_recall_5: 0.5749
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2762 - acc: 0.8911 - precision_5: 0.8568 - auc_5: 0.9047 - recall_5: 0.6097 - val_loss: 0.3038 - val_acc: 0.8895 - val_precision_5: 0.8234 - val_auc_5: 0.8920 - val_recall_5: 0.6003
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2710 - acc: 0.8938 - precision_5: 0.8556 - auc_5: 0.9083 - recall_5: 0.6251 - val_loss: 0.2647 - val_acc: 0.9035 - val_precision_5: 0.8665 - val_auc_5: 0.9147 - val_recall_5: 0.6478
Epoch 11/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2584 - acc: 0.8968 - precision_5: 0.8530 - auc_5: 0.9215 - recall_5: 0.6500 - val_loss: 0.3992 - val_acc: 0.8963 - val_precision_5: 0.7662 - val_auc_5: 0.9132 - val_recall_5: 0.7334
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2447 - acc: 0.9044 - precision_5: 0.8606 - auc_5: 0.9284 - recall_5: 0.6837 - val_loss: 0.2661 - val_acc: 0.8958 - val_precision_5: 0.7045 - val_auc_5: 0.9560 - val_recall_5: 0.8716
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2188 - acc: 0.9128 - precision_5: 0.8695 - auc_5: 0.9456 - recall_5: 0.7207 - val_loss: 0.2498 - val_acc: 0.9013 - val_precision_5: 0.7404 - val_auc_5: 0.9491 - val_recall_5: 0.8316
Epoch 14/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2111 - acc: 0.9143 - precision_5: 0.8716 - auc_5: 0.9497 - recall_5: 0.7268 - val_loss: 0.1946 - val_acc: 0.9304 - val_precision_5: 0.9001 - val_auc_5: 0.9543 - val_recall_5: 0.7452
Epoch 15/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1888 - acc: 0.9224 - precision_5: 0.8828 - auc_5: 0.9610 - recall_5: 0.7577 - val_loss: 0.2159 - val_acc: 0.9168 - val_precision_5: 0.7879 - val_auc_5: 0.9611 - val_recall_5: 0.8569
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1778 - acc: 0.9260 - precision_5: 0.8856 - auc_5: 0.9663 - recall_5: 0.7754 - val_loss: 0.1571 - val_acc: 0.9377 - val_precision_5: 0.9141 - val_auc_5: 0.9720 - val_recall_5: 0.7566
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1774 - acc: 0.9258 - precision_5: 0.8873 - auc_5: 0.9666 - recall_5: 0.7733 - val_loss: 0.1643 - val_acc: 0.9359 - val_precision_5: 0.8976 - val_auc_5: 0.9719 - val_recall_5: 0.7998
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1705 - acc: 0.9286 - precision_5: 0.8931 - auc_5: 0.9694 - recall_5: 0.7814 - val_loss: 0.1588 - val_acc: 0.9405 - val_precision_5: 0.8612 - val_auc_5: 0.9715 - val_recall_5: 0.8492
Epoch 19/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1655 - acc: 0.9310 - precision_5: 0.8897 - auc_5: 0.9714 - recall_5: 0.7974 - val_loss: 0.1503 - val_acc: 0.9415 - val_precision_5: 0.8968 - val_auc_5: 0.9771 - val_recall_5: 0.8297
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1622 - acc: 0.9313 - precision_5: 0.8931 - auc_5: 0.9731 - recall_5: 0.7968 - val_loss: 0.1341 - val_acc: 0.9479 - val_precision_5: 0.8590 - val_auc_5: 0.9804 - val_recall_5: 0.8706
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1626 - acc: 0.9315 - precision_5: 0.8934 - auc_5: 0.9724 - recall_5: 0.7966 - val_loss: 0.1527 - val_acc: 0.9410 - val_precision_5: 0.8633 - val_auc_5: 0.9807 - val_recall_5: 0.8867
Epoch 22/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1544 - acc: 0.9347 - precision_5: 0.8998 - auc_5: 0.9755 - recall_5: 0.8066 - val_loss: 0.1534 - val_acc: 0.9408 - val_precision_5: 0.8516 - val_auc_5: 0.9782 - val_recall_5: 0.8584
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1559 - acc: 0.9343 - precision_5: 0.8979 - auc_5: 0.9743 - recall_5: 0.8057 - val_loss: 0.1606 - val_acc: 0.9373 - val_precision_5: 0.8269 - val_auc_5: 0.9778 - val_recall_5: 0.8896
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1525 - acc: 0.9350 - precision_5: 0.9009 - auc_5: 0.9761 - recall_5: 0.8067 - val_loss: 0.1638 - val_acc: 0.9377 - val_precision_5: 0.8356 - val_auc_5: 0.9783 - val_recall_5: 0.8861
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1543 - acc: 0.9351 - precision_5: 0.8980 - auc_5: 0.9753 - recall_5: 0.8113 - val_loss: 0.1951 - val_acc: 0.9273 - val_precision_5: 0.7746 - val_auc_5: 0.9770 - val_recall_5: 0.9168
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1474 - acc: 0.9370 - precision_5: 0.9011 - auc_5: 0.9776 - recall_5: 0.8166 - val_loss: 0.1363 - val_acc: 0.9466 - val_precision_5: 0.9023 - val_auc_5: 0.9790 - val_recall_5: 0.8370
Epoch 27/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1511 - acc: 0.9364 - precision_5: 0.9029 - auc_5: 0.9763 - recall_5: 0.8126 - val_loss: 0.1417 - val_acc: 0.9437 - val_precision_5: 0.8692 - val_auc_5: 0.9798 - val_recall_5: 0.8656
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1471 - acc: 0.9371 - precision_5: 0.9029 - auc_5: 0.9779 - recall_5: 0.8159 - val_loss: 0.1680 - val_acc: 0.9446 - val_precision_5: 0.8774 - val_auc_5: 0.9763 - val_recall_5: 0.8596
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1487 - acc: 0.9370 - precision_5: 0.9036 - auc_5: 0.9770 - recall_5: 0.8154 - val_loss: 0.1794 - val_acc: 0.9217 - val_precision_5: 0.7820 - val_auc_5: 0.9760 - val_recall_5: 0.9058
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1488 - acc: 0.9360 - precision_5: 0.8989 - auc_5: 0.9773 - recall_5: 0.8159 - val_loss: 0.1264 - val_acc: 0.9502 - val_precision_5: 0.9066 - val_auc_5: 0.9842 - val_recall_5: 0.8527
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1462 - acc: 0.9376 - precision_5: 0.9038 - auc_5: 0.9782 - recall_5: 0.8178 - val_loss: 0.1748 - val_acc: 0.9277 - val_precision_5: 0.7818 - val_auc_5: 0.9764 - val_recall_5: 0.9090
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1488 - acc: 0.9361 - precision_5: 0.8998 - auc_5: 0.9770 - recall_5: 0.8153 - val_loss: 0.1407 - val_acc: 0.9481 - val_precision_5: 0.8831 - val_auc_5: 0.9827 - val_recall_5: 0.8726
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1459 - acc: 0.9377 - precision_5: 0.9010 - auc_5: 0.9781 - recall_5: 0.8213 - val_loss: 0.1283 - val_acc: 0.9490 - val_precision_5: 0.8710 - val_auc_5: 0.9844 - val_recall_5: 0.8898
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1479 - acc: 0.9376 - precision_5: 0.9011 - auc_5: 0.9771 - recall_5: 0.8202 - val_loss: 0.1404 - val_acc: 0.9492 - val_precision_5: 0.8801 - val_auc_5: 0.9809 - val_recall_5: 0.8574
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1420 - acc: 0.9395 - precision_5: 0.9055 - auc_5: 0.9790 - recall_5: 0.8253 - val_loss: 0.1471 - val_acc: 0.9410 - val_precision_5: 0.8459 - val_auc_5: 0.9825 - val_recall_5: 0.9023
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1407 - acc: 0.9401 - precision_5: 0.9056 - auc_5: 0.9794 - recall_5: 0.8273 - val_loss: 0.1261 - val_acc: 0.9486 - val_precision_5: 0.8907 - val_auc_5: 0.9831 - val_recall_5: 0.8644
Epoch 37/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1391 - acc: 0.9401 - precision_5: 0.9035 - auc_5: 0.9801 - recall_5: 0.8311 - val_loss: 0.1479 - val_acc: 0.9467 - val_precision_5: 0.9299 - val_auc_5: 0.9785 - val_recall_5: 0.8145
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1420 - acc: 0.9398 - precision_5: 0.9068 - auc_5: 0.9793 - recall_5: 0.8268 - val_loss: 0.1384 - val_acc: 0.9463 - val_precision_5: 0.8853 - val_auc_5: 0.9795 - val_recall_5: 0.8544
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1391 - acc: 0.9399 - precision_5: 0.9071 - auc_5: 0.9803 - recall_5: 0.8255 - val_loss: 0.1331 - val_acc: 0.9476 - val_precision_5: 0.9200 - val_auc_5: 0.9819 - val_recall_5: 0.8333
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1334 - acc: 0.9418 - precision_5: 0.9085 - auc_5: 0.9821 - recall_5: 0.8357 - val_loss: 0.1258 - val_acc: 0.9521 - val_precision_5: 0.9022 - val_auc_5: 0.9804 - val_recall_5: 0.8510
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1373 - acc: 0.9412 - precision_5: 0.9049 - auc_5: 0.9806 - recall_5: 0.8360 - val_loss: 0.1499 - val_acc: 0.9384 - val_precision_5: 0.8486 - val_auc_5: 0.9797 - val_recall_5: 0.8750
Epoch 42/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1322 - acc: 0.9426 - precision_5: 0.9088 - auc_5: 0.9824 - recall_5: 0.8395 - val_loss: 0.1464 - val_acc: 0.9448 - val_precision_5: 0.8562 - val_auc_5: 0.9828 - val_recall_5: 0.8865
Epoch 43/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1388 - acc: 0.9400 - precision_5: 0.9048 - auc_5: 0.9807 - recall_5: 0.8309 - val_loss: 0.1404 - val_acc: 0.9481 - val_precision_5: 0.9227 - val_auc_5: 0.9802 - val_recall_5: 0.8309
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1331 - acc: 0.9428 - precision_5: 0.9109 - auc_5: 0.9822 - recall_5: 0.8365 - val_loss: 0.1272 - val_acc: 0.9522 - val_precision_5: 0.9143 - val_auc_5: 0.9819 - val_recall_5: 0.8547
Epoch 45/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1354 - acc: 0.9421 - precision_5: 0.9093 - auc_5: 0.9812 - recall_5: 0.8350 - val_loss: 0.1360 - val_acc: 0.9460 - val_precision_5: 0.8878 - val_auc_5: 0.9807 - val_recall_5: 0.8565
Epoch 46/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1334 - acc: 0.9424 - precision_5: 0.9088 - auc_5: 0.9819 - recall_5: 0.8365 - val_loss: 0.1541 - val_acc: 0.9400 - val_precision_5: 0.8399 - val_auc_5: 0.9764 - val_recall_5: 0.8831
Epoch 47/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1336 - acc: 0.9420 - precision_5: 0.9100 - auc_5: 0.9818 - recall_5: 0.8339 - val_loss: 0.1282 - val_acc: 0.9509 - val_precision_5: 0.9202 - val_auc_5: 0.9825 - val_recall_5: 0.8440
Epoch 48/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1367 - acc: 0.9414 - precision_5: 0.9040 - auc_5: 0.9806 - recall_5: 0.8384 - val_loss: 0.1268 - val_acc: 0.9501 - val_precision_5: 0.9360 - val_auc_5: 0.9827 - val_recall_5: 0.8180
Epoch 49/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1348 - acc: 0.9421 - precision_5: 0.9121 - auc_5: 0.9813 - recall_5: 0.8313 - val_loss: 0.1562 - val_acc: 0.9359 - val_precision_5: 0.8164 - val_auc_5: 0.9810 - val_recall_5: 0.9008
Epoch 50/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1309 - acc: 0.9433 - precision_5: 0.9110 - auc_5: 0.9825 - recall_5: 0.8384 - val_loss: 0.1275 - val_acc: 0.9480 - val_precision_5: 0.8677 - val_auc_5: 0.9828 - val_recall_5: 0.8921
Epoch 51/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1370 - acc: 0.9416 - precision_5: 0.9085 - auc_5: 0.9807 - recall_5: 0.8341 - val_loss: 0.1346 - val_acc: 0.9472 - val_precision_5: 0.8831 - val_auc_5: 0.9813 - val_recall_5: 0.8627
Epoch 52/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1351 - acc: 0.9421 - precision_5: 0.9039 - auc_5: 0.9812 - recall_5: 0.8416 - val_loss: 0.1770 - val_acc: 0.9332 - val_precision_5: 0.8108 - val_auc_5: 0.9787 - val_recall_5: 0.8966
Epoch 53/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1279 - acc: 0.9449 - precision_5: 0.9150 - auc_5: 0.9830 - recall_5: 0.8426 - val_loss: 0.1319 - val_acc: 0.9489 - val_precision_5: 0.8877 - val_auc_5: 0.9817 - val_recall_5: 0.8673
Epoch 54/100
254/254 [==============================] - ETA: 0s - loss: 0.1306 - acc: 0.9434 - precision_5: 0.9109 - auc_5: 0.9827 - recall_5: 0.8414Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1306 - acc: 0.9434 - precision_5: 0.9109 - auc_5: 0.9827 - recall_5: 0.8414 - val_loss: 0.1362 - val_acc: 0.9457 - val_precision_5: 0.8682 - val_auc_5: 0.9820 - val_recall_5: 0.8788
Epoch 00054: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1427 - acc: 0.9439 - precision_5: 0.8959 - auc_5: 0.9794 - recall_5: 0.8476
---------------TEST METRICS----------------------
jaccard_index 0.7829739742544878
test_sensitivity 0.8530403422891177
test_specifitivity 0.968223618814169
test_accuracy 0.9424684673336381
test_precision 0.8854701228263988
test_jaccard_score 0.7829739742544878
test_dicecoef 0.8689527636664863
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-164805.h5
[1.89628644 1.59660085 1.80543119 1.72186517 1.94651948] [0.94246847 0.78297397 0.88547012 0.85304034 0.96822362]

-------------------------
Averaged metrics for Baseline + Augumentations - lesion: [0.94625164 0.79319161 0.89696711 0.85830184 0.97158103]
-------------------------


-------------------------
RUN: Baseline + Histogram Equalization - lesion, PARAMS: {'histogram_equalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 16:48:05.935030: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 16:48:05.935144: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_13"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_7 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_114 (Conv2D)             (None, 256, 256, 32) 320         input_7[0][0]
__________________________________________________________________________________________________
conv2d_115 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_114[0][0]
__________________________________________________________________________________________________
max_pooling2d_24 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_115[0][0]
__________________________________________________________________________________________________
conv2d_116 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_24[0][0]
__________________________________________________________________________________________________
conv2d_117 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_116[0][0]
__________________________________________________________________________________________________
max_pooling2d_25 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_117[0][0]
__________________________________________________________________________________________________
conv2d_118 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_25[0][0]
__________________________________________________________________________________________________
conv2d_119 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_118[0][0]
__________________________________________________________________________________________________
max_pooling2d_26 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_119[0][0]
__________________________________________________________________________________________________
conv2d_120 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_26[0][0]
__________________________________________________________________________________________________
conv2d_121 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_120[0][0]
__________________________________________________________________________________________________
max_pooling2d_27 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_121[0][0]
__________________________________________________________________________________________________
conv2d_122 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_27[0][0]
__________________________________________________________________________________________________
conv2d_123 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_122[0][0]
__________________________________________________________________________________________________
up_sampling2d_24 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_123[0][0]
__________________________________________________________________________________________________
concatenate_24 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_24[0][0]
                                                                 conv2d_121[0][0]
__________________________________________________________________________________________________
conv2d_124 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_24[0][0]
__________________________________________________________________________________________________
conv2d_125 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_124[0][0]
__________________________________________________________________________________________________
up_sampling2d_25 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_125[0][0]
__________________________________________________________________________________________________
concatenate_25 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_25[0][0]
                                                                 conv2d_119[0][0]
__________________________________________________________________________________________________
conv2d_126 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_25[0][0]
__________________________________________________________________________________________________
conv2d_127 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_126[0][0]
__________________________________________________________________________________________________
up_sampling2d_26 (UpSampling2D) (None, 128, 128, 128 0           conv2d_127[0][0]
__________________________________________________________________________________________________
concatenate_26 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_26[0][0]
                                                                 conv2d_117[0][0]
__________________________________________________________________________________________________
conv2d_128 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_26[0][0]
__________________________________________________________________________________________________
conv2d_129 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_128[0][0]
__________________________________________________________________________________________________
up_sampling2d_27 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_129[0][0]
__________________________________________________________________________________________________
concatenate_27 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_27[0][0]
                                                                 conv2d_115[0][0]
__________________________________________________________________________________________________
conv2d_130 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_27[0][0]
__________________________________________________________________________________________________
conv2d_131 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_130[0][0]
__________________________________________________________________________________________________
conv2d_132 (Conv2D)             (None, 256, 256, 1)  33          conv2d_131[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6913 - acc: 0.8325 - precision_6: 0.4734 - auc_6: 0.5079 - recall_6: 0.17072021-09-24 16:48:41.848441: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 16:48:41.848974: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 16:48:42.353156: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 16:48:42.367341: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42
2021-09-24 16:48:42.370660: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.trace.json.gz
2021-09-24 16:48:42.397591: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42
2021-09-24 16:48:42.406207: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.memory_profile.json.gz
2021-09-24 16:48:42.418315: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42Dumped tool data for xplane.pb to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-164805/train/plugins/profile/2021_09_24_16_48_42/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:12 - loss: 0.6848 - acc: 0.7362 - precision_6: 0.4718 - auc_6: 0.4380 - recall_6: 0.0535WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2010s vs `on_train_batch_end` time: 0.3699s). Check your callbacks.
254/254 [==============================] - 55s 218ms/step - loss: 0.4466 - acc: 0.8177 - precision_6: 0.7594 - auc_6: 0.8059 - recall_6: 0.2314 - val_loss: 0.3102 - val_acc: 0.8765 - val_precision_6: 0.7804 - val_auc_6: 0.8904 - val_recall_6: 0.5826
Epoch 2/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3121 - acc: 0.8758 - precision_6: 0.8068 - auc_6: 0.8891 - recall_6: 0.5607 - val_loss: 0.3188 - val_acc: 0.8726 - val_precision_6: 0.9551 - val_auc_6: 0.8994 - val_recall_6: 0.4189
Epoch 3/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3064 - acc: 0.8796 - precision_6: 0.8272 - auc_6: 0.8919 - recall_6: 0.5609 - val_loss: 0.3003 - val_acc: 0.8810 - val_precision_6: 0.9166 - val_auc_6: 0.9013 - val_recall_6: 0.4883
Epoch 4/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2991 - acc: 0.8820 - precision_6: 0.8238 - auc_6: 0.8974 - recall_6: 0.5787 - val_loss: 0.2808 - val_acc: 0.8920 - val_precision_6: 0.8906 - val_auc_6: 0.9069 - val_recall_6: 0.5573
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3063 - acc: 0.8803 - precision_6: 0.8227 - auc_6: 0.8919 - recall_6: 0.5702 - val_loss: 0.3166 - val_acc: 0.8785 - val_precision_6: 0.7614 - val_auc_6: 0.8756 - val_recall_6: 0.5971
Epoch 6/100
254/254 [==============================] - 55s 218ms/step - loss: 0.3024 - acc: 0.8833 - precision_6: 0.8335 - auc_6: 0.8935 - recall_6: 0.5758 - val_loss: 0.2770 - val_acc: 0.8928 - val_precision_6: 0.8707 - val_auc_6: 0.9116 - val_recall_6: 0.5939
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2913 - acc: 0.8881 - precision_6: 0.8401 - auc_6: 0.9000 - recall_6: 0.5968 - val_loss: 0.2735 - val_acc: 0.8945 - val_precision_6: 0.8352 - val_auc_6: 0.9098 - val_recall_6: 0.6185
Epoch 8/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2851 - acc: 0.8939 - precision_6: 0.8456 - auc_6: 0.9017 - recall_6: 0.6237 - val_loss: 0.3161 - val_acc: 0.8905 - val_precision_6: 0.9345 - val_auc_6: 0.9023 - val_recall_6: 0.5384
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2747 - acc: 0.8981 - precision_6: 0.8404 - auc_6: 0.9101 - recall_6: 0.6532 - val_loss: 0.2604 - val_acc: 0.9098 - val_precision_6: 0.9180 - val_auc_6: 0.9246 - val_recall_6: 0.6244
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2501 - acc: 0.9064 - precision_6: 0.8305 - auc_6: 0.9275 - recall_6: 0.7134 - val_loss: 0.2311 - val_acc: 0.9193 - val_precision_6: 0.9224 - val_auc_6: 0.9355 - val_recall_6: 0.6790
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2173 - acc: 0.9187 - precision_6: 0.8566 - auc_6: 0.9469 - recall_6: 0.7501 - val_loss: 0.2556 - val_acc: 0.9152 - val_precision_6: 0.8244 - val_auc_6: 0.9342 - val_recall_6: 0.7612
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1986 - acc: 0.9263 - precision_6: 0.8695 - auc_6: 0.9553 - recall_6: 0.7763 - val_loss: 0.1542 - val_acc: 0.9388 - val_precision_6: 0.8446 - val_auc_6: 0.9771 - val_recall_6: 0.8700
Epoch 13/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1803 - acc: 0.9314 - precision_6: 0.8766 - auc_6: 0.9652 - recall_6: 0.7949 - val_loss: 0.1527 - val_acc: 0.9442 - val_precision_6: 0.8908 - val_auc_6: 0.9753 - val_recall_6: 0.8430
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1780 - acc: 0.9325 - precision_6: 0.8807 - auc_6: 0.9653 - recall_6: 0.7963 - val_loss: 0.1581 - val_acc: 0.9405 - val_precision_6: 0.8971 - val_auc_6: 0.9720 - val_recall_6: 0.8040
Epoch 15/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1690 - acc: 0.9364 - precision_6: 0.8880 - auc_6: 0.9695 - recall_6: 0.8080 - val_loss: 0.1850 - val_acc: 0.9320 - val_precision_6: 0.8449 - val_auc_6: 0.9721 - val_recall_6: 0.8508
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1551 - acc: 0.9395 - precision_6: 0.8884 - auc_6: 0.9750 - recall_6: 0.8242 - val_loss: 0.1460 - val_acc: 0.9454 - val_precision_6: 0.9405 - val_auc_6: 0.9757 - val_recall_6: 0.7729
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1559 - acc: 0.9396 - precision_6: 0.8918 - auc_6: 0.9743 - recall_6: 0.8207 - val_loss: 0.1345 - val_acc: 0.9456 - val_precision_6: 0.8884 - val_auc_6: 0.9822 - val_recall_6: 0.8606
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1511 - acc: 0.9414 - precision_6: 0.8941 - auc_6: 0.9758 - recall_6: 0.8271 - val_loss: 0.1306 - val_acc: 0.9494 - val_precision_6: 0.9293 - val_auc_6: 0.9798 - val_recall_6: 0.8175
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1431 - acc: 0.9441 - precision_6: 0.8998 - auc_6: 0.9787 - recall_6: 0.8349 - val_loss: 0.1315 - val_acc: 0.9471 - val_precision_6: 0.9238 - val_auc_6: 0.9831 - val_recall_6: 0.8280
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1409 - acc: 0.9454 - precision_6: 0.9015 - auc_6: 0.9796 - recall_6: 0.8395 - val_loss: 0.1310 - val_acc: 0.9486 - val_precision_6: 0.8720 - val_auc_6: 0.9795 - val_recall_6: 0.8573
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1378 - acc: 0.9460 - precision_6: 0.9007 - auc_6: 0.9803 - recall_6: 0.8437 - val_loss: 0.1339 - val_acc: 0.9473 - val_precision_6: 0.9047 - val_auc_6: 0.9838 - val_recall_6: 0.8642
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1322 - acc: 0.9484 - precision_6: 0.9062 - auc_6: 0.9815 - recall_6: 0.8496 - val_loss: 0.1385 - val_acc: 0.9457 - val_precision_6: 0.8645 - val_auc_6: 0.9815 - val_recall_6: 0.8687
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1260 - acc: 0.9503 - precision_6: 0.9069 - auc_6: 0.9836 - recall_6: 0.8584 - val_loss: 0.1341 - val_acc: 0.9449 - val_precision_6: 0.8613 - val_auc_6: 0.9812 - val_recall_6: 0.8811
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1208 - acc: 0.9516 - precision_6: 0.9088 - auc_6: 0.9851 - recall_6: 0.8631 - val_loss: 0.1374 - val_acc: 0.9438 - val_precision_6: 0.8486 - val_auc_6: 0.9830 - val_recall_6: 0.9006
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1187 - acc: 0.9526 - precision_6: 0.9086 - auc_6: 0.9857 - recall_6: 0.8687 - val_loss: 0.1317 - val_acc: 0.9465 - val_precision_6: 0.8653 - val_auc_6: 0.9820 - val_recall_6: 0.8792
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1125 - acc: 0.9550 - precision_6: 0.9138 - auc_6: 0.9871 - recall_6: 0.8749 - val_loss: 0.1430 - val_acc: 0.9450 - val_precision_6: 0.9006 - val_auc_6: 0.9775 - val_recall_6: 0.8308
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1103 - acc: 0.9563 - precision_6: 0.9182 - auc_6: 0.9873 - recall_6: 0.8763 - val_loss: 0.1259 - val_acc: 0.9497 - val_precision_6: 0.8918 - val_auc_6: 0.9825 - val_recall_6: 0.8690
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0981 - acc: 0.9605 - precision_6: 0.9239 - auc_6: 0.9902 - recall_6: 0.8910 - val_loss: 0.1426 - val_acc: 0.9432 - val_precision_6: 0.8491 - val_auc_6: 0.9798 - val_recall_6: 0.8912
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0939 - acc: 0.9621 - precision_6: 0.9261 - auc_6: 0.9909 - recall_6: 0.8966 - val_loss: 0.1582 - val_acc: 0.9422 - val_precision_6: 0.8586 - val_auc_6: 0.9782 - val_recall_6: 0.8908
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0903 - acc: 0.9636 - precision_6: 0.9290 - auc_6: 0.9916 - recall_6: 0.9007 - val_loss: 0.1337 - val_acc: 0.9506 - val_precision_6: 0.9004 - val_auc_6: 0.9810 - val_recall_6: 0.8624
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0836 - acc: 0.9661 - precision_6: 0.9337 - auc_6: 0.9928 - recall_6: 0.9081 - val_loss: 0.1440 - val_acc: 0.9491 - val_precision_6: 0.8777 - val_auc_6: 0.9783 - val_recall_6: 0.8798
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0778 - acc: 0.9683 - precision_6: 0.9366 - auc_6: 0.9938 - recall_6: 0.9155 - val_loss: 0.1531 - val_acc: 0.9482 - val_precision_6: 0.8962 - val_auc_6: 0.9744 - val_recall_6: 0.8568
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0797 - acc: 0.9675 - precision_6: 0.9340 - auc_6: 0.9935 - recall_6: 0.9147 - val_loss: 0.1377 - val_acc: 0.9517 - val_precision_6: 0.8974 - val_auc_6: 0.9797 - val_recall_6: 0.8705
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0770 - acc: 0.9688 - precision_6: 0.9369 - auc_6: 0.9938 - recall_6: 0.9178 - val_loss: 0.1671 - val_acc: 0.9449 - val_precision_6: 0.8472 - val_auc_6: 0.9723 - val_recall_6: 0.8769
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0637 - acc: 0.9736 - precision_6: 0.9456 - auc_6: 0.9958 - recall_6: 0.9319 - val_loss: 0.1675 - val_acc: 0.9491 - val_precision_6: 0.8899 - val_auc_6: 0.9747 - val_recall_6: 0.8830
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0565 - acc: 0.9764 - precision_6: 0.9513 - auc_6: 0.9967 - recall_6: 0.9393 - val_loss: 0.1771 - val_acc: 0.9457 - val_precision_6: 0.8781 - val_auc_6: 0.9717 - val_recall_6: 0.8651
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0547 - acc: 0.9771 - precision_6: 0.9527 - auc_6: 0.9969 - recall_6: 0.9410 - val_loss: 0.1673 - val_acc: 0.9463 - val_precision_6: 0.9187 - val_auc_6: 0.9719 - val_recall_6: 0.8241
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0545 - acc: 0.9772 - precision_6: 0.9524 - auc_6: 0.9969 - recall_6: 0.9419 - val_loss: 0.1732 - val_acc: 0.9488 - val_precision_6: 0.8831 - val_auc_6: 0.9738 - val_recall_6: 0.8711
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0572 - acc: 0.9765 - precision_6: 0.9514 - auc_6: 0.9965 - recall_6: 0.9392 - val_loss: 0.1830 - val_acc: 0.9432 - val_precision_6: 0.8712 - val_auc_6: 0.9703 - val_recall_6: 0.8692
Epoch 40/100
254/254 [==============================] - 54s 214ms/step - loss: 0.0494 - acc: 0.9793 - precision_6: 0.9560 - auc_6: 0.9975 - recall_6: 0.9478 - val_loss: 0.1830 - val_acc: 0.9526 - val_precision_6: 0.8978 - val_auc_6: 0.9673 - val_recall_6: 0.8592
Epoch 41/100
254/254 [==============================] - ETA: 0s - loss: 0.0440 - acc: 0.9814 - precision_6: 0.9607 - auc_6: 0.9980 - recall_6: 0.9532Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 216ms/step - loss: 0.0440 - acc: 0.9814 - precision_6: 0.9607 - auc_6: 0.9980 - recall_6: 0.9532 - val_loss: 0.2174 - val_acc: 0.9395 - val_precision_6: 0.8490 - val_auc_6: 0.9682 - val_recall_6: 0.8804
Epoch 00041: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1381 - acc: 0.9452 - precision_6: 0.8834 - auc_6: 0.9812 - recall_6: 0.8696
---------------TEST METRICS----------------------
jaccard_index 0.7776433167296238
test_sensitivity 0.8728416061027782
test_specifitivity 0.9635959647743262
test_accuracy 0.9433031555608655
test_precision 0.8735008938574004
test_jaccard_score 0.7776433167296238
test_dicecoef 0.8731711255313218
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-172644.h5
[0. 0. 0. 0. 0.] [0.94330316 0.77764332 0.87350089 0.87284161 0.96359596]

-------------------------
Rep: 1
-------------------------

2021-09-24 17:26:44.951120: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 17:26:44.951243: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_15"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_8 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_133 (Conv2D)             (None, 256, 256, 32) 320         input_8[0][0]
__________________________________________________________________________________________________
conv2d_134 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_133[0][0]
__________________________________________________________________________________________________
max_pooling2d_28 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_134[0][0]
__________________________________________________________________________________________________
conv2d_135 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_28[0][0]
__________________________________________________________________________________________________
conv2d_136 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_135[0][0]
__________________________________________________________________________________________________
max_pooling2d_29 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_136[0][0]
__________________________________________________________________________________________________
conv2d_137 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_29[0][0]
__________________________________________________________________________________________________
conv2d_138 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_137[0][0]
__________________________________________________________________________________________________
max_pooling2d_30 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_138[0][0]
__________________________________________________________________________________________________
conv2d_139 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_30[0][0]
__________________________________________________________________________________________________
conv2d_140 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_139[0][0]
__________________________________________________________________________________________________
max_pooling2d_31 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_140[0][0]
__________________________________________________________________________________________________
conv2d_141 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_31[0][0]
__________________________________________________________________________________________________
conv2d_142 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_141[0][0]
__________________________________________________________________________________________________
up_sampling2d_28 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_142[0][0]
__________________________________________________________________________________________________
concatenate_28 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_28[0][0]
                                                                 conv2d_140[0][0]
__________________________________________________________________________________________________
conv2d_143 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_28[0][0]
__________________________________________________________________________________________________
conv2d_144 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_143[0][0]
__________________________________________________________________________________________________
up_sampling2d_29 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_144[0][0]
__________________________________________________________________________________________________
concatenate_29 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_29[0][0]
                                                                 conv2d_138[0][0]
__________________________________________________________________________________________________
conv2d_145 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_29[0][0]
__________________________________________________________________________________________________
conv2d_146 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_145[0][0]
__________________________________________________________________________________________________
up_sampling2d_30 (UpSampling2D) (None, 128, 128, 128 0           conv2d_146[0][0]
__________________________________________________________________________________________________
concatenate_30 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_30[0][0]
                                                                 conv2d_136[0][0]
__________________________________________________________________________________________________
conv2d_147 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_30[0][0]
__________________________________________________________________________________________________
conv2d_148 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_147[0][0]
__________________________________________________________________________________________________
up_sampling2d_31 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_148[0][0]
__________________________________________________________________________________________________
concatenate_31 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_31[0][0]
                                                                 conv2d_134[0][0]
__________________________________________________________________________________________________
conv2d_149 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_31[0][0]
__________________________________________________________________________________________________
conv2d_150 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_149[0][0]
__________________________________________________________________________________________________
conv2d_151 (Conv2D)             (None, 256, 256, 1)  33          conv2d_150[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.7158 - acc: 0.1644 - precision_7: 0.1644 - auc_7: 0.1176 - recall_7: 1.00002021-09-24 17:27:20.882777: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 17:27:20.883513: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 17:27:21.371835: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 17:27:21.382740: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21
2021-09-24 17:27:21.386584: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.trace.json.gz
2021-09-24 17:27:21.411767: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21
2021-09-24 17:27:21.420162: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.memory_profile.json.gz
2021-09-24 17:27:21.442828: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21Dumped tool data for xplane.pb to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-172644/train/plugins/profile/2021_09_24_17_27_21/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:10 - loss: 0.6965 - acc: 0.4008 - precision_7: 0.1639 - auc_7: 0.3604 - recall_7: 0.3135WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0664s vs `on_train_batch_end` time: 0.4941s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.3824 - acc: 0.8321 - precision_7: 0.7715 - auc_7: 0.8414 - recall_7: 0.3191 - val_loss: 0.3348 - val_acc: 0.8662 - val_precision_7: 0.6766 - val_auc_7: 0.8972 - val_recall_7: 0.7085
Epoch 2/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3190 - acc: 0.8767 - precision_7: 0.8220 - auc_7: 0.8815 - recall_7: 0.5496 - val_loss: 0.3290 - val_acc: 0.8762 - val_precision_7: 0.9081 - val_auc_7: 0.8654 - val_recall_7: 0.4630
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3129 - acc: 0.8820 - precision_7: 0.8321 - auc_7: 0.8805 - recall_7: 0.5698 - val_loss: 0.2814 - val_acc: 0.8927 - val_precision_7: 0.8923 - val_auc_7: 0.9083 - val_recall_7: 0.5672
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2798 - acc: 0.8956 - precision_7: 0.8484 - auc_7: 0.9047 - recall_7: 0.6306 - val_loss: 0.2506 - val_acc: 0.9073 - val_precision_7: 0.9064 - val_auc_7: 0.9221 - val_recall_7: 0.6258
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2655 - acc: 0.9018 - precision_7: 0.8539 - auc_7: 0.9136 - recall_7: 0.6595 - val_loss: 0.2588 - val_acc: 0.9085 - val_precision_7: 0.9074 - val_auc_7: 0.9144 - val_recall_7: 0.6189
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2645 - acc: 0.9029 - precision_7: 0.8484 - auc_7: 0.9149 - recall_7: 0.6716 - val_loss: 0.2372 - val_acc: 0.9128 - val_precision_7: 0.8482 - val_auc_7: 0.9317 - val_recall_7: 0.7280
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2561 - acc: 0.9065 - precision_7: 0.8512 - auc_7: 0.9203 - recall_7: 0.6883 - val_loss: 0.2438 - val_acc: 0.9105 - val_precision_7: 0.8581 - val_auc_7: 0.9256 - val_recall_7: 0.6860
Epoch 8/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2460 - acc: 0.9090 - precision_7: 0.8385 - auc_7: 0.9276 - recall_7: 0.7182 - val_loss: 0.2783 - val_acc: 0.9001 - val_precision_7: 0.8757 - val_auc_7: 0.9048 - val_recall_7: 0.6344
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2277 - acc: 0.9161 - precision_7: 0.8546 - auc_7: 0.9402 - recall_7: 0.7378 - val_loss: 0.1830 - val_acc: 0.9342 - val_precision_7: 0.9275 - val_auc_7: 0.9607 - val_recall_7: 0.7434
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2074 - acc: 0.9216 - precision_7: 0.8706 - auc_7: 0.9519 - recall_7: 0.7491 - val_loss: 0.1676 - val_acc: 0.9373 - val_precision_7: 0.9064 - val_auc_7: 0.9670 - val_recall_7: 0.7877
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1843 - acc: 0.9302 - precision_7: 0.8816 - auc_7: 0.9627 - recall_7: 0.7829 - val_loss: 0.1805 - val_acc: 0.9299 - val_precision_7: 0.8348 - val_auc_7: 0.9676 - val_recall_7: 0.8331
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1793 - acc: 0.9319 - precision_7: 0.8825 - auc_7: 0.9649 - recall_7: 0.7907 - val_loss: 0.1492 - val_acc: 0.9417 - val_precision_7: 0.8911 - val_auc_7: 0.9754 - val_recall_7: 0.8246
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1688 - acc: 0.9358 - precision_7: 0.8891 - auc_7: 0.9693 - recall_7: 0.8035 - val_loss: 0.1363 - val_acc: 0.9468 - val_precision_7: 0.8854 - val_auc_7: 0.9811 - val_recall_7: 0.8637
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1645 - acc: 0.9368 - precision_7: 0.8882 - auc_7: 0.9709 - recall_7: 0.8098 - val_loss: 0.1666 - val_acc: 0.9348 - val_precision_7: 0.8755 - val_auc_7: 0.9696 - val_recall_7: 0.7974
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1586 - acc: 0.9389 - precision_7: 0.8941 - auc_7: 0.9727 - recall_7: 0.8140 - val_loss: 0.1518 - val_acc: 0.9419 - val_precision_7: 0.8901 - val_auc_7: 0.9762 - val_recall_7: 0.8433
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1471 - acc: 0.9420 - precision_7: 0.8972 - auc_7: 0.9777 - recall_7: 0.8270 - val_loss: 0.1346 - val_acc: 0.9479 - val_precision_7: 0.9450 - val_auc_7: 0.9792 - val_recall_7: 0.7823
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1480 - acc: 0.9422 - precision_7: 0.8985 - auc_7: 0.9770 - recall_7: 0.8265 - val_loss: 0.1365 - val_acc: 0.9439 - val_precision_7: 0.8904 - val_auc_7: 0.9813 - val_recall_7: 0.8497
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1422 - acc: 0.9442 - precision_7: 0.9011 - auc_7: 0.9788 - recall_7: 0.8339 - val_loss: 0.1328 - val_acc: 0.9479 - val_precision_7: 0.9236 - val_auc_7: 0.9796 - val_recall_7: 0.8156
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1353 - acc: 0.9463 - precision_7: 0.9039 - auc_7: 0.9812 - recall_7: 0.8412 - val_loss: 0.1301 - val_acc: 0.9480 - val_precision_7: 0.9162 - val_auc_7: 0.9832 - val_recall_7: 0.8406
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1329 - acc: 0.9482 - precision_7: 0.9061 - auc_7: 0.9817 - recall_7: 0.8486 - val_loss: 0.1339 - val_acc: 0.9501 - val_precision_7: 0.8744 - val_auc_7: 0.9783 - val_recall_7: 0.8636
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1326 - acc: 0.9474 - precision_7: 0.9052 - auc_7: 0.9817 - recall_7: 0.8454 - val_loss: 0.1303 - val_acc: 0.9469 - val_precision_7: 0.8942 - val_auc_7: 0.9847 - val_recall_7: 0.8751
Epoch 22/100
254/254 [==============================] - 54s 215ms/step - loss: 0.1224 - acc: 0.9517 - precision_7: 0.9126 - auc_7: 0.9847 - recall_7: 0.8591 - val_loss: 0.1189 - val_acc: 0.9520 - val_precision_7: 0.9078 - val_auc_7: 0.9847 - val_recall_7: 0.8504
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1172 - acc: 0.9531 - precision_7: 0.9138 - auc_7: 0.9861 - recall_7: 0.8649 - val_loss: 0.1302 - val_acc: 0.9483 - val_precision_7: 0.8777 - val_auc_7: 0.9819 - val_recall_7: 0.8776
Epoch 24/100
254/254 [==============================] - 54s 214ms/step - loss: 0.1105 - acc: 0.9558 - precision_7: 0.9183 - auc_7: 0.9877 - recall_7: 0.8734 - val_loss: 0.1316 - val_acc: 0.9480 - val_precision_7: 0.8845 - val_auc_7: 0.9833 - val_recall_7: 0.8735
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1020 - acc: 0.9586 - precision_7: 0.9205 - auc_7: 0.9895 - recall_7: 0.8855 - val_loss: 0.1430 - val_acc: 0.9425 - val_precision_7: 0.8376 - val_auc_7: 0.9818 - val_recall_7: 0.8969
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1015 - acc: 0.9591 - precision_7: 0.9227 - auc_7: 0.9895 - recall_7: 0.8853 - val_loss: 0.1349 - val_acc: 0.9487 - val_precision_7: 0.9063 - val_auc_7: 0.9800 - val_recall_7: 0.8437
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0946 - acc: 0.9619 - precision_7: 0.9271 - auc_7: 0.9908 - recall_7: 0.8945 - val_loss: 0.1233 - val_acc: 0.9494 - val_precision_7: 0.8816 - val_auc_7: 0.9844 - val_recall_7: 0.8804
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0830 - acc: 0.9664 - precision_7: 0.9344 - auc_7: 0.9930 - recall_7: 0.9084 - val_loss: 0.1367 - val_acc: 0.9477 - val_precision_7: 0.8665 - val_auc_7: 0.9817 - val_recall_7: 0.8911
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1053 - acc: 0.9581 - precision_7: 0.9210 - auc_7: 0.9887 - recall_7: 0.8819 - val_loss: 0.1521 - val_acc: 0.9422 - val_precision_7: 0.8721 - val_auc_7: 0.9782 - val_recall_7: 0.8716
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0847 - acc: 0.9658 - precision_7: 0.9340 - auc_7: 0.9926 - recall_7: 0.9060 - val_loss: 0.1315 - val_acc: 0.9512 - val_precision_7: 0.9026 - val_auc_7: 0.9818 - val_recall_7: 0.8629
Epoch 31/100
254/254 [==============================] - 54s 215ms/step - loss: 0.0894 - acc: 0.9637 - precision_7: 0.9281 - auc_7: 0.9918 - recall_7: 0.9023 - val_loss: 0.1388 - val_acc: 0.9506 - val_precision_7: 0.9104 - val_auc_7: 0.9772 - val_recall_7: 0.8478
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0710 - acc: 0.9704 - precision_7: 0.9401 - auc_7: 0.9950 - recall_7: 0.9220 - val_loss: 0.1496 - val_acc: 0.9495 - val_precision_7: 0.8915 - val_auc_7: 0.9768 - val_recall_7: 0.8699
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0657 - acc: 0.9727 - precision_7: 0.9435 - auc_7: 0.9956 - recall_7: 0.9297 - val_loss: 0.1455 - val_acc: 0.9516 - val_precision_7: 0.8919 - val_auc_7: 0.9787 - val_recall_7: 0.8767
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0605 - acc: 0.9747 - precision_7: 0.9480 - auc_7: 0.9963 - recall_7: 0.9343 - val_loss: 0.1702 - val_acc: 0.9482 - val_precision_7: 0.8839 - val_auc_7: 0.9702 - val_recall_7: 0.8472
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0599 - acc: 0.9750 - precision_7: 0.9483 - auc_7: 0.9963 - recall_7: 0.9356 - val_loss: 0.1698 - val_acc: 0.9488 - val_precision_7: 0.9013 - val_auc_7: 0.9734 - val_recall_7: 0.8674
Epoch 36/100
254/254 [==============================] - ETA: 0s - loss: 0.0550 - acc: 0.9769 - precision_7: 0.9524 - auc_7: 0.9969 - recall_7: 0.9405Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0550 - acc: 0.9769 - precision_7: 0.9524 - auc_7: 0.9969 - recall_7: 0.9405 - val_loss: 0.1899 - val_acc: 0.9455 - val_precision_7: 0.8745 - val_auc_7: 0.9697 - val_recall_7: 0.8687
Epoch 00036: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1388 - acc: 0.9458 - precision_7: 0.9037 - auc_7: 0.9813 - recall_7: 0.8482
---------------TEST METRICS----------------------
jaccard_index 0.7920393296814113
test_sensitivity 0.8496171607242549
test_specifitivity 0.9726068611325358
test_accuracy 0.9451061816925698
test_precision 0.8993200864953256
test_jaccard_score 0.7920393296814113
test_dicecoef 0.8737623716175784
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-180045.h5
[0.94330316 0.77764332 0.87350089 0.87284161 0.96359596] [0.94510618 0.79203933 0.89932009 0.84961716 0.97260686]

-------------------------
Rep: 2
-------------------------

2021-09-24 18:00:46.572998: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 18:00:46.573123: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_17"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_9 (InputLayer)            [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_152 (Conv2D)             (None, 256, 256, 32) 320         input_9[0][0]
__________________________________________________________________________________________________
conv2d_153 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_152[0][0]
__________________________________________________________________________________________________
max_pooling2d_32 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_153[0][0]
__________________________________________________________________________________________________
conv2d_154 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_32[0][0]
__________________________________________________________________________________________________
conv2d_155 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_154[0][0]
__________________________________________________________________________________________________
max_pooling2d_33 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_155[0][0]
__________________________________________________________________________________________________
conv2d_156 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_33[0][0]
__________________________________________________________________________________________________
conv2d_157 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_156[0][0]
__________________________________________________________________________________________________
max_pooling2d_34 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_157[0][0]
__________________________________________________________________________________________________
conv2d_158 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_34[0][0]
__________________________________________________________________________________________________
conv2d_159 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_158[0][0]
__________________________________________________________________________________________________
max_pooling2d_35 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_159[0][0]
__________________________________________________________________________________________________
conv2d_160 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_35[0][0]
__________________________________________________________________________________________________
conv2d_161 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_160[0][0]
__________________________________________________________________________________________________
up_sampling2d_32 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_161[0][0]
__________________________________________________________________________________________________
concatenate_32 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_32[0][0]
                                                                 conv2d_159[0][0]
__________________________________________________________________________________________________
conv2d_162 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_32[0][0]
__________________________________________________________________________________________________
conv2d_163 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_162[0][0]
__________________________________________________________________________________________________
up_sampling2d_33 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_163[0][0]
__________________________________________________________________________________________________
concatenate_33 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_33[0][0]
                                                                 conv2d_157[0][0]
__________________________________________________________________________________________________
conv2d_164 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_33[0][0]
__________________________________________________________________________________________________
conv2d_165 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_164[0][0]
__________________________________________________________________________________________________
up_sampling2d_34 (UpSampling2D) (None, 128, 128, 128 0           conv2d_165[0][0]
__________________________________________________________________________________________________
concatenate_34 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_34[0][0]
                                                                 conv2d_155[0][0]
__________________________________________________________________________________________________
conv2d_166 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_34[0][0]
__________________________________________________________________________________________________
conv2d_167 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_166[0][0]
__________________________________________________________________________________________________
up_sampling2d_35 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_167[0][0]
__________________________________________________________________________________________________
concatenate_35 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_35[0][0]
                                                                 conv2d_153[0][0]
__________________________________________________________________________________________________
conv2d_168 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_35[0][0]
__________________________________________________________________________________________________
conv2d_169 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_168[0][0]
__________________________________________________________________________________________________
conv2d_170 (Conv2D)             (None, 256, 256, 1)  33          conv2d_169[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6903 - acc: 0.8526 - precision_8: 0.6391 - auc_8: 0.5227 - recall_8: 0.23692021-09-24 18:01:22.768477: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 18:01:22.769118: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 18:01:23.294515: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 18:01:23.304646: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23
2021-09-24 18:01:23.307715: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.trace.json.gz
2021-09-24 18:01:23.335025: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23
2021-09-24 18:01:23.342890: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.memory_profile.json.gz
2021-09-24 18:01:23.356706: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23Dumped tool data for xplane.pb to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-180046/train/plugins/profile/2021_09_24_18_01_23/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6759 - acc: 0.7463 - precision_8: 0.6391 - auc_8: 0.4438 - recall_8: 0.0743WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1950s vs `on_train_batch_end` time: 0.3942s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.3817 - acc: 0.8368 - precision_8: 0.7590 - auc_8: 0.8450 - recall_8: 0.3611 - val_loss: 0.3037 - val_acc: 0.8809 - val_precision_8: 0.9296 - val_auc_8: 0.9015 - val_recall_8: 0.4750
Epoch 2/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3229 - acc: 0.8751 - precision_8: 0.8215 - auc_8: 0.8847 - recall_8: 0.5407 - val_loss: 0.3021 - val_acc: 0.8795 - val_precision_8: 0.9543 - val_auc_8: 0.9067 - val_recall_8: 0.4533
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3107 - acc: 0.8810 - precision_8: 0.8315 - auc_8: 0.8874 - recall_8: 0.5648 - val_loss: 0.3124 - val_acc: 0.8707 - val_precision_8: 0.7115 - val_auc_8: 0.8981 - val_recall_8: 0.6657
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2888 - acc: 0.8919 - precision_8: 0.8413 - auc_8: 0.9003 - recall_8: 0.6171 - val_loss: 0.2567 - val_acc: 0.9049 - val_precision_8: 0.8703 - val_auc_8: 0.9204 - val_recall_8: 0.6461
Epoch 5/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2700 - acc: 0.9008 - precision_8: 0.8453 - auc_8: 0.9129 - recall_8: 0.6633 - val_loss: 0.2218 - val_acc: 0.9211 - val_precision_8: 0.8639 - val_auc_8: 0.9363 - val_recall_8: 0.7321
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2508 - acc: 0.9104 - precision_8: 0.8508 - auc_8: 0.9230 - recall_8: 0.7109 - val_loss: 0.2230 - val_acc: 0.9192 - val_precision_8: 0.8591 - val_auc_8: 0.9430 - val_recall_8: 0.7505
Epoch 7/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2242 - acc: 0.9168 - precision_8: 0.8533 - auc_8: 0.9416 - recall_8: 0.7438 - val_loss: 0.2126 - val_acc: 0.9241 - val_precision_8: 0.9118 - val_auc_8: 0.9562 - val_recall_8: 0.7060
Epoch 8/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2086 - acc: 0.9218 - precision_8: 0.8606 - auc_8: 0.9510 - recall_8: 0.7625 - val_loss: 0.2143 - val_acc: 0.9243 - val_precision_8: 0.8761 - val_auc_8: 0.9484 - val_recall_8: 0.7629
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1934 - acc: 0.9267 - precision_8: 0.8712 - auc_8: 0.9588 - recall_8: 0.7761 - val_loss: 0.1894 - val_acc: 0.9293 - val_precision_8: 0.9702 - val_auc_8: 0.9720 - val_recall_8: 0.6830
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1786 - acc: 0.9312 - precision_8: 0.8767 - auc_8: 0.9658 - recall_8: 0.7940 - val_loss: 0.1747 - val_acc: 0.9406 - val_precision_8: 0.9365 - val_auc_8: 0.9635 - val_recall_8: 0.7745
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1679 - acc: 0.9340 - precision_8: 0.8824 - auc_8: 0.9697 - recall_8: 0.8019 - val_loss: 0.1569 - val_acc: 0.9400 - val_precision_8: 0.9232 - val_auc_8: 0.9739 - val_recall_8: 0.7813
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1614 - acc: 0.9367 - precision_8: 0.8874 - auc_8: 0.9727 - recall_8: 0.8106 - val_loss: 0.1446 - val_acc: 0.9413 - val_precision_8: 0.8601 - val_auc_8: 0.9779 - val_recall_8: 0.8618
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1570 - acc: 0.9378 - precision_8: 0.8877 - auc_8: 0.9749 - recall_8: 0.8157 - val_loss: 0.1317 - val_acc: 0.9454 - val_precision_8: 0.8677 - val_auc_8: 0.9822 - val_recall_8: 0.8796
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1552 - acc: 0.9388 - precision_8: 0.8874 - auc_8: 0.9750 - recall_8: 0.8217 - val_loss: 0.1463 - val_acc: 0.9436 - val_precision_8: 0.8931 - val_auc_8: 0.9752 - val_recall_8: 0.8256
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1501 - acc: 0.9407 - precision_8: 0.8934 - auc_8: 0.9766 - recall_8: 0.8245 - val_loss: 0.1464 - val_acc: 0.9411 - val_precision_8: 0.8786 - val_auc_8: 0.9789 - val_recall_8: 0.8532
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1440 - acc: 0.9427 - precision_8: 0.8939 - auc_8: 0.9787 - recall_8: 0.8344 - val_loss: 0.1484 - val_acc: 0.9450 - val_precision_8: 0.9417 - val_auc_8: 0.9750 - val_recall_8: 0.7701
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1423 - acc: 0.9434 - precision_8: 0.8977 - auc_8: 0.9792 - recall_8: 0.8337 - val_loss: 0.1347 - val_acc: 0.9450 - val_precision_8: 0.8733 - val_auc_8: 0.9832 - val_recall_8: 0.8771
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1389 - acc: 0.9442 - precision_8: 0.8989 - auc_8: 0.9803 - recall_8: 0.8365 - val_loss: 0.1307 - val_acc: 0.9504 - val_precision_8: 0.9490 - val_auc_8: 0.9803 - val_recall_8: 0.8033
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1314 - acc: 0.9474 - precision_8: 0.9039 - auc_8: 0.9823 - recall_8: 0.8469 - val_loss: 0.1382 - val_acc: 0.9450 - val_precision_8: 0.9320 - val_auc_8: 0.9821 - val_recall_8: 0.8092
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1327 - acc: 0.9470 - precision_8: 0.8992 - auc_8: 0.9823 - recall_8: 0.8505 - val_loss: 0.1352 - val_acc: 0.9503 - val_precision_8: 0.8747 - val_auc_8: 0.9782 - val_recall_8: 0.8644
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1314 - acc: 0.9470 - precision_8: 0.9001 - auc_8: 0.9826 - recall_8: 0.8496 - val_loss: 0.1264 - val_acc: 0.9486 - val_precision_8: 0.8854 - val_auc_8: 0.9851 - val_recall_8: 0.8948
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1241 - acc: 0.9500 - precision_8: 0.9070 - auc_8: 0.9844 - recall_8: 0.8572 - val_loss: 0.1657 - val_acc: 0.9289 - val_precision_8: 0.7760 - val_auc_8: 0.9802 - val_recall_8: 0.9143
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1223 - acc: 0.9511 - precision_8: 0.9080 - auc_8: 0.9850 - recall_8: 0.8612 - val_loss: 0.1393 - val_acc: 0.9418 - val_precision_8: 0.8471 - val_auc_8: 0.9810 - val_recall_8: 0.8842
Epoch 24/100
254/254 [==============================] - 54s 215ms/step - loss: 0.1198 - acc: 0.9522 - precision_8: 0.9103 - auc_8: 0.9855 - recall_8: 0.8643 - val_loss: 0.1426 - val_acc: 0.9436 - val_precision_8: 0.8449 - val_auc_8: 0.9840 - val_recall_8: 0.9051
Epoch 25/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1103 - acc: 0.9553 - precision_8: 0.9140 - auc_8: 0.9877 - recall_8: 0.8762 - val_loss: 0.1257 - val_acc: 0.9480 - val_precision_8: 0.8560 - val_auc_8: 0.9853 - val_recall_8: 0.9012
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1063 - acc: 0.9570 - precision_8: 0.9175 - auc_8: 0.9886 - recall_8: 0.8807 - val_loss: 0.1454 - val_acc: 0.9465 - val_precision_8: 0.8993 - val_auc_8: 0.9770 - val_recall_8: 0.8400
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1084 - acc: 0.9560 - precision_8: 0.9171 - auc_8: 0.9881 - recall_8: 0.8757 - val_loss: 0.1312 - val_acc: 0.9468 - val_precision_8: 0.8710 - val_auc_8: 0.9821 - val_recall_8: 0.8804
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1009 - acc: 0.9592 - precision_8: 0.9202 - auc_8: 0.9898 - recall_8: 0.8886 - val_loss: 0.1323 - val_acc: 0.9496 - val_precision_8: 0.8753 - val_auc_8: 0.9826 - val_recall_8: 0.8899
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1056 - acc: 0.9573 - precision_8: 0.9177 - auc_8: 0.9890 - recall_8: 0.8818 - val_loss: 0.1635 - val_acc: 0.9363 - val_precision_8: 0.8297 - val_auc_8: 0.9782 - val_recall_8: 0.9033
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0946 - acc: 0.9613 - precision_8: 0.9250 - auc_8: 0.9909 - recall_8: 0.8938 - val_loss: 0.1245 - val_acc: 0.9514 - val_precision_8: 0.8983 - val_auc_8: 0.9833 - val_recall_8: 0.8689
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0916 - acc: 0.9624 - precision_8: 0.9253 - auc_8: 0.9917 - recall_8: 0.8986 - val_loss: 0.1495 - val_acc: 0.9429 - val_precision_8: 0.8311 - val_auc_8: 0.9797 - val_recall_8: 0.9135
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0853 - acc: 0.9649 - precision_8: 0.9299 - auc_8: 0.9927 - recall_8: 0.9063 - val_loss: 0.1456 - val_acc: 0.9480 - val_precision_8: 0.8830 - val_auc_8: 0.9772 - val_recall_8: 0.8726
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0805 - acc: 0.9671 - precision_8: 0.9346 - auc_8: 0.9934 - recall_8: 0.9120 - val_loss: 0.1289 - val_acc: 0.9515 - val_precision_8: 0.9004 - val_auc_8: 0.9819 - val_recall_8: 0.8659
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0779 - acc: 0.9678 - precision_8: 0.9360 - auc_8: 0.9939 - recall_8: 0.9138 - val_loss: 0.1574 - val_acc: 0.9487 - val_precision_8: 0.8726 - val_auc_8: 0.9737 - val_recall_8: 0.8646
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0706 - acc: 0.9707 - precision_8: 0.9411 - auc_8: 0.9950 - recall_8: 0.9224 - val_loss: 0.1497 - val_acc: 0.9482 - val_precision_8: 0.8937 - val_auc_8: 0.9780 - val_recall_8: 0.8739
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0668 - acc: 0.9723 - precision_8: 0.9444 - auc_8: 0.9954 - recall_8: 0.9267 - val_loss: 0.1608 - val_acc: 0.9456 - val_precision_8: 0.8761 - val_auc_8: 0.9746 - val_recall_8: 0.8673
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0699 - acc: 0.9709 - precision_8: 0.9413 - auc_8: 0.9951 - recall_8: 0.9232 - val_loss: 0.1658 - val_acc: 0.9458 - val_precision_8: 0.9176 - val_auc_8: 0.9728 - val_recall_8: 0.8226
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0656 - acc: 0.9727 - precision_8: 0.9444 - auc_8: 0.9957 - recall_8: 0.9286 - val_loss: 0.1650 - val_acc: 0.9487 - val_precision_8: 0.8692 - val_auc_8: 0.9785 - val_recall_8: 0.8891
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0640 - acc: 0.9734 - precision_8: 0.9450 - auc_8: 0.9959 - recall_8: 0.9315 - val_loss: 0.1585 - val_acc: 0.9474 - val_precision_8: 0.8993 - val_auc_8: 0.9755 - val_recall_8: 0.8560
Epoch 40/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0678 - acc: 0.9720 - precision_8: 0.9420 - auc_8: 0.9954 - recall_8: 0.9280 - val_loss: 0.1617 - val_acc: 0.9506 - val_precision_8: 0.8801 - val_auc_8: 0.9735 - val_recall_8: 0.8700
Epoch 41/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0544 - acc: 0.9771 - precision_8: 0.9530 - auc_8: 0.9970 - recall_8: 0.9404 - val_loss: 0.1968 - val_acc: 0.9422 - val_precision_8: 0.8628 - val_auc_8: 0.9713 - val_recall_8: 0.8754
Epoch 42/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0738 - acc: 0.9704 - precision_8: 0.9412 - auc_8: 0.9942 - recall_8: 0.9211 - val_loss: 0.1464 - val_acc: 0.9401 - val_precision_8: 0.8382 - val_auc_8: 0.9784 - val_recall_8: 0.8865
Epoch 43/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0711 - acc: 0.9712 - precision_8: 0.9410 - auc_8: 0.9948 - recall_8: 0.9251 - val_loss: 0.1698 - val_acc: 0.9481 - val_precision_8: 0.8825 - val_auc_8: 0.9744 - val_recall_8: 0.8780
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.0475 - acc: 0.9798 - precision_8: 0.9581 - auc_8: 0.9977 - recall_8: 0.9481Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0475 - acc: 0.9798 - precision_8: 0.9581 - auc_8: 0.9977 - recall_8: 0.9481 - val_loss: 0.2126 - val_acc: 0.9483 - val_precision_8: 0.9383 - val_auc_8: 0.9624 - val_recall_8: 0.8098
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1410 - acc: 0.9476 - precision_8: 0.8899 - auc_8: 0.9813 - recall_8: 0.8738
---------------TEST METRICS----------------------
jaccard_index 0.7898312931521064
test_sensitivity 0.8757641116703485
test_specifitivity 0.966585217000602
test_accuracy 0.9462774831352504
test_precision 0.8830151380548735
test_jaccard_score 0.7898312931521064
test_dicecoef 0.8793746777384517
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-184207.h5
[1.88840934 1.56968265 1.77282098 1.72245877 1.93620283] [0.94627748 0.78983129 0.88301514 0.87576411 0.96658522]

-------------------------
Averaged metrics for Baseline + Histogram Equalization - lesion: [0.94489561 0.78650465 0.88527871 0.86607429 0.96759601]
-------------------------


-------------------------
RUN: Baseline + Per Channel Normalization - lesion, PARAMS: {'per_channel_normalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 18:42:08.731379: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 18:42:08.731478: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_19"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_10 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_171 (Conv2D)             (None, 256, 256, 32) 320         input_10[0][0]
__________________________________________________________________________________________________
conv2d_172 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_171[0][0]
__________________________________________________________________________________________________
max_pooling2d_36 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_172[0][0]
__________________________________________________________________________________________________
conv2d_173 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_36[0][0]
__________________________________________________________________________________________________
conv2d_174 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_173[0][0]
__________________________________________________________________________________________________
max_pooling2d_37 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_174[0][0]
__________________________________________________________________________________________________
conv2d_175 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_37[0][0]
__________________________________________________________________________________________________
conv2d_176 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_175[0][0]
__________________________________________________________________________________________________
max_pooling2d_38 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_176[0][0]
__________________________________________________________________________________________________
conv2d_177 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_38[0][0]
__________________________________________________________________________________________________
conv2d_178 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_177[0][0]
__________________________________________________________________________________________________
max_pooling2d_39 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_178[0][0]
__________________________________________________________________________________________________
conv2d_179 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_39[0][0]
__________________________________________________________________________________________________
conv2d_180 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_179[0][0]
__________________________________________________________________________________________________
up_sampling2d_36 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_180[0][0]
__________________________________________________________________________________________________
concatenate_36 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_36[0][0]
                                                                 conv2d_178[0][0]
__________________________________________________________________________________________________
conv2d_181 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_36[0][0]
__________________________________________________________________________________________________
conv2d_182 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_181[0][0]
__________________________________________________________________________________________________
up_sampling2d_37 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_182[0][0]
__________________________________________________________________________________________________
concatenate_37 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_37[0][0]
                                                                 conv2d_176[0][0]
__________________________________________________________________________________________________
conv2d_183 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_37[0][0]
__________________________________________________________________________________________________
conv2d_184 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_183[0][0]
__________________________________________________________________________________________________
up_sampling2d_38 (UpSampling2D) (None, 128, 128, 128 0           conv2d_184[0][0]
__________________________________________________________________________________________________
concatenate_38 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_38[0][0]
                                                                 conv2d_174[0][0]
__________________________________________________________________________________________________
conv2d_185 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_38[0][0]
__________________________________________________________________________________________________
conv2d_186 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_185[0][0]
__________________________________________________________________________________________________
up_sampling2d_39 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_186[0][0]
__________________________________________________________________________________________________
concatenate_39 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_39[0][0]
                                                                 conv2d_172[0][0]
__________________________________________________________________________________________________
conv2d_187 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_39[0][0]
__________________________________________________________________________________________________
conv2d_188 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_187[0][0]
__________________________________________________________________________________________________
conv2d_189 (Conv2D)             (None, 256, 256, 1)  33          conv2d_188[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6948 - acc: 0.2566 - precision_9: 0.1150 - auc_9: 0.4115 - recall_9: 0.52592021-09-24 18:42:17.929076: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 18:42:17.929163: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 18:42:18.423050: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 18:42:18.433743: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18
2021-09-24 18:42:18.438471: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.trace.json.gz
2021-09-24 18:42:18.464690: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18
2021-09-24 18:42:18.474895: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.memory_profile.json.gz
2021-09-24 18:42:18.495238: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18Dumped tool data for xplane.pb to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-184208/train/plugins/profile/2021_09_24_18_42_18/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:11 - loss: 0.6940 - acc: 0.4482 - precision_9: 0.1149 - auc_9: 0.2731 - recall_9: 0.1649WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1944s vs `on_train_batch_end` time: 0.3730s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4503 - acc: 0.8197 - precision_9: 0.6644 - auc_9: 0.7641 - recall_9: 0.3378 - val_loss: 0.4057 - val_acc: 0.8438 - val_precision_9: 0.7575 - val_auc_9: 0.8297 - val_recall_9: 0.3892
Epoch 2/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4091 - acc: 0.8416 - precision_9: 0.7424 - auc_9: 0.8276 - recall_9: 0.4109 - val_loss: 0.3124 - val_acc: 0.8792 - val_precision_9: 0.7906 - val_auc_9: 0.8952 - val_recall_9: 0.5854
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3278 - acc: 0.8736 - precision_9: 0.7854 - auc_9: 0.8796 - recall_9: 0.5725 - val_loss: 0.3112 - val_acc: 0.8769 - val_precision_9: 0.8359 - val_auc_9: 0.9002 - val_recall_9: 0.5285
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3011 - acc: 0.8841 - precision_9: 0.7969 - auc_9: 0.8976 - recall_9: 0.6237 - val_loss: 0.2549 - val_acc: 0.9032 - val_precision_9: 0.9207 - val_auc_9: 0.9326 - val_recall_9: 0.5929
Epoch 5/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2708 - acc: 0.8992 - precision_9: 0.8262 - auc_9: 0.9172 - recall_9: 0.6765 - val_loss: 0.2427 - val_acc: 0.9153 - val_precision_9: 0.8942 - val_auc_9: 0.9254 - val_recall_9: 0.6677
Epoch 6/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2349 - acc: 0.9116 - precision_9: 0.8526 - auc_9: 0.9391 - recall_9: 0.7154 - val_loss: 0.2089 - val_acc: 0.9243 - val_precision_9: 0.8923 - val_auc_9: 0.9568 - val_recall_9: 0.7405
Epoch 7/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2154 - acc: 0.9174 - precision_9: 0.8554 - auc_9: 0.9502 - recall_9: 0.7440 - val_loss: 0.1855 - val_acc: 0.9277 - val_precision_9: 0.9142 - val_auc_9: 0.9642 - val_recall_9: 0.7223
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1965 - acc: 0.9234 - precision_9: 0.8644 - auc_9: 0.9583 - recall_9: 0.7663 - val_loss: 0.1740 - val_acc: 0.9305 - val_precision_9: 0.8613 - val_auc_9: 0.9690 - val_recall_9: 0.8143
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1898 - acc: 0.9266 - precision_9: 0.8702 - auc_9: 0.9615 - recall_9: 0.7770 - val_loss: 0.2258 - val_acc: 0.8995 - val_precision_9: 0.9925 - val_auc_9: 0.9741 - val_recall_9: 0.5231
Epoch 10/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1846 - acc: 0.9279 - precision_9: 0.8676 - auc_9: 0.9646 - recall_9: 0.7870 - val_loss: 0.1951 - val_acc: 0.9282 - val_precision_9: 0.9584 - val_auc_9: 0.9626 - val_recall_9: 0.6937
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1810 - acc: 0.9290 - precision_9: 0.8732 - auc_9: 0.9651 - recall_9: 0.7864 - val_loss: 0.1642 - val_acc: 0.9365 - val_precision_9: 0.9290 - val_auc_9: 0.9714 - val_recall_9: 0.7574
Epoch 12/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1747 - acc: 0.9311 - precision_9: 0.8761 - auc_9: 0.9679 - recall_9: 0.7941 - val_loss: 0.1574 - val_acc: 0.9386 - val_precision_9: 0.8378 - val_auc_9: 0.9786 - val_recall_9: 0.8793
Epoch 13/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1695 - acc: 0.9341 - precision_9: 0.8794 - auc_9: 0.9698 - recall_9: 0.8063 - val_loss: 0.1373 - val_acc: 0.9441 - val_precision_9: 0.8605 - val_auc_9: 0.9818 - val_recall_9: 0.8826
Epoch 14/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1625 - acc: 0.9359 - precision_9: 0.8793 - auc_9: 0.9724 - recall_9: 0.8159 - val_loss: 0.1486 - val_acc: 0.9405 - val_precision_9: 0.8790 - val_auc_9: 0.9753 - val_recall_9: 0.8254
Epoch 15/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1614 - acc: 0.9368 - precision_9: 0.8848 - auc_9: 0.9731 - recall_9: 0.8141 - val_loss: 0.1526 - val_acc: 0.9380 - val_precision_9: 0.8482 - val_auc_9: 0.9784 - val_recall_9: 0.8787
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1527 - acc: 0.9386 - precision_9: 0.8839 - auc_9: 0.9763 - recall_9: 0.8250 - val_loss: 0.1326 - val_acc: 0.9490 - val_precision_9: 0.9234 - val_auc_9: 0.9783 - val_recall_9: 0.8095
Epoch 17/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1658 - acc: 0.9358 - precision_9: 0.8854 - auc_9: 0.9722 - recall_9: 0.8083 - val_loss: 0.1594 - val_acc: 0.9407 - val_precision_9: 0.9069 - val_auc_9: 0.9754 - val_recall_9: 0.8141
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1681 - acc: 0.9344 - precision_9: 0.8816 - auc_9: 0.9701 - recall_9: 0.8048 - val_loss: 0.1368 - val_acc: 0.9493 - val_precision_9: 0.9210 - val_auc_9: 0.9784 - val_recall_9: 0.8260
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1547 - acc: 0.9385 - precision_9: 0.8829 - auc_9: 0.9755 - recall_9: 0.8252 - val_loss: 0.1429 - val_acc: 0.9430 - val_precision_9: 0.9483 - val_auc_9: 0.9814 - val_recall_9: 0.7840
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1529 - acc: 0.9396 - precision_9: 0.8859 - auc_9: 0.9763 - recall_9: 0.8274 - val_loss: 0.1272 - val_acc: 0.9505 - val_precision_9: 0.8879 - val_auc_9: 0.9790 - val_recall_9: 0.8483
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1535 - acc: 0.9388 - precision_9: 0.8829 - auc_9: 0.9757 - recall_9: 0.8271 - val_loss: 0.1372 - val_acc: 0.9463 - val_precision_9: 0.8985 - val_auc_9: 0.9823 - val_recall_9: 0.8672
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1451 - acc: 0.9419 - precision_9: 0.8897 - auc_9: 0.9784 - recall_9: 0.8352 - val_loss: 0.1379 - val_acc: 0.9450 - val_precision_9: 0.8677 - val_auc_9: 0.9798 - val_recall_9: 0.8603
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1419 - acc: 0.9434 - precision_9: 0.8918 - auc_9: 0.9790 - recall_9: 0.8407 - val_loss: 0.1385 - val_acc: 0.9438 - val_precision_9: 0.8582 - val_auc_9: 0.9804 - val_recall_9: 0.8791
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1388 - acc: 0.9446 - precision_9: 0.8944 - auc_9: 0.9804 - recall_9: 0.8439 - val_loss: 0.1424 - val_acc: 0.9430 - val_precision_9: 0.8536 - val_auc_9: 0.9795 - val_recall_9: 0.8886
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1386 - acc: 0.9447 - precision_9: 0.8951 - auc_9: 0.9803 - recall_9: 0.8435 - val_loss: 0.1307 - val_acc: 0.9498 - val_precision_9: 0.8581 - val_auc_9: 0.9861 - val_recall_9: 0.9087
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1312 - acc: 0.9470 - precision_9: 0.9002 - auc_9: 0.9822 - recall_9: 0.8493 - val_loss: 0.1366 - val_acc: 0.9457 - val_precision_9: 0.9247 - val_auc_9: 0.9797 - val_recall_9: 0.8082
Epoch 27/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1329 - acc: 0.9465 - precision_9: 0.9007 - auc_9: 0.9815 - recall_9: 0.8464 - val_loss: 0.1358 - val_acc: 0.9470 - val_precision_9: 0.9228 - val_auc_9: 0.9814 - val_recall_9: 0.8197
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1318 - acc: 0.9470 - precision_9: 0.8987 - auc_9: 0.9824 - recall_9: 0.8511 - val_loss: 0.1533 - val_acc: 0.9348 - val_precision_9: 0.8002 - val_auc_9: 0.9824 - val_recall_9: 0.9238
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1310 - acc: 0.9475 - precision_9: 0.9002 - auc_9: 0.9819 - recall_9: 0.8522 - val_loss: 0.1633 - val_acc: 0.9289 - val_precision_9: 0.8028 - val_auc_9: 0.9785 - val_recall_9: 0.9079
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1257 - acc: 0.9496 - precision_9: 0.9021 - auc_9: 0.9838 - recall_9: 0.8608 - val_loss: 0.1243 - val_acc: 0.9507 - val_precision_9: 0.9217 - val_auc_9: 0.9831 - val_recall_9: 0.8385
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1224 - acc: 0.9500 - precision_9: 0.9030 - auc_9: 0.9847 - recall_9: 0.8619 - val_loss: 0.1361 - val_acc: 0.9453 - val_precision_9: 0.8501 - val_auc_9: 0.9808 - val_recall_9: 0.8971
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1217 - acc: 0.9508 - precision_9: 0.9045 - auc_9: 0.9846 - recall_9: 0.8641 - val_loss: 0.1339 - val_acc: 0.9488 - val_precision_9: 0.9067 - val_auc_9: 0.9799 - val_recall_9: 0.8477
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1172 - acc: 0.9526 - precision_9: 0.9074 - auc_9: 0.9860 - recall_9: 0.8698 - val_loss: 0.1204 - val_acc: 0.9522 - val_precision_9: 0.9128 - val_auc_9: 0.9838 - val_recall_9: 0.8551
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1090 - acc: 0.9557 - precision_9: 0.9131 - auc_9: 0.9878 - recall_9: 0.8789 - val_loss: 0.1394 - val_acc: 0.9472 - val_precision_9: 0.8643 - val_auc_9: 0.9777 - val_recall_9: 0.8667
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1068 - acc: 0.9569 - precision_9: 0.9150 - auc_9: 0.9882 - recall_9: 0.8829 - val_loss: 0.1376 - val_acc: 0.9469 - val_precision_9: 0.9111 - val_auc_9: 0.9804 - val_recall_9: 0.8466
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1062 - acc: 0.9576 - precision_9: 0.9172 - auc_9: 0.9884 - recall_9: 0.8840 - val_loss: 0.1480 - val_acc: 0.9478 - val_precision_9: 0.9160 - val_auc_9: 0.9758 - val_recall_9: 0.8307
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1033 - acc: 0.9579 - precision_9: 0.9155 - auc_9: 0.9892 - recall_9: 0.8873 - val_loss: 0.1449 - val_acc: 0.9473 - val_precision_9: 0.9522 - val_auc_9: 0.9779 - val_recall_9: 0.7959
Epoch 38/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0998 - acc: 0.9594 - precision_9: 0.9179 - auc_9: 0.9897 - recall_9: 0.8925 - val_loss: 0.1352 - val_acc: 0.9487 - val_precision_9: 0.8790 - val_auc_9: 0.9794 - val_recall_9: 0.8757
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0897 - acc: 0.9634 - precision_9: 0.9252 - auc_9: 0.9917 - recall_9: 0.9042 - val_loss: 0.1436 - val_acc: 0.9478 - val_precision_9: 0.8923 - val_auc_9: 0.9785 - val_recall_9: 0.8661
Epoch 40/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0902 - acc: 0.9638 - precision_9: 0.9268 - auc_9: 0.9914 - recall_9: 0.9043 - val_loss: 0.1466 - val_acc: 0.9499 - val_precision_9: 0.9174 - val_auc_9: 0.9734 - val_recall_9: 0.8217
Epoch 41/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0855 - acc: 0.9652 - precision_9: 0.9281 - auc_9: 0.9924 - recall_9: 0.9100 - val_loss: 0.1535 - val_acc: 0.9433 - val_precision_9: 0.8718 - val_auc_9: 0.9755 - val_recall_9: 0.8689
Epoch 42/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0797 - acc: 0.9675 - precision_9: 0.9331 - auc_9: 0.9933 - recall_9: 0.9157 - val_loss: 0.1447 - val_acc: 0.9482 - val_precision_9: 0.8930 - val_auc_9: 0.9757 - val_recall_9: 0.8565
Epoch 43/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0794 - acc: 0.9676 - precision_9: 0.9329 - auc_9: 0.9934 - recall_9: 0.9164 - val_loss: 0.1566 - val_acc: 0.9412 - val_precision_9: 0.8629 - val_auc_9: 0.9765 - val_recall_9: 0.8670
Epoch 44/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0760 - acc: 0.9690 - precision_9: 0.9364 - auc_9: 0.9939 - recall_9: 0.9191 - val_loss: 0.1899 - val_acc: 0.9456 - val_precision_9: 0.9527 - val_auc_9: 0.9630 - val_recall_9: 0.7823
Epoch 45/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0709 - acc: 0.9713 - precision_9: 0.9414 - auc_9: 0.9947 - recall_9: 0.9250 - val_loss: 0.1632 - val_acc: 0.9477 - val_precision_9: 0.9101 - val_auc_9: 0.9720 - val_recall_9: 0.8390
Epoch 46/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0742 - acc: 0.9699 - precision_9: 0.9376 - auc_9: 0.9942 - recall_9: 0.9224 - val_loss: 0.1644 - val_acc: 0.9473 - val_precision_9: 0.9020 - val_auc_9: 0.9698 - val_recall_9: 0.8408
Epoch 47/100
254/254 [==============================] - ETA: 0s - loss: 0.0601 - acc: 0.9753 - precision_9: 0.9481 - auc_9: 0.9961 - recall_9: 0.9370Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0601 - acc: 0.9753 - precision_9: 0.9481 - auc_9: 0.9961 - recall_9: 0.9370 - val_loss: 0.1774 - val_acc: 0.9469 - val_precision_9: 0.8823 - val_auc_9: 0.9726 - val_recall_9: 0.8681
Epoch 00047: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1419 - acc: 0.9456 - precision_9: 0.9041 - auc_9: 0.9803 - recall_9: 0.8467
---------------TEST METRICS----------------------
jaccard_index 0.784927595311293
test_sensitivity 0.8551538774077034
test_specifitivity 0.9712292487862523
test_accuracy 0.9452746235732491
test_precision 0.8953994921798689
test_jaccard_score 0.784927595311293
test_dicecoef 0.8748140569366386
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-192550.h5
[0. 0. 0. 0. 0.] [0.94527462 0.7849276  0.89539949 0.85515388 0.97122925]

-------------------------
Rep: 1
-------------------------

2021-09-24 19:25:51.361872: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 19:25:51.361999: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_21"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_11 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_190 (Conv2D)             (None, 256, 256, 32) 320         input_11[0][0]
__________________________________________________________________________________________________
conv2d_191 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_190[0][0]
__________________________________________________________________________________________________
max_pooling2d_40 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_191[0][0]
__________________________________________________________________________________________________
conv2d_192 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_40[0][0]
__________________________________________________________________________________________________
conv2d_193 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_192[0][0]
__________________________________________________________________________________________________
max_pooling2d_41 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_193[0][0]
__________________________________________________________________________________________________
conv2d_194 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_41[0][0]
__________________________________________________________________________________________________
conv2d_195 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_194[0][0]
__________________________________________________________________________________________________
max_pooling2d_42 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_195[0][0]
__________________________________________________________________________________________________
conv2d_196 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_42[0][0]
__________________________________________________________________________________________________
conv2d_197 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_196[0][0]
__________________________________________________________________________________________________
max_pooling2d_43 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_197[0][0]
__________________________________________________________________________________________________
conv2d_198 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_43[0][0]
__________________________________________________________________________________________________
conv2d_199 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_198[0][0]
__________________________________________________________________________________________________
up_sampling2d_40 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_199[0][0]
__________________________________________________________________________________________________
concatenate_40 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_40[0][0]
                                                                 conv2d_197[0][0]
__________________________________________________________________________________________________
conv2d_200 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_40[0][0]
__________________________________________________________________________________________________
conv2d_201 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_200[0][0]
__________________________________________________________________________________________________
up_sampling2d_41 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_201[0][0]
__________________________________________________________________________________________________
concatenate_41 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_41[0][0]
                                                                 conv2d_195[0][0]
__________________________________________________________________________________________________
conv2d_202 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_41[0][0]
__________________________________________________________________________________________________
conv2d_203 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_202[0][0]
__________________________________________________________________________________________________
up_sampling2d_42 (UpSampling2D) (None, 128, 128, 128 0           conv2d_203[0][0]
__________________________________________________________________________________________________
concatenate_42 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_42[0][0]
                                                                 conv2d_193[0][0]
__________________________________________________________________________________________________
conv2d_204 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_42[0][0]
__________________________________________________________________________________________________
conv2d_205 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_204[0][0]
__________________________________________________________________________________________________
up_sampling2d_43 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_205[0][0]
__________________________________________________________________________________________________
concatenate_43 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_43[0][0]
                                                                 conv2d_191[0][0]
__________________________________________________________________________________________________
conv2d_206 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_43[0][0]
__________________________________________________________________________________________________
conv2d_207 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_206[0][0]
__________________________________________________________________________________________________
conv2d_208 (Conv2D)             (None, 256, 256, 1)  33          conv2d_207[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6928 - acc: 0.5776 - precision_10: 0.2654 - auc_10: 0.5583 - recall_10: 0.88772021-09-24 19:26:00.356337: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 19:26:00.356425: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 19:26:00.857628: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 19:26:00.867904: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00
2021-09-24 19:26:00.870958: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.trace.json.gz
2021-09-24 19:26:00.897591: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00
2021-09-24 19:26:00.907670: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.memory_profile.json.gz
2021-09-24 19:26:00.925555: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00Dumped tool data for xplane.pb to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-192551/train/plugins/profile/2021_09_24_19_26_00/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:11 - loss: 0.6927 - acc: 0.6088 - precision_10: 0.2654 - auc_10: 0.3245 - recall_10: 0.2783WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2022s vs `on_train_batch_end` time: 0.3682s). Check your callbacks.
254/254 [==============================] - 55s 218ms/step - loss: 0.4642 - acc: 0.8255 - precision_10: 0.6903 - auc_10: 0.7506 - recall_10: 0.3519 - val_loss: 0.4084 - val_acc: 0.8531 - val_precision_10: 0.7577 - val_auc_10: 0.8019 - val_recall_10: 0.4534
Epoch 2/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4230 - acc: 0.8346 - precision_10: 0.7040 - auc_10: 0.7837 - recall_10: 0.4071 - val_loss: 0.4148 - val_acc: 0.8416 - val_precision_10: 0.8501 - val_auc_10: 0.7817 - val_recall_10: 0.3067
Epoch 3/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4220 - acc: 0.8360 - precision_10: 0.7313 - auc_10: 0.7740 - recall_10: 0.3834 - val_loss: 0.4104 - val_acc: 0.8449 - val_precision_10: 0.8553 - val_auc_10: 0.7785 - val_recall_10: 0.3312
Epoch 4/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4159 - acc: 0.8382 - precision_10: 0.7502 - auc_10: 0.7779 - recall_10: 0.3787 - val_loss: 0.3944 - val_acc: 0.8521 - val_precision_10: 0.8495 - val_auc_10: 0.7917 - val_recall_10: 0.3640
Epoch 5/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4094 - acc: 0.8441 - precision_10: 0.7582 - auc_10: 0.7959 - recall_10: 0.4110 - val_loss: 0.3472 - val_acc: 0.8713 - val_precision_10: 0.8094 - val_auc_10: 0.8389 - val_recall_10: 0.4904
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3425 - acc: 0.8705 - precision_10: 0.8195 - auc_10: 0.8579 - recall_10: 0.5155 - val_loss: 0.3342 - val_acc: 0.8657 - val_precision_10: 0.6820 - val_auc_10: 0.8957 - val_recall_10: 0.7132
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3006 - acc: 0.8865 - precision_10: 0.8395 - auc_10: 0.8911 - recall_10: 0.5883 - val_loss: 0.2636 - val_acc: 0.9027 - val_precision_10: 0.8802 - val_auc_10: 0.9158 - val_recall_10: 0.6198
Epoch 8/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2718 - acc: 0.9005 - precision_10: 0.8354 - auc_10: 0.9118 - recall_10: 0.6729 - val_loss: 0.2861 - val_acc: 0.8993 - val_precision_10: 0.8843 - val_auc_10: 0.8896 - val_recall_10: 0.6220
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2591 - acc: 0.9040 - precision_10: 0.8396 - auc_10: 0.9222 - recall_10: 0.6879 - val_loss: 0.2664 - val_acc: 0.8944 - val_precision_10: 0.9620 - val_auc_10: 0.9352 - val_recall_10: 0.5152
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2329 - acc: 0.9117 - precision_10: 0.8499 - auc_10: 0.9396 - recall_10: 0.7189 - val_loss: 0.2331 - val_acc: 0.9167 - val_precision_10: 0.9318 - val_auc_10: 0.9340 - val_recall_10: 0.6580
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2178 - acc: 0.9176 - precision_10: 0.8636 - auc_10: 0.9476 - recall_10: 0.7355 - val_loss: 0.1993 - val_acc: 0.9220 - val_precision_10: 0.9185 - val_auc_10: 0.9591 - val_recall_10: 0.6924
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2036 - acc: 0.9229 - precision_10: 0.8722 - auc_10: 0.9546 - recall_10: 0.7541 - val_loss: 0.1891 - val_acc: 0.9293 - val_precision_10: 0.8401 - val_auc_10: 0.9655 - val_recall_10: 0.8210
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2040 - acc: 0.9219 - precision_10: 0.8718 - auc_10: 0.9551 - recall_10: 0.7497 - val_loss: 0.1779 - val_acc: 0.9318 - val_precision_10: 0.8453 - val_auc_10: 0.9715 - val_recall_10: 0.8351
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1893 - acc: 0.9283 - precision_10: 0.8773 - auc_10: 0.9611 - recall_10: 0.7776 - val_loss: 0.1840 - val_acc: 0.9323 - val_precision_10: 0.9209 - val_auc_10: 0.9655 - val_recall_10: 0.7348
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1867 - acc: 0.9300 - precision_10: 0.8822 - auc_10: 0.9616 - recall_10: 0.7808 - val_loss: 0.1667 - val_acc: 0.9363 - val_precision_10: 0.8643 - val_auc_10: 0.9717 - val_recall_10: 0.8470
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1747 - acc: 0.9320 - precision_10: 0.8836 - auc_10: 0.9682 - recall_10: 0.7900 - val_loss: 0.1446 - val_acc: 0.9429 - val_precision_10: 0.9011 - val_auc_10: 0.9759 - val_recall_10: 0.7992
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1731 - acc: 0.9333 - precision_10: 0.8890 - auc_10: 0.9682 - recall_10: 0.7905 - val_loss: 0.1497 - val_acc: 0.9416 - val_precision_10: 0.8873 - val_auc_10: 0.9771 - val_recall_10: 0.8412
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1683 - acc: 0.9357 - precision_10: 0.8923 - auc_10: 0.9691 - recall_10: 0.7996 - val_loss: 0.1397 - val_acc: 0.9461 - val_precision_10: 0.9130 - val_auc_10: 0.9779 - val_recall_10: 0.8175
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1638 - acc: 0.9365 - precision_10: 0.8914 - auc_10: 0.9718 - recall_10: 0.8049 - val_loss: 0.1514 - val_acc: 0.9394 - val_precision_10: 0.9329 - val_auc_10: 0.9773 - val_recall_10: 0.7810
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1573 - acc: 0.9393 - precision_10: 0.8945 - auc_10: 0.9741 - recall_10: 0.8159 - val_loss: 0.1363 - val_acc: 0.9482 - val_precision_10: 0.8928 - val_auc_10: 0.9766 - val_recall_10: 0.8287
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1611 - acc: 0.9377 - precision_10: 0.8932 - auc_10: 0.9724 - recall_10: 0.8091 - val_loss: 0.1398 - val_acc: 0.9447 - val_precision_10: 0.9153 - val_auc_10: 0.9807 - val_recall_10: 0.8396
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1493 - acc: 0.9419 - precision_10: 0.8997 - auc_10: 0.9763 - recall_10: 0.8234 - val_loss: 0.1413 - val_acc: 0.9460 - val_precision_10: 0.8809 - val_auc_10: 0.9785 - val_recall_10: 0.8489
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1497 - acc: 0.9420 - precision_10: 0.8994 - auc_10: 0.9762 - recall_10: 0.8245 - val_loss: 0.1496 - val_acc: 0.9403 - val_precision_10: 0.8472 - val_auc_10: 0.9777 - val_recall_10: 0.8752
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1479 - acc: 0.9422 - precision_10: 0.8992 - auc_10: 0.9774 - recall_10: 0.8257 - val_loss: 0.1625 - val_acc: 0.9369 - val_precision_10: 0.8344 - val_auc_10: 0.9779 - val_recall_10: 0.8835
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1448 - acc: 0.9434 - precision_10: 0.8990 - auc_10: 0.9785 - recall_10: 0.8320 - val_loss: 0.1520 - val_acc: 0.9452 - val_precision_10: 0.8902 - val_auc_10: 0.9775 - val_recall_10: 0.8398
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1380 - acc: 0.9457 - precision_10: 0.9012 - auc_10: 0.9802 - recall_10: 0.8412 - val_loss: 0.1408 - val_acc: 0.9460 - val_precision_10: 0.8938 - val_auc_10: 0.9778 - val_recall_10: 0.8441
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1347 - acc: 0.9476 - precision_10: 0.9056 - auc_10: 0.9808 - recall_10: 0.8462 - val_loss: 0.1271 - val_acc: 0.9493 - val_precision_10: 0.9176 - val_auc_10: 0.9837 - val_recall_10: 0.8372
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1285 - acc: 0.9487 - precision_10: 0.9056 - auc_10: 0.9834 - recall_10: 0.8519 - val_loss: 0.1469 - val_acc: 0.9444 - val_precision_10: 0.8568 - val_auc_10: 0.9795 - val_recall_10: 0.8868
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1375 - acc: 0.9471 - precision_10: 0.9042 - auc_10: 0.9799 - recall_10: 0.8451 - val_loss: 0.1354 - val_acc: 0.9448 - val_precision_10: 0.8667 - val_auc_10: 0.9815 - val_recall_10: 0.8929
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1274 - acc: 0.9494 - precision_10: 0.9059 - auc_10: 0.9835 - recall_10: 0.8551 - val_loss: 0.1275 - val_acc: 0.9502 - val_precision_10: 0.9137 - val_auc_10: 0.9823 - val_recall_10: 0.8444
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1222 - acc: 0.9514 - precision_10: 0.9114 - auc_10: 0.9848 - recall_10: 0.8591 - val_loss: 0.1463 - val_acc: 0.9453 - val_precision_10: 0.8743 - val_auc_10: 0.9748 - val_recall_10: 0.8631
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1198 - acc: 0.9526 - precision_10: 0.9119 - auc_10: 0.9852 - recall_10: 0.8647 - val_loss: 0.1334 - val_acc: 0.9480 - val_precision_10: 0.9147 - val_auc_10: 0.9810 - val_recall_10: 0.8346
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1184 - acc: 0.9527 - precision_10: 0.9134 - auc_10: 0.9857 - recall_10: 0.8632 - val_loss: 0.1300 - val_acc: 0.9498 - val_precision_10: 0.8821 - val_auc_10: 0.9819 - val_recall_10: 0.8794
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1124 - acc: 0.9553 - precision_10: 0.9163 - auc_10: 0.9872 - recall_10: 0.8732 - val_loss: 0.1400 - val_acc: 0.9464 - val_precision_10: 0.8521 - val_auc_10: 0.9797 - val_recall_10: 0.8789
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1023 - acc: 0.9586 - precision_10: 0.9205 - auc_10: 0.9894 - recall_10: 0.8854 - val_loss: 0.1360 - val_acc: 0.9507 - val_precision_10: 0.9155 - val_auc_10: 0.9807 - val_recall_10: 0.8605
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0972 - acc: 0.9606 - precision_10: 0.9240 - auc_10: 0.9906 - recall_10: 0.8915 - val_loss: 0.1693 - val_acc: 0.9485 - val_precision_10: 0.9136 - val_auc_10: 0.9719 - val_recall_10: 0.8370
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1052 - acc: 0.9582 - precision_10: 0.9208 - auc_10: 0.9887 - recall_10: 0.8829 - val_loss: 0.1578 - val_acc: 0.9449 - val_precision_10: 0.9409 - val_auc_10: 0.9710 - val_recall_10: 0.7948
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0922 - acc: 0.9624 - precision_10: 0.9257 - auc_10: 0.9915 - recall_10: 0.8983 - val_loss: 0.1451 - val_acc: 0.9487 - val_precision_10: 0.8965 - val_auc_10: 0.9765 - val_recall_10: 0.8539
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1040 - acc: 0.9586 - precision_10: 0.9215 - auc_10: 0.9889 - recall_10: 0.8842 - val_loss: 0.1462 - val_acc: 0.9452 - val_precision_10: 0.8910 - val_auc_10: 0.9771 - val_recall_10: 0.8542
Epoch 40/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0892 - acc: 0.9640 - precision_10: 0.9287 - auc_10: 0.9920 - recall_10: 0.9028 - val_loss: 0.1772 - val_acc: 0.9493 - val_precision_10: 0.9106 - val_auc_10: 0.9660 - val_recall_10: 0.8261
Epoch 41/100
254/254 [==============================] - ETA: 0s - loss: 0.0819 - acc: 0.9662 - precision_10: 0.9321 - auc_10: 0.9934 - recall_10: 0.9102Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 216ms/step - loss: 0.0819 - acc: 0.9662 - precision_10: 0.9321 - auc_10: 0.9934 - recall_10: 0.9102 - val_loss: 0.1669 - val_acc: 0.9439 - val_precision_10: 0.8858 - val_auc_10: 0.9723 - val_recall_10: 0.8543
Epoch 00041: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1419 - acc: 0.9450 - precision_10: 0.9066 - auc_10: 0.9801 - recall_10: 0.8404
---------------TEST METRICS----------------------
jaccard_index 0.7894842437126111
test_sensitivity 0.8479348506550531
test_specifitivity 0.972883470805344
test_accuracy 0.94494477400543
test_precision 0.90005715232443
test_jaccard_score 0.7894842437126111
test_dicecoef 0.8732189000136814
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-200359.h5
[0.94527462 0.7849276  0.89539949 0.85515388 0.97122925] [0.94494477 0.78948424 0.90005715 0.84793485 0.97288347]

-------------------------
Rep: 2
-------------------------

2021-09-24 20:04:00.212116: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 20:04:00.212246: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_23"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_12 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_209 (Conv2D)             (None, 256, 256, 32) 320         input_12[0][0]
__________________________________________________________________________________________________
conv2d_210 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_209[0][0]
__________________________________________________________________________________________________
max_pooling2d_44 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_210[0][0]
__________________________________________________________________________________________________
conv2d_211 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_44[0][0]
__________________________________________________________________________________________________
conv2d_212 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_211[0][0]
__________________________________________________________________________________________________
max_pooling2d_45 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_212[0][0]
__________________________________________________________________________________________________
conv2d_213 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_45[0][0]
__________________________________________________________________________________________________
conv2d_214 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_213[0][0]
__________________________________________________________________________________________________
max_pooling2d_46 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_214[0][0]
__________________________________________________________________________________________________
conv2d_215 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_46[0][0]
__________________________________________________________________________________________________
conv2d_216 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_215[0][0]
__________________________________________________________________________________________________
max_pooling2d_47 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_216[0][0]
__________________________________________________________________________________________________
conv2d_217 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_47[0][0]
__________________________________________________________________________________________________
conv2d_218 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_217[0][0]
__________________________________________________________________________________________________
up_sampling2d_44 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_218[0][0]
__________________________________________________________________________________________________
concatenate_44 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_44[0][0]
                                                                 conv2d_216[0][0]
__________________________________________________________________________________________________
conv2d_219 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_44[0][0]
__________________________________________________________________________________________________
conv2d_220 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_219[0][0]
__________________________________________________________________________________________________
up_sampling2d_45 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_220[0][0]
__________________________________________________________________________________________________
concatenate_45 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_45[0][0]
                                                                 conv2d_214[0][0]
__________________________________________________________________________________________________
conv2d_221 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_45[0][0]
__________________________________________________________________________________________________
conv2d_222 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_221[0][0]
__________________________________________________________________________________________________
up_sampling2d_46 (UpSampling2D) (None, 128, 128, 128 0           conv2d_222[0][0]
__________________________________________________________________________________________________
concatenate_46 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_46[0][0]
                                                                 conv2d_212[0][0]
__________________________________________________________________________________________________
conv2d_223 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_46[0][0]
__________________________________________________________________________________________________
conv2d_224 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_223[0][0]
__________________________________________________________________________________________________
up_sampling2d_47 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_224[0][0]
__________________________________________________________________________________________________
concatenate_47 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_47[0][0]
                                                                 conv2d_210[0][0]
__________________________________________________________________________________________________
conv2d_225 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_47[0][0]
__________________________________________________________________________________________________
conv2d_226 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_225[0][0]
__________________________________________________________________________________________________
conv2d_227 (Conv2D)             (None, 256, 256, 1)  33          conv2d_226[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6955 - acc: 0.1738 - precision_11: 0.1645 - auc_11: 0.4135 - recall_11: 0.98732021-09-24 20:04:09.291235: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 20:04:09.291309: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 20:04:09.788942: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 20:04:09.799097: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09
2021-09-24 20:04:09.802333: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.trace.json.gz
2021-09-24 20:04:09.828263: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09
2021-09-24 20:04:09.836991: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.memory_profile.json.gz
2021-09-24 20:04:09.852255: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09Dumped tool data for xplane.pb to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-200400/train/plugins/profile/2021_09_24_20_04_09/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:10 - loss: 0.6942 - acc: 0.4069 - precision_11: 0.1645 - auc_11: 0.2905 - recall_11: 0.3096WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1987s vs `on_train_batch_end` time: 0.3640s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4551 - acc: 0.8214 - precision_11: 0.6869 - auc_11: 0.7508 - recall_11: 0.3216 - val_loss: 0.3835 - val_acc: 0.8578 - val_precision_11: 0.8230 - val_auc_11: 0.8191 - val_recall_11: 0.4209
Epoch 2/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4533 - acc: 0.8398 - precision_11: 0.7292 - auc_11: 0.7932 - recall_11: 0.4137 - val_loss: 0.4178 - val_acc: 0.8391 - val_precision_11: 0.9245 - val_auc_11: 0.7847 - val_recall_11: 0.2624
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3771 - acc: 0.8545 - precision_11: 0.7771 - auc_11: 0.8287 - recall_11: 0.4597 - val_loss: 0.3481 - val_acc: 0.8713 - val_precision_11: 0.9336 - val_auc_11: 0.8578 - val_recall_11: 0.4290
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3611 - acc: 0.8625 - precision_11: 0.8071 - auc_11: 0.8443 - recall_11: 0.4797 - val_loss: 0.3058 - val_acc: 0.8838 - val_precision_11: 0.8822 - val_auc_11: 0.8916 - val_recall_11: 0.5192
Epoch 5/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2945 - acc: 0.8925 - precision_11: 0.8318 - auc_11: 0.8967 - recall_11: 0.6309 - val_loss: 0.2321 - val_acc: 0.9140 - val_precision_11: 0.8938 - val_auc_11: 0.9316 - val_recall_11: 0.6610
Epoch 6/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2435 - acc: 0.9096 - precision_11: 0.8576 - auc_11: 0.9326 - recall_11: 0.6987 - val_loss: 0.2055 - val_acc: 0.9231 - val_precision_11: 0.8791 - val_auc_11: 0.9536 - val_recall_11: 0.7485
Epoch 7/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2231 - acc: 0.9166 - precision_11: 0.8589 - auc_11: 0.9446 - recall_11: 0.7357 - val_loss: 0.2002 - val_acc: 0.9233 - val_precision_11: 0.9448 - val_auc_11: 0.9585 - val_recall_11: 0.6730
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2092 - acc: 0.9209 - precision_11: 0.8685 - auc_11: 0.9514 - recall_11: 0.7477 - val_loss: 0.1893 - val_acc: 0.9244 - val_precision_11: 0.8678 - val_auc_11: 0.9616 - val_recall_11: 0.7731
Epoch 9/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2041 - acc: 0.9227 - precision_11: 0.8672 - auc_11: 0.9542 - recall_11: 0.7593 - val_loss: 0.2254 - val_acc: 0.9092 - val_precision_11: 0.9872 - val_auc_11: 0.9642 - val_recall_11: 0.5730
Epoch 10/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1926 - acc: 0.9265 - precision_11: 0.8743 - auc_11: 0.9597 - recall_11: 0.7713 - val_loss: 0.1805 - val_acc: 0.9311 - val_precision_11: 0.9442 - val_auc_11: 0.9663 - val_recall_11: 0.7200
Epoch 11/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1864 - acc: 0.9280 - precision_11: 0.8760 - auc_11: 0.9628 - recall_11: 0.7775 - val_loss: 0.1613 - val_acc: 0.9367 - val_precision_11: 0.8896 - val_auc_11: 0.9721 - val_recall_11: 0.7998
Epoch 12/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1814 - acc: 0.9296 - precision_11: 0.8762 - auc_11: 0.9655 - recall_11: 0.7859 - val_loss: 0.1562 - val_acc: 0.9411 - val_precision_11: 0.8669 - val_auc_11: 0.9771 - val_recall_11: 0.8513
Epoch 13/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1728 - acc: 0.9330 - precision_11: 0.8833 - auc_11: 0.9690 - recall_11: 0.7956 - val_loss: 0.1445 - val_acc: 0.9417 - val_precision_11: 0.8634 - val_auc_11: 0.9801 - val_recall_11: 0.8652
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1671 - acc: 0.9355 - precision_11: 0.8868 - auc_11: 0.9702 - recall_11: 0.8047 - val_loss: 0.1447 - val_acc: 0.9419 - val_precision_11: 0.9156 - val_auc_11: 0.9769 - val_recall_11: 0.7912
Epoch 15/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1603 - acc: 0.9381 - precision_11: 0.8931 - auc_11: 0.9732 - recall_11: 0.8109 - val_loss: 0.1559 - val_acc: 0.9374 - val_precision_11: 0.8390 - val_auc_11: 0.9788 - val_recall_11: 0.8894
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1588 - acc: 0.9374 - precision_11: 0.8894 - auc_11: 0.9740 - recall_11: 0.8119 - val_loss: 0.1333 - val_acc: 0.9481 - val_precision_11: 0.9032 - val_auc_11: 0.9775 - val_recall_11: 0.8261
Epoch 17/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1580 - acc: 0.9386 - precision_11: 0.8930 - auc_11: 0.9741 - recall_11: 0.8139 - val_loss: 0.1383 - val_acc: 0.9426 - val_precision_11: 0.8731 - val_auc_11: 0.9818 - val_recall_11: 0.8649
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1552 - acc: 0.9394 - precision_11: 0.8958 - auc_11: 0.9750 - recall_11: 0.8151 - val_loss: 0.1366 - val_acc: 0.9458 - val_precision_11: 0.9435 - val_auc_11: 0.9796 - val_recall_11: 0.7848
Epoch 19/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1501 - acc: 0.9411 - precision_11: 0.8959 - auc_11: 0.9769 - recall_11: 0.8239 - val_loss: 0.1426 - val_acc: 0.9419 - val_precision_11: 0.9389 - val_auc_11: 0.9815 - val_recall_11: 0.7877
Epoch 20/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1505 - acc: 0.9413 - precision_11: 0.8949 - auc_11: 0.9763 - recall_11: 0.8259 - val_loss: 0.1229 - val_acc: 0.9513 - val_precision_11: 0.8758 - val_auc_11: 0.9812 - val_recall_11: 0.8686
Epoch 21/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1584 - acc: 0.9380 - precision_11: 0.8832 - auc_11: 0.9745 - recall_11: 0.8222 - val_loss: 0.1410 - val_acc: 0.9457 - val_precision_11: 0.8868 - val_auc_11: 0.9832 - val_recall_11: 0.8785
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1482 - acc: 0.9419 - precision_11: 0.8950 - auc_11: 0.9773 - recall_11: 0.8291 - val_loss: 0.1374 - val_acc: 0.9439 - val_precision_11: 0.8661 - val_auc_11: 0.9797 - val_recall_11: 0.8563
Epoch 23/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1381 - acc: 0.9453 - precision_11: 0.8967 - auc_11: 0.9802 - recall_11: 0.8445 - val_loss: 0.1333 - val_acc: 0.9454 - val_precision_11: 0.8597 - val_auc_11: 0.9828 - val_recall_11: 0.8863
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1369 - acc: 0.9456 - precision_11: 0.9009 - auc_11: 0.9808 - recall_11: 0.8414 - val_loss: 0.1414 - val_acc: 0.9441 - val_precision_11: 0.8529 - val_auc_11: 0.9802 - val_recall_11: 0.8957
Epoch 25/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1364 - acc: 0.9461 - precision_11: 0.8986 - auc_11: 0.9810 - recall_11: 0.8466 - val_loss: 0.1325 - val_acc: 0.9475 - val_precision_11: 0.8445 - val_auc_11: 0.9868 - val_recall_11: 0.9157
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1283 - acc: 0.9488 - precision_11: 0.9049 - auc_11: 0.9828 - recall_11: 0.8531 - val_loss: 0.1349 - val_acc: 0.9469 - val_precision_11: 0.9374 - val_auc_11: 0.9796 - val_recall_11: 0.8014
Epoch 27/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1285 - acc: 0.9492 - precision_11: 0.9103 - auc_11: 0.9827 - recall_11: 0.8489 - val_loss: 0.1213 - val_acc: 0.9519 - val_precision_11: 0.9118 - val_auc_11: 0.9846 - val_recall_11: 0.8566
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1237 - acc: 0.9503 - precision_11: 0.9073 - auc_11: 0.9844 - recall_11: 0.8580 - val_loss: 0.1513 - val_acc: 0.9352 - val_precision_11: 0.8030 - val_auc_11: 0.9821 - val_recall_11: 0.9213
Epoch 29/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1297 - acc: 0.9481 - precision_11: 0.9014 - auc_11: 0.9827 - recall_11: 0.8538 - val_loss: 0.1591 - val_acc: 0.9316 - val_precision_11: 0.8087 - val_auc_11: 0.9803 - val_recall_11: 0.9131
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1206 - acc: 0.9519 - precision_11: 0.9103 - auc_11: 0.9850 - recall_11: 0.8630 - val_loss: 0.1129 - val_acc: 0.9548 - val_precision_11: 0.9116 - val_auc_11: 0.9866 - val_recall_11: 0.8710
Epoch 31/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1211 - acc: 0.9514 - precision_11: 0.9095 - auc_11: 0.9850 - recall_11: 0.8613 - val_loss: 0.1334 - val_acc: 0.9472 - val_precision_11: 0.8657 - val_auc_11: 0.9802 - val_recall_11: 0.8858
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1165 - acc: 0.9533 - precision_11: 0.9113 - auc_11: 0.9860 - recall_11: 0.8688 - val_loss: 0.1238 - val_acc: 0.9500 - val_precision_11: 0.8833 - val_auc_11: 0.9838 - val_recall_11: 0.8827
Epoch 33/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1112 - acc: 0.9550 - precision_11: 0.9125 - auc_11: 0.9875 - recall_11: 0.8760 - val_loss: 0.1224 - val_acc: 0.9514 - val_precision_11: 0.9114 - val_auc_11: 0.9831 - val_recall_11: 0.8522
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1146 - acc: 0.9536 - precision_11: 0.9106 - auc_11: 0.9866 - recall_11: 0.8712 - val_loss: 0.1344 - val_acc: 0.9458 - val_precision_11: 0.8383 - val_auc_11: 0.9822 - val_recall_11: 0.8962
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1056 - acc: 0.9569 - precision_11: 0.9148 - auc_11: 0.9888 - recall_11: 0.8834 - val_loss: 0.1275 - val_acc: 0.9491 - val_precision_11: 0.8831 - val_auc_11: 0.9836 - val_recall_11: 0.8921
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0992 - acc: 0.9596 - precision_11: 0.9209 - auc_11: 0.9901 - recall_11: 0.8898 - val_loss: 0.1350 - val_acc: 0.9493 - val_precision_11: 0.9060 - val_auc_11: 0.9790 - val_recall_11: 0.8501
Epoch 37/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0988 - acc: 0.9596 - precision_11: 0.9180 - auc_11: 0.9902 - recall_11: 0.8933 - val_loss: 0.1403 - val_acc: 0.9483 - val_precision_11: 0.9490 - val_auc_11: 0.9792 - val_recall_11: 0.8039
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1004 - acc: 0.9590 - precision_11: 0.9171 - auc_11: 0.9898 - recall_11: 0.8913 - val_loss: 0.1387 - val_acc: 0.9433 - val_precision_11: 0.8286 - val_auc_11: 0.9834 - val_recall_11: 0.9200
Epoch 39/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0942 - acc: 0.9616 - precision_11: 0.9229 - auc_11: 0.9911 - recall_11: 0.8977 - val_loss: 0.1414 - val_acc: 0.9479 - val_precision_11: 0.9052 - val_auc_11: 0.9780 - val_recall_11: 0.8512
Epoch 40/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0855 - acc: 0.9648 - precision_11: 0.9281 - auc_11: 0.9927 - recall_11: 0.9078 - val_loss: 0.1321 - val_acc: 0.9507 - val_precision_11: 0.8785 - val_auc_11: 0.9784 - val_recall_11: 0.8730
Epoch 41/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0836 - acc: 0.9657 - precision_11: 0.9287 - auc_11: 0.9930 - recall_11: 0.9113 - val_loss: 0.1469 - val_acc: 0.9458 - val_precision_11: 0.8768 - val_auc_11: 0.9782 - val_recall_11: 0.8758
Epoch 42/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0797 - acc: 0.9672 - precision_11: 0.9332 - auc_11: 0.9936 - recall_11: 0.9142 - val_loss: 0.1365 - val_acc: 0.9498 - val_precision_11: 0.9234 - val_auc_11: 0.9781 - val_recall_11: 0.8303
Epoch 43/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0793 - acc: 0.9675 - precision_11: 0.9327 - auc_11: 0.9937 - recall_11: 0.9159 - val_loss: 0.1492 - val_acc: 0.9500 - val_precision_11: 0.8958 - val_auc_11: 0.9773 - val_recall_11: 0.8712
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.0728 - acc: 0.9702 - precision_11: 0.9381 - auc_11: 0.9946 - recall_11: 0.9233Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0728 - acc: 0.9702 - precision_11: 0.9381 - auc_11: 0.9946 - recall_11: 0.9233 - val_loss: 0.1858 - val_acc: 0.9472 - val_precision_11: 0.9443 - val_auc_11: 0.9673 - val_recall_11: 0.7985
Epoch 00044: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1326 - acc: 0.9476 - precision_11: 0.8912 - auc_11: 0.9827 - recall_11: 0.8722
---------------TEST METRICS----------------------
jaccard_index 0.787345794409171
test_sensitivity 0.8777133360097357
test_specifitivity 0.9652134588272879
test_accuracy 0.9456483015777372
test_precision 0.8790313257882463
test_jaccard_score 0.787345794409171
test_dicecoef 0.8783718364910403
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-204459.h5
[1.8902194  1.57441184 1.79545664 1.70308873 1.94411272] [0.9456483  0.78734579 0.87903133 0.87771334 0.96521346]

-------------------------
Averaged metrics for Baseline + Per Channel Normalization - lesion: [0.94528923 0.78725254 0.89149599 0.86026735 0.96977539]
-------------------------


-------------------------
RUN: Baseline + Gaussian Blur - lesion, PARAMS: {'per_channel_normalization': True, 'gaussian_blur': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 20:45:00.662432: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 20:45:00.662545: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: -3.4061795605182844e-18,
Validation samples: 383, channel mean: 0.005347822958799939
Model built.
Model: "functional_25"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_13 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_228 (Conv2D)             (None, 256, 256, 32) 320         input_13[0][0]
__________________________________________________________________________________________________
conv2d_229 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_228[0][0]
__________________________________________________________________________________________________
max_pooling2d_48 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_229[0][0]
__________________________________________________________________________________________________
conv2d_230 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_48[0][0]
__________________________________________________________________________________________________
conv2d_231 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_230[0][0]
__________________________________________________________________________________________________
max_pooling2d_49 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_231[0][0]
__________________________________________________________________________________________________
conv2d_232 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_49[0][0]
__________________________________________________________________________________________________
conv2d_233 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_232[0][0]
__________________________________________________________________________________________________
max_pooling2d_50 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_233[0][0]
__________________________________________________________________________________________________
conv2d_234 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_50[0][0]
__________________________________________________________________________________________________
conv2d_235 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_234[0][0]
__________________________________________________________________________________________________
max_pooling2d_51 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_235[0][0]
__________________________________________________________________________________________________
conv2d_236 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_51[0][0]
__________________________________________________________________________________________________
conv2d_237 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_236[0][0]
__________________________________________________________________________________________________
up_sampling2d_48 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_237[0][0]
__________________________________________________________________________________________________
concatenate_48 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_48[0][0]
                                                                 conv2d_235[0][0]
__________________________________________________________________________________________________
conv2d_238 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_48[0][0]
__________________________________________________________________________________________________
conv2d_239 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_238[0][0]
__________________________________________________________________________________________________
up_sampling2d_49 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_239[0][0]
__________________________________________________________________________________________________
concatenate_49 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_49[0][0]
                                                                 conv2d_233[0][0]
__________________________________________________________________________________________________
conv2d_240 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_49[0][0]
__________________________________________________________________________________________________
conv2d_241 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_240[0][0]
__________________________________________________________________________________________________
up_sampling2d_50 (UpSampling2D) (None, 128, 128, 128 0           conv2d_241[0][0]
__________________________________________________________________________________________________
concatenate_50 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_50[0][0]
                                                                 conv2d_231[0][0]
__________________________________________________________________________________________________
conv2d_242 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_50[0][0]
__________________________________________________________________________________________________
conv2d_243 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_242[0][0]
__________________________________________________________________________________________________
up_sampling2d_51 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_243[0][0]
__________________________________________________________________________________________________
concatenate_51 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_51[0][0]
                                                                 conv2d_229[0][0]
__________________________________________________________________________________________________
conv2d_244 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_51[0][0]
__________________________________________________________________________________________________
conv2d_245 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_244[0][0]
__________________________________________________________________________________________________
conv2d_246 (Conv2D)             (None, 256, 256, 1)  33          conv2d_245[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6939 - acc: 0.3259 - precision_12: 0.0409 - auc_12: 0.2446 - recall_12: 0.13802021-09-24 20:45:39.638960: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 20:45:39.639945: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 20:45:40.133743: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 20:45:40.143652: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40
2021-09-24 20:45:40.146695: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.trace.json.gz
2021-09-24 20:45:40.172452: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40
2021-09-24 20:45:40.181096: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.memory_profile.json.gz
2021-09-24 20:45:40.196533: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40Dumped tool data for xplane.pb to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-204500/train/plugins/profile/2021_09_24_20_45_40/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:10 - loss: 0.6967 - acc: 0.4830 - precision_12: 0.0409 - auc_12: 0.1843 - recall_12: 0.0433WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2009s vs `on_train_batch_end` time: 0.3569s). Check your callbacks.
254/254 [==============================] - 55s 218ms/step - loss: 0.4788 - acc: 0.8188 - precision_12: 0.6721 - auc_12: 0.7437 - recall_12: 0.3186 - val_loss: 0.4145 - val_acc: 0.8492 - val_precision_12: 0.8743 - val_auc_12: 0.7938 - val_recall_12: 0.3388
Epoch 2/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4235 - acc: 0.8311 - precision_12: 0.7001 - auc_12: 0.7765 - recall_12: 0.3842 - val_loss: 0.3924 - val_acc: 0.8559 - val_precision_12: 0.8024 - val_auc_12: 0.7843 - val_recall_12: 0.4247
Epoch 3/100
254/254 [==============================] - 54s 214ms/step - loss: 0.4279 - acc: 0.8357 - precision_12: 0.7241 - auc_12: 0.7827 - recall_12: 0.3893 - val_loss: 0.4212 - val_acc: 0.8285 - val_precision_12: 0.9674 - val_auc_12: 0.7949 - val_recall_12: 0.2054
Epoch 4/100
254/254 [==============================] - 54s 214ms/step - loss: 0.3990 - acc: 0.8437 - precision_12: 0.7655 - auc_12: 0.8049 - recall_12: 0.4011 - val_loss: 0.3532 - val_acc: 0.8645 - val_precision_12: 0.9103 - val_auc_12: 0.8478 - val_recall_12: 0.3974
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3899 - acc: 0.8509 - precision_12: 0.7867 - auc_12: 0.8337 - recall_12: 0.4271 - val_loss: 0.3453 - val_acc: 0.8654 - val_precision_12: 0.8517 - val_auc_12: 0.8491 - val_recall_12: 0.4190
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3474 - acc: 0.8595 - precision_12: 0.8070 - auc_12: 0.8598 - recall_12: 0.4616 - val_loss: 0.3423 - val_acc: 0.8671 - val_precision_12: 0.8659 - val_auc_12: 0.8726 - val_recall_12: 0.4580
Epoch 7/100
254/254 [==============================] - 54s 215ms/step - loss: 0.3392 - acc: 0.8630 - precision_12: 0.8050 - auc_12: 0.8654 - recall_12: 0.4845 - val_loss: 0.3192 - val_acc: 0.8741 - val_precision_12: 0.8379 - val_auc_12: 0.8800 - val_recall_12: 0.4944
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2899 - acc: 0.8910 - precision_12: 0.8401 - auc_12: 0.9019 - recall_12: 0.6132 - val_loss: 0.2403 - val_acc: 0.9075 - val_precision_12: 0.8350 - val_auc_12: 0.9337 - val_recall_12: 0.7204
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2374 - acc: 0.9103 - precision_12: 0.8477 - auc_12: 0.9377 - recall_12: 0.7137 - val_loss: 0.2570 - val_acc: 0.8956 - val_precision_12: 0.9866 - val_auc_12: 0.9535 - val_recall_12: 0.5077
Epoch 10/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2088 - acc: 0.9193 - precision_12: 0.8606 - auc_12: 0.9529 - recall_12: 0.7485 - val_loss: 0.1871 - val_acc: 0.9261 - val_precision_12: 0.8818 - val_auc_12: 0.9629 - val_recall_12: 0.7554
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1994 - acc: 0.9235 - precision_12: 0.8700 - auc_12: 0.9570 - recall_12: 0.7601 - val_loss: 0.2359 - val_acc: 0.9141 - val_precision_12: 0.7945 - val_auc_12: 0.9417 - val_recall_12: 0.8005
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1949 - acc: 0.9248 - precision_12: 0.8728 - auc_12: 0.9590 - recall_12: 0.7641 - val_loss: 0.1721 - val_acc: 0.9387 - val_precision_12: 0.8778 - val_auc_12: 0.9728 - val_recall_12: 0.8238
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1763 - acc: 0.9304 - precision_12: 0.8783 - auc_12: 0.9679 - recall_12: 0.7874 - val_loss: 0.1511 - val_acc: 0.9409 - val_precision_12: 0.8486 - val_auc_12: 0.9796 - val_recall_12: 0.8816
Epoch 14/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1715 - acc: 0.9322 - precision_12: 0.8804 - auc_12: 0.9691 - recall_12: 0.7947 - val_loss: 0.1574 - val_acc: 0.9393 - val_precision_12: 0.9146 - val_auc_12: 0.9699 - val_recall_12: 0.7787
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1691 - acc: 0.9334 - precision_12: 0.8870 - auc_12: 0.9704 - recall_12: 0.7934 - val_loss: 0.1565 - val_acc: 0.9376 - val_precision_12: 0.8598 - val_auc_12: 0.9768 - val_recall_12: 0.8601
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1649 - acc: 0.9342 - precision_12: 0.8809 - auc_12: 0.9725 - recall_12: 0.8050 - val_loss: 0.1333 - val_acc: 0.9480 - val_precision_12: 0.9220 - val_auc_12: 0.9787 - val_recall_12: 0.8054
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1615 - acc: 0.9358 - precision_12: 0.8876 - auc_12: 0.9732 - recall_12: 0.8056 - val_loss: 0.1434 - val_acc: 0.9416 - val_precision_12: 0.8853 - val_auc_12: 0.9791 - val_recall_12: 0.8436
Epoch 18/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1598 - acc: 0.9364 - precision_12: 0.8854 - auc_12: 0.9736 - recall_12: 0.8112 - val_loss: 0.1410 - val_acc: 0.9458 - val_precision_12: 0.9413 - val_auc_12: 0.9779 - val_recall_12: 0.7868
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1531 - acc: 0.9385 - precision_12: 0.8822 - auc_12: 0.9762 - recall_12: 0.8261 - val_loss: 0.1480 - val_acc: 0.9405 - val_precision_12: 0.9518 - val_auc_12: 0.9804 - val_recall_12: 0.7688
Epoch 20/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1515 - acc: 0.9394 - precision_12: 0.8877 - auc_12: 0.9770 - recall_12: 0.8242 - val_loss: 0.1361 - val_acc: 0.9485 - val_precision_12: 0.8713 - val_auc_12: 0.9767 - val_recall_12: 0.8576
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1565 - acc: 0.9371 - precision_12: 0.8773 - auc_12: 0.9753 - recall_12: 0.8248 - val_loss: 0.1373 - val_acc: 0.9460 - val_precision_12: 0.9091 - val_auc_12: 0.9823 - val_recall_12: 0.8533
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1444 - acc: 0.9416 - precision_12: 0.8861 - auc_12: 0.9789 - recall_12: 0.8378 - val_loss: 0.1375 - val_acc: 0.9436 - val_precision_12: 0.8658 - val_auc_12: 0.9793 - val_recall_12: 0.8551
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1412 - acc: 0.9429 - precision_12: 0.8858 - auc_12: 0.9795 - recall_12: 0.8454 - val_loss: 0.1412 - val_acc: 0.9413 - val_precision_12: 0.8373 - val_auc_12: 0.9813 - val_recall_12: 0.8965
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1385 - acc: 0.9437 - precision_12: 0.8878 - auc_12: 0.9810 - recall_12: 0.8470 - val_loss: 0.1415 - val_acc: 0.9438 - val_precision_12: 0.8610 - val_auc_12: 0.9788 - val_recall_12: 0.8822
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1391 - acc: 0.9437 - precision_12: 0.8842 - auc_12: 0.9806 - recall_12: 0.8514 - val_loss: 0.1252 - val_acc: 0.9488 - val_precision_12: 0.8618 - val_auc_12: 0.9852 - val_recall_12: 0.8972
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1319 - acc: 0.9461 - precision_12: 0.8889 - auc_12: 0.9826 - recall_12: 0.8582 - val_loss: 0.1569 - val_acc: 0.9431 - val_precision_12: 0.9158 - val_auc_12: 0.9730 - val_recall_12: 0.8039
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1294 - acc: 0.9473 - precision_12: 0.8939 - auc_12: 0.9831 - recall_12: 0.8587 - val_loss: 0.1480 - val_acc: 0.9448 - val_precision_12: 0.9047 - val_auc_12: 0.9798 - val_recall_12: 0.8279
Epoch 28/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1243 - acc: 0.9489 - precision_12: 0.8952 - auc_12: 0.9848 - recall_12: 0.8651 - val_loss: 0.1634 - val_acc: 0.9316 - val_precision_12: 0.7926 - val_auc_12: 0.9797 - val_recall_12: 0.9189
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1279 - acc: 0.9481 - precision_12: 0.8937 - auc_12: 0.9834 - recall_12: 0.8627 - val_loss: 0.1554 - val_acc: 0.9352 - val_precision_12: 0.8389 - val_auc_12: 0.9771 - val_recall_12: 0.8825
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1156 - acc: 0.9528 - precision_12: 0.9005 - auc_12: 0.9865 - recall_12: 0.8793 - val_loss: 0.1301 - val_acc: 0.9491 - val_precision_12: 0.9112 - val_auc_12: 0.9812 - val_recall_12: 0.8414
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1131 - acc: 0.9538 - precision_12: 0.9047 - auc_12: 0.9872 - recall_12: 0.8790 - val_loss: 0.1465 - val_acc: 0.9412 - val_precision_12: 0.8396 - val_auc_12: 0.9774 - val_recall_12: 0.8896
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1065 - acc: 0.9563 - precision_12: 0.9054 - auc_12: 0.9888 - recall_12: 0.8913 - val_loss: 0.1485 - val_acc: 0.9464 - val_precision_12: 0.8977 - val_auc_12: 0.9752 - val_recall_12: 0.8458
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1009 - acc: 0.9583 - precision_12: 0.9107 - auc_12: 0.9900 - recall_12: 0.8952 - val_loss: 0.1372 - val_acc: 0.9486 - val_precision_12: 0.9146 - val_auc_12: 0.9784 - val_recall_12: 0.8342
Epoch 34/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0937 - acc: 0.9614 - precision_12: 0.9175 - auc_12: 0.9913 - recall_12: 0.9028 - val_loss: 0.1480 - val_acc: 0.9448 - val_precision_12: 0.8616 - val_auc_12: 0.9739 - val_recall_12: 0.8560
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0971 - acc: 0.9611 - precision_12: 0.9186 - auc_12: 0.9907 - recall_12: 0.9002 - val_loss: 0.1481 - val_acc: 0.9431 - val_precision_12: 0.9102 - val_auc_12: 0.9758 - val_recall_12: 0.8288
Epoch 36/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0977 - acc: 0.9606 - precision_12: 0.9169 - auc_12: 0.9905 - recall_12: 0.8998 - val_loss: 0.1721 - val_acc: 0.9422 - val_precision_12: 0.9269 - val_auc_12: 0.9685 - val_recall_12: 0.7908
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0804 - acc: 0.9669 - precision_12: 0.9287 - auc_12: 0.9936 - recall_12: 0.9178 - val_loss: 0.1601 - val_acc: 0.9432 - val_precision_12: 0.9076 - val_auc_12: 0.9719 - val_recall_12: 0.8201
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0730 - acc: 0.9697 - precision_12: 0.9335 - auc_12: 0.9947 - recall_12: 0.9258 - val_loss: 0.1596 - val_acc: 0.9462 - val_precision_12: 0.8901 - val_auc_12: 0.9726 - val_recall_12: 0.8479
Epoch 39/100
254/254 [==============================] - ETA: 0s - loss: 0.0689 - acc: 0.9715 - precision_12: 0.9386 - auc_12: 0.9952 - recall_12: 0.9294Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0689 - acc: 0.9715 - precision_12: 0.9386 - auc_12: 0.9952 - recall_12: 0.9294 - val_loss: 0.1836 - val_acc: 0.9445 - val_precision_12: 0.8904 - val_auc_12: 0.9687 - val_recall_12: 0.8516
Epoch 00039: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1496 - acc: 0.9422 - precision_12: 0.8609 - auc_12: 0.9798 - recall_12: 0.8843
---------------TEST METRICS----------------------
jaccard_index 0.7499179103312984
test_sensitivity 0.8921095253547987
test_specifitivity 0.9531398543105426
test_accuracy 0.9394933822307181
test_precision 0.845746302606431
test_jaccard_score 0.7499179103312984
test_dicecoef 0.8683094655486364
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-212151.h5
[0. 0. 0. 0. 0.] [0.93949338 0.74991791 0.8457463  0.89210953 0.95313985]

-------------------------
Rep: 1
-------------------------

2021-09-24 21:21:52.279271: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 21:21:52.279400: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: -3.4061795605182844e-18,
Validation samples: 383, channel mean: 0.005347822958799939
Model built.
Model: "functional_27"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_14 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_247 (Conv2D)             (None, 256, 256, 32) 320         input_14[0][0]
__________________________________________________________________________________________________
conv2d_248 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_247[0][0]
__________________________________________________________________________________________________
max_pooling2d_52 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_248[0][0]
__________________________________________________________________________________________________
conv2d_249 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_52[0][0]
__________________________________________________________________________________________________
conv2d_250 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_249[0][0]
__________________________________________________________________________________________________
max_pooling2d_53 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_250[0][0]
__________________________________________________________________________________________________
conv2d_251 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_53[0][0]
__________________________________________________________________________________________________
conv2d_252 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_251[0][0]
__________________________________________________________________________________________________
max_pooling2d_54 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_252[0][0]
__________________________________________________________________________________________________
conv2d_253 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_54[0][0]
__________________________________________________________________________________________________
conv2d_254 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_253[0][0]
__________________________________________________________________________________________________
max_pooling2d_55 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_254[0][0]
__________________________________________________________________________________________________
conv2d_255 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_55[0][0]
__________________________________________________________________________________________________
conv2d_256 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_255[0][0]
__________________________________________________________________________________________________
up_sampling2d_52 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_256[0][0]
__________________________________________________________________________________________________
concatenate_52 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_52[0][0]
                                                                 conv2d_254[0][0]
__________________________________________________________________________________________________
conv2d_257 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_52[0][0]
__________________________________________________________________________________________________
conv2d_258 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_257[0][0]
__________________________________________________________________________________________________
up_sampling2d_53 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_258[0][0]
__________________________________________________________________________________________________
concatenate_53 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_53[0][0]
                                                                 conv2d_252[0][0]
__________________________________________________________________________________________________
conv2d_259 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_53[0][0]
__________________________________________________________________________________________________
conv2d_260 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_259[0][0]
__________________________________________________________________________________________________
up_sampling2d_54 (UpSampling2D) (None, 128, 128, 128 0           conv2d_260[0][0]
__________________________________________________________________________________________________
concatenate_54 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_54[0][0]
                                                                 conv2d_250[0][0]
__________________________________________________________________________________________________
conv2d_261 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_54[0][0]
__________________________________________________________________________________________________
conv2d_262 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_261[0][0]
__________________________________________________________________________________________________
up_sampling2d_55 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_262[0][0]
__________________________________________________________________________________________________
concatenate_55 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_55[0][0]
                                                                 conv2d_248[0][0]
__________________________________________________________________________________________________
conv2d_263 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_55[0][0]
__________________________________________________________________________________________________
conv2d_264 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_263[0][0]
__________________________________________________________________________________________________
conv2d_265 (Conv2D)             (None, 256, 256, 1)  33          conv2d_264[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6945 - acc: 0.1661 - precision_13: 0.1644 - auc_13: 0.5623 - recall_13: 0.99802021-09-24 21:22:31.833691: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 21:22:31.834369: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 21:22:32.378448: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 21:22:32.388928: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32
2021-09-24 21:22:32.392120: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.trace.json.gz
2021-09-24 21:22:32.419802: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32
2021-09-24 21:22:32.428132: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.memory_profile.json.gz
2021-09-24 21:22:32.444048: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32Dumped tool data for xplane.pb to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-212152/train/plugins/profile/2021_09_24_21_22_32/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:17 - loss: 0.6936 - acc: 0.4021 - precision_13: 0.1645 - auc_13: 0.4029 - recall_13: 0.3139WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1940s vs `on_train_batch_end` time: 0.4174s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4449 - acc: 0.8227 - precision_13: 0.6904 - auc_13: 0.7662 - recall_13: 0.3284 - val_loss: 0.4062 - val_acc: 0.8528 - val_precision_13: 0.7838 - val_auc_13: 0.8242 - val_recall_13: 0.4238
Epoch 2/100
254/254 [==============================] - 55s 215ms/step - loss: 0.4042 - acc: 0.8495 - precision_13: 0.7603 - auc_13: 0.8054 - recall_13: 0.4450 - val_loss: 0.3538 - val_acc: 0.8650 - val_precision_13: 0.8392 - val_auc_13: 0.8516 - val_recall_13: 0.4490
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3529 - acc: 0.8674 - precision_13: 0.8296 - auc_13: 0.8503 - recall_13: 0.4877 - val_loss: 0.3160 - val_acc: 0.8998 - val_precision_13: 0.8815 - val_auc_13: 0.8914 - val_recall_13: 0.6143
Epoch 4/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3052 - acc: 0.8909 - precision_13: 0.8726 - auc_13: 0.8892 - recall_13: 0.5807 - val_loss: 0.2289 - val_acc: 0.9178 - val_precision_13: 0.8893 - val_auc_13: 0.9324 - val_recall_13: 0.6976
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2509 - acc: 0.9075 - precision_13: 0.8589 - auc_13: 0.9266 - recall_13: 0.6852 - val_loss: 0.2055 - val_acc: 0.9251 - val_precision_13: 0.9039 - val_auc_13: 0.9490 - val_recall_13: 0.7119
Epoch 6/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2302 - acc: 0.9132 - precision_13: 0.8602 - auc_13: 0.9406 - recall_13: 0.7153 - val_loss: 0.2015 - val_acc: 0.9236 - val_precision_13: 0.8742 - val_auc_13: 0.9553 - val_recall_13: 0.7564
Epoch 7/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2233 - acc: 0.9162 - precision_13: 0.8600 - auc_13: 0.9445 - recall_13: 0.7319 - val_loss: 0.2025 - val_acc: 0.9219 - val_precision_13: 0.9352 - val_auc_13: 0.9584 - val_recall_13: 0.6739
Epoch 8/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2129 - acc: 0.9195 - precision_13: 0.8663 - auc_13: 0.9486 - recall_13: 0.7428 - val_loss: 0.1944 - val_acc: 0.9260 - val_precision_13: 0.8955 - val_auc_13: 0.9573 - val_recall_13: 0.7499
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2045 - acc: 0.9230 - precision_13: 0.8679 - auc_13: 0.9531 - recall_13: 0.7599 - val_loss: 0.2224 - val_acc: 0.9080 - val_precision_13: 0.9806 - val_auc_13: 0.9626 - val_recall_13: 0.5716
Epoch 10/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2001 - acc: 0.9232 - precision_13: 0.8681 - auc_13: 0.9564 - recall_13: 0.7608 - val_loss: 0.1792 - val_acc: 0.9315 - val_precision_13: 0.8941 - val_auc_13: 0.9620 - val_recall_13: 0.7708
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2006 - acc: 0.9231 - precision_13: 0.8672 - auc_13: 0.9562 - recall_13: 0.7612 - val_loss: 0.1755 - val_acc: 0.9322 - val_precision_13: 0.9151 - val_auc_13: 0.9690 - val_recall_13: 0.7488
Epoch 12/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1960 - acc: 0.9250 - precision_13: 0.8711 - auc_13: 0.9586 - recall_13: 0.7669 - val_loss: 0.1702 - val_acc: 0.9381 - val_precision_13: 0.9243 - val_auc_13: 0.9717 - val_recall_13: 0.7695
Epoch 13/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1836 - acc: 0.9294 - precision_13: 0.8753 - auc_13: 0.9634 - recall_13: 0.7859 - val_loss: 0.1543 - val_acc: 0.9403 - val_precision_13: 0.8654 - val_auc_13: 0.9761 - val_recall_13: 0.8545
Epoch 14/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1864 - acc: 0.9292 - precision_13: 0.8765 - auc_13: 0.9617 - recall_13: 0.7832 - val_loss: 0.1779 - val_acc: 0.9293 - val_precision_13: 0.9229 - val_auc_13: 0.9647 - val_recall_13: 0.7171
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1834 - acc: 0.9304 - precision_13: 0.8796 - auc_13: 0.9636 - recall_13: 0.7861 - val_loss: 0.1607 - val_acc: 0.9396 - val_precision_13: 0.8862 - val_auc_13: 0.9748 - val_recall_13: 0.8359
Epoch 16/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1747 - acc: 0.9316 - precision_13: 0.8773 - auc_13: 0.9675 - recall_13: 0.7952 - val_loss: 0.1515 - val_acc: 0.9443 - val_precision_13: 0.9154 - val_auc_13: 0.9695 - val_recall_13: 0.7919
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1762 - acc: 0.9327 - precision_13: 0.8847 - auc_13: 0.9665 - recall_13: 0.7924 - val_loss: 0.1620 - val_acc: 0.9362 - val_precision_13: 0.8589 - val_auc_13: 0.9750 - val_recall_13: 0.8493
Epoch 18/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1736 - acc: 0.9331 - precision_13: 0.8851 - auc_13: 0.9678 - recall_13: 0.7938 - val_loss: 0.1507 - val_acc: 0.9421 - val_precision_13: 0.9307 - val_auc_13: 0.9741 - val_recall_13: 0.7778
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1693 - acc: 0.9343 - precision_13: 0.8845 - auc_13: 0.9697 - recall_13: 0.8013 - val_loss: 0.1539 - val_acc: 0.9398 - val_precision_13: 0.9207 - val_auc_13: 0.9753 - val_recall_13: 0.7949
Epoch 20/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1694 - acc: 0.9346 - precision_13: 0.8887 - auc_13: 0.9698 - recall_13: 0.7976 - val_loss: 0.1385 - val_acc: 0.9463 - val_precision_13: 0.8693 - val_auc_13: 0.9761 - val_recall_13: 0.8470
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1685 - acc: 0.9349 - precision_13: 0.8832 - auc_13: 0.9703 - recall_13: 0.8061 - val_loss: 0.1499 - val_acc: 0.9428 - val_precision_13: 0.8916 - val_auc_13: 0.9795 - val_recall_13: 0.8583
Epoch 22/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1614 - acc: 0.9374 - precision_13: 0.8910 - auc_13: 0.9722 - recall_13: 0.8100 - val_loss: 0.1574 - val_acc: 0.9365 - val_precision_13: 0.8350 - val_auc_13: 0.9747 - val_recall_13: 0.8570
Epoch 23/100
254/254 [==============================] - 54s 214ms/step - loss: 0.1595 - acc: 0.9379 - precision_13: 0.8890 - auc_13: 0.9730 - recall_13: 0.8148 - val_loss: 0.1476 - val_acc: 0.9408 - val_precision_13: 0.8566 - val_auc_13: 0.9782 - val_recall_13: 0.8648
Epoch 24/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1578 - acc: 0.9384 - precision_13: 0.8921 - auc_13: 0.9742 - recall_13: 0.8140 - val_loss: 0.1520 - val_acc: 0.9391 - val_precision_13: 0.8518 - val_auc_13: 0.9756 - val_recall_13: 0.8695
Epoch 25/100
254/254 [==============================] - 54s 215ms/step - loss: 0.1551 - acc: 0.9388 - precision_13: 0.8872 - auc_13: 0.9750 - recall_13: 0.8219 - val_loss: 0.1434 - val_acc: 0.9487 - val_precision_13: 0.8826 - val_auc_13: 0.9816 - val_recall_13: 0.8683
Epoch 26/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1492 - acc: 0.9418 - precision_13: 0.8949 - auc_13: 0.9761 - recall_13: 0.8285 - val_loss: 0.1463 - val_acc: 0.9428 - val_precision_13: 0.9133 - val_auc_13: 0.9767 - val_recall_13: 0.8050
Epoch 27/100
254/254 [==============================] - 54s 214ms/step - loss: 0.1497 - acc: 0.9421 - precision_13: 0.8991 - auc_13: 0.9761 - recall_13: 0.8252 - val_loss: 0.1447 - val_acc: 0.9439 - val_precision_13: 0.8928 - val_auc_13: 0.9774 - val_recall_13: 0.8370
Epoch 28/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1427 - acc: 0.9433 - precision_13: 0.8979 - auc_13: 0.9788 - recall_13: 0.8326 - val_loss: 0.1788 - val_acc: 0.9232 - val_precision_13: 0.7702 - val_auc_13: 0.9763 - val_recall_13: 0.9104
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1517 - acc: 0.9409 - precision_13: 0.8927 - auc_13: 0.9761 - recall_13: 0.8263 - val_loss: 0.1885 - val_acc: 0.9159 - val_precision_13: 0.7583 - val_auc_13: 0.9758 - val_recall_13: 0.9211
Epoch 30/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1435 - acc: 0.9434 - precision_13: 0.8950 - auc_13: 0.9783 - recall_13: 0.8369 - val_loss: 0.1324 - val_acc: 0.9481 - val_precision_13: 0.9142 - val_auc_13: 0.9808 - val_recall_13: 0.8329
Epoch 31/100
254/254 [==============================] - 54s 214ms/step - loss: 0.1384 - acc: 0.9451 - precision_13: 0.9001 - auc_13: 0.9802 - recall_13: 0.8395 - val_loss: 0.1400 - val_acc: 0.9469 - val_precision_13: 0.8790 - val_auc_13: 0.9777 - val_recall_13: 0.8658
Epoch 32/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1338 - acc: 0.9466 - precision_13: 0.9007 - auc_13: 0.9811 - recall_13: 0.8465 - val_loss: 0.1417 - val_acc: 0.9455 - val_precision_13: 0.8944 - val_auc_13: 0.9778 - val_recall_13: 0.8446
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1272 - acc: 0.9488 - precision_13: 0.9029 - auc_13: 0.9831 - recall_13: 0.8552 - val_loss: 0.1263 - val_acc: 0.9497 - val_precision_13: 0.9039 - val_auc_13: 0.9820 - val_recall_13: 0.8524
Epoch 34/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1238 - acc: 0.9505 - precision_13: 0.9069 - auc_13: 0.9842 - recall_13: 0.8595 - val_loss: 0.1407 - val_acc: 0.9450 - val_precision_13: 0.8575 - val_auc_13: 0.9764 - val_recall_13: 0.8628
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1183 - acc: 0.9523 - precision_13: 0.9080 - auc_13: 0.9856 - recall_13: 0.8674 - val_loss: 0.1411 - val_acc: 0.9449 - val_precision_13: 0.8808 - val_auc_13: 0.9789 - val_recall_13: 0.8733
Epoch 36/100
254/254 [==============================] - 54s 215ms/step - loss: 0.1259 - acc: 0.9507 - precision_13: 0.9064 - auc_13: 0.9837 - recall_13: 0.8611 - val_loss: 0.1610 - val_acc: 0.9441 - val_precision_13: 0.9215 - val_auc_13: 0.9735 - val_recall_13: 0.8058
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1220 - acc: 0.9513 - precision_13: 0.9040 - auc_13: 0.9845 - recall_13: 0.8669 - val_loss: 0.1549 - val_acc: 0.9428 - val_precision_13: 0.9463 - val_auc_13: 0.9741 - val_recall_13: 0.7792
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1085 - acc: 0.9561 - precision_13: 0.9139 - auc_13: 0.9878 - recall_13: 0.8799 - val_loss: 0.1317 - val_acc: 0.9477 - val_precision_13: 0.8623 - val_auc_13: 0.9812 - val_recall_13: 0.8932
Epoch 39/100
254/254 [==============================] - 54s 215ms/step - loss: 0.1062 - acc: 0.9569 - precision_13: 0.9152 - auc_13: 0.9883 - recall_13: 0.8825 - val_loss: 0.1514 - val_acc: 0.9448 - val_precision_13: 0.9154 - val_auc_13: 0.9736 - val_recall_13: 0.8245
Epoch 40/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1018 - acc: 0.9588 - precision_13: 0.9181 - auc_13: 0.9893 - recall_13: 0.8889 - val_loss: 0.1421 - val_acc: 0.9500 - val_precision_13: 0.9184 - val_auc_13: 0.9742 - val_recall_13: 0.8216
Epoch 41/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0976 - acc: 0.9605 - precision_13: 0.9205 - auc_13: 0.9900 - recall_13: 0.8950 - val_loss: 0.1534 - val_acc: 0.9418 - val_precision_13: 0.8774 - val_auc_13: 0.9751 - val_recall_13: 0.8540
Epoch 42/100
254/254 [==============================] - 54s 214ms/step - loss: 0.0964 - acc: 0.9608 - precision_13: 0.9215 - auc_13: 0.9904 - recall_13: 0.8953 - val_loss: 0.1430 - val_acc: 0.9471 - val_precision_13: 0.9088 - val_auc_13: 0.9753 - val_recall_13: 0.8320
Epoch 43/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0935 - acc: 0.9624 - precision_13: 0.9241 - auc_13: 0.9910 - recall_13: 0.9000 - val_loss: 0.1471 - val_acc: 0.9440 - val_precision_13: 0.8748 - val_auc_13: 0.9774 - val_recall_13: 0.8663
Epoch 44/100
254/254 [==============================] - 54s 214ms/step - loss: 0.0885 - acc: 0.9641 - precision_13: 0.9290 - auc_13: 0.9919 - recall_13: 0.9033 - val_loss: 0.1813 - val_acc: 0.9438 - val_precision_13: 0.9560 - val_auc_13: 0.9668 - val_recall_13: 0.7704
Epoch 45/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0821 - acc: 0.9666 - precision_13: 0.9309 - auc_13: 0.9930 - recall_13: 0.9132 - val_loss: 0.1814 - val_acc: 0.9427 - val_precision_13: 0.9098 - val_auc_13: 0.9670 - val_recall_13: 0.8133
Epoch 46/100
254/254 [==============================] - 54s 214ms/step - loss: 0.0785 - acc: 0.9681 - precision_13: 0.9359 - auc_13: 0.9936 - recall_13: 0.9155 - val_loss: 0.1823 - val_acc: 0.9461 - val_precision_13: 0.8988 - val_auc_13: 0.9663 - val_recall_13: 0.8383
Epoch 47/100
254/254 [==============================] - ETA: 0s - loss: 0.0747 - acc: 0.9695 - precision_13: 0.9365 - auc_13: 0.9942 - recall_13: 0.9215Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 215ms/step - loss: 0.0747 - acc: 0.9695 - precision_13: 0.9365 - auc_13: 0.9942 - recall_13: 0.9215 - val_loss: 0.1684 - val_acc: 0.9442 - val_precision_13: 0.8650 - val_auc_13: 0.9734 - val_recall_13: 0.8766
Epoch 00047: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1441 - acc: 0.9449 - precision_13: 0.8959 - auc_13: 0.9785 - recall_13: 0.8527
---------------TEST METRICS----------------------
jaccard_index 0.7752782448049057
test_sensitivity 0.8594830671571307
test_specifitivity 0.9681787368214029
test_accuracy 0.9438742238579066
test_precision 0.886088584434331
test_jaccard_score 0.7752782448049057
test_dicecoef 0.8725830688510526
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-220557.h5
[0.93949338 0.74991791 0.8457463  0.89210953 0.95313985] [0.94387422 0.77527824 0.88608858 0.85948307 0.96817874]

-------------------------
Rep: 2
-------------------------

2021-09-24 22:05:58.566199: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 22:05:58.566312: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Normalized per channel
Applied normalization
Data augumentation off
Prep done
Training samples: 2029, channel mean: -3.4061795605182844e-18,
Validation samples: 383, channel mean: 0.005347822958799939
Model built.
Model: "functional_29"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_15 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_266 (Conv2D)             (None, 256, 256, 32) 320         input_15[0][0]
__________________________________________________________________________________________________
conv2d_267 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_266[0][0]
__________________________________________________________________________________________________
max_pooling2d_56 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_267[0][0]
__________________________________________________________________________________________________
conv2d_268 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_56[0][0]
__________________________________________________________________________________________________
conv2d_269 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_268[0][0]
__________________________________________________________________________________________________
max_pooling2d_57 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_269[0][0]
__________________________________________________________________________________________________
conv2d_270 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_57[0][0]
__________________________________________________________________________________________________
conv2d_271 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_270[0][0]
__________________________________________________________________________________________________
max_pooling2d_58 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_271[0][0]
__________________________________________________________________________________________________
conv2d_272 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_58[0][0]
__________________________________________________________________________________________________
conv2d_273 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_272[0][0]
__________________________________________________________________________________________________
max_pooling2d_59 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_273[0][0]
__________________________________________________________________________________________________
conv2d_274 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_59[0][0]
__________________________________________________________________________________________________
conv2d_275 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_274[0][0]
__________________________________________________________________________________________________
up_sampling2d_56 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_275[0][0]
__________________________________________________________________________________________________
concatenate_56 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_56[0][0]
                                                                 conv2d_273[0][0]
__________________________________________________________________________________________________
conv2d_276 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_56[0][0]
__________________________________________________________________________________________________
conv2d_277 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_276[0][0]
__________________________________________________________________________________________________
up_sampling2d_57 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_277[0][0]
__________________________________________________________________________________________________
concatenate_57 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_57[0][0]
                                                                 conv2d_271[0][0]
__________________________________________________________________________________________________
conv2d_278 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_57[0][0]
__________________________________________________________________________________________________
conv2d_279 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_278[0][0]
__________________________________________________________________________________________________
up_sampling2d_58 (UpSampling2D) (None, 128, 128, 128 0           conv2d_279[0][0]
__________________________________________________________________________________________________
concatenate_58 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_58[0][0]
                                                                 conv2d_269[0][0]
__________________________________________________________________________________________________
conv2d_280 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_58[0][0]
__________________________________________________________________________________________________
conv2d_281 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_280[0][0]
__________________________________________________________________________________________________
up_sampling2d_59 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_281[0][0]
__________________________________________________________________________________________________
concatenate_59 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_59[0][0]
                                                                 conv2d_267[0][0]
__________________________________________________________________________________________________
conv2d_282 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_59[0][0]
__________________________________________________________________________________________________
conv2d_283 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_282[0][0]
__________________________________________________________________________________________________
conv2d_284 (Conv2D)             (None, 256, 256, 1)  33          conv2d_283[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.6112 - precision_14: 0.2789 - auc_14: 0.4999 - recall_14: 0.86142021-09-24 22:06:37.992089: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 22:06:37.992723: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 22:06:38.515307: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 22:06:38.525706: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38
2021-09-24 22:06:38.528989: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.trace.json.gz
2021-09-24 22:06:38.556939: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38
2021-09-24 22:06:38.565718: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.memory_profile.json.gz
2021-09-24 22:06:38.580173: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38Dumped tool data for xplane.pb to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-220558/train/plugins/profile/2021_09_24_22_06_38/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6929 - acc: 0.6256 - precision_14: 0.2789 - auc_14: 0.2878 - recall_14: 0.2701WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1998s vs `on_train_batch_end` time: 0.3894s). Check your callbacks.
254/254 [==============================] - 56s 219ms/step - loss: 0.4397 - acc: 0.8299 - precision_14: 0.7060 - auc_14: 0.7815 - recall_14: 0.3672 - val_loss: 0.3963 - val_acc: 0.8636 - val_precision_14: 0.8363 - val_auc_14: 0.8227 - val_recall_14: 0.4447
Epoch 2/100
254/254 [==============================] - 55s 216ms/step - loss: 0.3617 - acc: 0.8595 - precision_14: 0.7524 - auc_14: 0.8547 - recall_14: 0.5232 - val_loss: 0.2785 - val_acc: 0.9042 - val_precision_14: 0.8481 - val_auc_14: 0.8998 - val_recall_14: 0.6676
Epoch 3/100
254/254 [==============================] - 55s 215ms/step - loss: 0.3032 - acc: 0.8906 - precision_14: 0.8285 - auc_14: 0.8853 - recall_14: 0.6238 - val_loss: 0.2594 - val_acc: 0.9079 - val_precision_14: 0.8299 - val_auc_14: 0.9165 - val_recall_14: 0.7165
Epoch 4/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2784 - acc: 0.9002 - precision_14: 0.8370 - auc_14: 0.9042 - recall_14: 0.6693 - val_loss: 0.2252 - val_acc: 0.9182 - val_precision_14: 0.8832 - val_auc_14: 0.9383 - val_recall_14: 0.7060
Epoch 5/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2490 - acc: 0.9076 - precision_14: 0.8440 - auc_14: 0.9280 - recall_14: 0.7032 - val_loss: 0.2019 - val_acc: 0.9247 - val_precision_14: 0.8453 - val_auc_14: 0.9508 - val_recall_14: 0.7762
Epoch 6/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2306 - acc: 0.9121 - precision_14: 0.8513 - auc_14: 0.9405 - recall_14: 0.7198 - val_loss: 0.2038 - val_acc: 0.9229 - val_precision_14: 0.8580 - val_auc_14: 0.9546 - val_recall_14: 0.7722
Epoch 7/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2222 - acc: 0.9155 - precision_14: 0.8542 - auc_14: 0.9453 - recall_14: 0.7354 - val_loss: 0.2297 - val_acc: 0.9162 - val_precision_14: 0.9500 - val_auc_14: 0.9415 - val_recall_14: 0.6331
Epoch 8/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2127 - acc: 0.9191 - precision_14: 0.8593 - auc_14: 0.9491 - recall_14: 0.7489 - val_loss: 0.1938 - val_acc: 0.9252 - val_precision_14: 0.8652 - val_auc_14: 0.9578 - val_recall_14: 0.7804
Epoch 9/100
254/254 [==============================] - 55s 215ms/step - loss: 0.2090 - acc: 0.9218 - precision_14: 0.8629 - auc_14: 0.9513 - recall_14: 0.7596 - val_loss: 0.2314 - val_acc: 0.9098 - val_precision_14: 0.9826 - val_auc_14: 0.9604 - val_recall_14: 0.5791
Epoch 10/100
254/254 [==============================] - 55s 216ms/step - loss: 0.2068 - acc: 0.9215 - precision_14: 0.8588 - auc_14: 0.9531 - recall_14: 0.7628 - val_loss: 0.1953 - val_acc: 0.9275 - val_precision_14: 0.9467 - val_auc_14: 0.9587 - val_recall_14: 0.6999
Epoch 11/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1981 - acc: 0.9238 - precision_14: 0.8646 - auc_14: 0.9574 - recall_14: 0.7684 - val_loss: 0.1834 - val_acc: 0.9305 - val_precision_14: 0.9254 - val_auc_14: 0.9656 - val_recall_14: 0.7300
Epoch 12/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1898 - acc: 0.9269 - precision_14: 0.8698 - auc_14: 0.9608 - recall_14: 0.7788 - val_loss: 0.1650 - val_acc: 0.9378 - val_precision_14: 0.8677 - val_auc_14: 0.9712 - val_recall_14: 0.8320
Epoch 13/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1865 - acc: 0.9290 - precision_14: 0.8707 - auc_14: 0.9632 - recall_14: 0.7892 - val_loss: 0.1525 - val_acc: 0.9398 - val_precision_14: 0.8603 - val_auc_14: 0.9766 - val_recall_14: 0.8589
Epoch 14/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1771 - acc: 0.9310 - precision_14: 0.8716 - auc_14: 0.9666 - recall_14: 0.7987 - val_loss: 0.1702 - val_acc: 0.9360 - val_precision_14: 0.9115 - val_auc_14: 0.9651 - val_recall_14: 0.7640
Epoch 15/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1750 - acc: 0.9320 - precision_14: 0.8777 - auc_14: 0.9676 - recall_14: 0.7968 - val_loss: 0.1714 - val_acc: 0.9314 - val_precision_14: 0.8238 - val_auc_14: 0.9744 - val_recall_14: 0.8799
Epoch 16/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1667 - acc: 0.9342 - precision_14: 0.8774 - auc_14: 0.9711 - recall_14: 0.8091 - val_loss: 0.1422 - val_acc: 0.9460 - val_precision_14: 0.9258 - val_auc_14: 0.9753 - val_recall_14: 0.7907
Epoch 17/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1688 - acc: 0.9345 - precision_14: 0.8831 - auc_14: 0.9700 - recall_14: 0.8036 - val_loss: 0.1504 - val_acc: 0.9399 - val_precision_14: 0.8775 - val_auc_14: 0.9767 - val_recall_14: 0.8443
Epoch 18/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1706 - acc: 0.9340 - precision_14: 0.8838 - auc_14: 0.9694 - recall_14: 0.8004 - val_loss: 0.1416 - val_acc: 0.9456 - val_precision_14: 0.9223 - val_auc_14: 0.9760 - val_recall_14: 0.8047
Epoch 19/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1632 - acc: 0.9358 - precision_14: 0.8802 - auc_14: 0.9726 - recall_14: 0.8142 - val_loss: 0.1553 - val_acc: 0.9388 - val_precision_14: 0.9510 - val_auc_14: 0.9771 - val_recall_14: 0.7615
Epoch 20/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1607 - acc: 0.9371 - precision_14: 0.8885 - auc_14: 0.9735 - recall_14: 0.8110 - val_loss: 0.1394 - val_acc: 0.9463 - val_precision_14: 0.8702 - val_auc_14: 0.9738 - val_recall_14: 0.8457
Epoch 21/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1626 - acc: 0.9357 - precision_14: 0.8801 - auc_14: 0.9724 - recall_14: 0.8140 - val_loss: 0.1403 - val_acc: 0.9444 - val_precision_14: 0.9019 - val_auc_14: 0.9815 - val_recall_14: 0.8535
Epoch 22/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1529 - acc: 0.9391 - precision_14: 0.8887 - auc_14: 0.9759 - recall_14: 0.8217 - val_loss: 0.1453 - val_acc: 0.9402 - val_precision_14: 0.8458 - val_auc_14: 0.9781 - val_recall_14: 0.8631
Epoch 23/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1489 - acc: 0.9406 - precision_14: 0.8903 - auc_14: 0.9768 - recall_14: 0.8278 - val_loss: 0.1499 - val_acc: 0.9387 - val_precision_14: 0.8324 - val_auc_14: 0.9768 - val_recall_14: 0.8888
Epoch 24/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1495 - acc: 0.9404 - precision_14: 0.8943 - auc_14: 0.9770 - recall_14: 0.8219 - val_loss: 0.1465 - val_acc: 0.9422 - val_precision_14: 0.8537 - val_auc_14: 0.9793 - val_recall_14: 0.8838
Epoch 25/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1490 - acc: 0.9410 - precision_14: 0.8897 - auc_14: 0.9772 - recall_14: 0.8305 - val_loss: 0.1465 - val_acc: 0.9436 - val_precision_14: 0.8411 - val_auc_14: 0.9829 - val_recall_14: 0.8981
Epoch 26/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1401 - acc: 0.9434 - precision_14: 0.8964 - auc_14: 0.9798 - recall_14: 0.8349 - val_loss: 0.1450 - val_acc: 0.9436 - val_precision_14: 0.9189 - val_auc_14: 0.9764 - val_recall_14: 0.8029
Epoch 27/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1451 - acc: 0.9432 - precision_14: 0.8992 - auc_14: 0.9777 - recall_14: 0.8310 - val_loss: 0.1421 - val_acc: 0.9449 - val_precision_14: 0.8982 - val_auc_14: 0.9797 - val_recall_14: 0.8357
Epoch 28/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1350 - acc: 0.9456 - precision_14: 0.9004 - auc_14: 0.9815 - recall_14: 0.8416 - val_loss: 0.1682 - val_acc: 0.9291 - val_precision_14: 0.7842 - val_auc_14: 0.9799 - val_recall_14: 0.9197
Epoch 29/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1376 - acc: 0.9450 - precision_14: 0.8989 - auc_14: 0.9805 - recall_14: 0.8407 - val_loss: 0.1683 - val_acc: 0.9280 - val_precision_14: 0.7983 - val_auc_14: 0.9774 - val_recall_14: 0.9109
Epoch 30/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1293 - acc: 0.9480 - precision_14: 0.9019 - auc_14: 0.9828 - recall_14: 0.8526 - val_loss: 0.1244 - val_acc: 0.9506 - val_precision_14: 0.9120 - val_auc_14: 0.9827 - val_recall_14: 0.8484
Epoch 31/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1276 - acc: 0.9485 - precision_14: 0.9025 - auc_14: 0.9834 - recall_14: 0.8543 - val_loss: 0.1345 - val_acc: 0.9470 - val_precision_14: 0.8690 - val_auc_14: 0.9806 - val_recall_14: 0.8797
Epoch 32/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1249 - acc: 0.9501 - precision_14: 0.9018 - auc_14: 0.9839 - recall_14: 0.8635 - val_loss: 0.1378 - val_acc: 0.9472 - val_precision_14: 0.8958 - val_auc_14: 0.9792 - val_recall_14: 0.8520
Epoch 33/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1193 - acc: 0.9513 - precision_14: 0.9036 - auc_14: 0.9856 - recall_14: 0.8676 - val_loss: 0.1413 - val_acc: 0.9455 - val_precision_14: 0.9086 - val_auc_14: 0.9777 - val_recall_14: 0.8248
Epoch 34/100
254/254 [==============================] - 55s 216ms/step - loss: 0.1171 - acc: 0.9525 - precision_14: 0.9068 - auc_14: 0.9861 - recall_14: 0.8698 - val_loss: 0.1358 - val_acc: 0.9463 - val_precision_14: 0.8688 - val_auc_14: 0.9779 - val_recall_14: 0.8549
Epoch 35/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1076 - acc: 0.9562 - precision_14: 0.9111 - auc_14: 0.9885 - recall_14: 0.8837 - val_loss: 0.1446 - val_acc: 0.9448 - val_precision_14: 0.8996 - val_auc_14: 0.9774 - val_recall_14: 0.8494
Epoch 36/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1074 - acc: 0.9564 - precision_14: 0.9128 - auc_14: 0.9882 - recall_14: 0.8827 - val_loss: 0.1560 - val_acc: 0.9440 - val_precision_14: 0.9134 - val_auc_14: 0.9729 - val_recall_14: 0.8141
Epoch 37/100
254/254 [==============================] - 55s 215ms/step - loss: 0.1009 - acc: 0.9588 - precision_14: 0.9158 - auc_14: 0.9899 - recall_14: 0.8914 - val_loss: 0.1474 - val_acc: 0.9463 - val_precision_14: 0.9306 - val_auc_14: 0.9764 - val_recall_14: 0.8119
Epoch 38/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0955 - acc: 0.9611 - precision_14: 0.9203 - auc_14: 0.9907 - recall_14: 0.8981 - val_loss: 0.1429 - val_acc: 0.9474 - val_precision_14: 0.8762 - val_auc_14: 0.9773 - val_recall_14: 0.8723
Epoch 39/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0927 - acc: 0.9620 - precision_14: 0.9203 - auc_14: 0.9913 - recall_14: 0.9026 - val_loss: 0.1400 - val_acc: 0.9464 - val_precision_14: 0.8859 - val_auc_14: 0.9788 - val_recall_14: 0.8671
Epoch 40/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0962 - acc: 0.9619 - precision_14: 0.9197 - auc_14: 0.9911 - recall_14: 0.9028 - val_loss: 0.1525 - val_acc: 0.9488 - val_precision_14: 0.9210 - val_auc_14: 0.9717 - val_recall_14: 0.8119
Epoch 41/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0972 - acc: 0.9611 - precision_14: 0.9186 - auc_14: 0.9904 - recall_14: 0.9001 - val_loss: 0.1600 - val_acc: 0.9419 - val_precision_14: 0.8805 - val_auc_14: 0.9749 - val_recall_14: 0.8504
Epoch 42/100
254/254 [==============================] - 55s 216ms/step - loss: 0.0793 - acc: 0.9675 - precision_14: 0.9310 - auc_14: 0.9936 - recall_14: 0.9180 - val_loss: 0.1595 - val_acc: 0.9462 - val_precision_14: 0.9165 - val_auc_14: 0.9728 - val_recall_14: 0.8189
Epoch 43/100
254/254 [==============================] - 55s 215ms/step - loss: 0.0693 - acc: 0.9713 - precision_14: 0.9392 - auc_14: 0.9951 - recall_14: 0.9274 - val_loss: 0.1646 - val_acc: 0.9453 - val_precision_14: 0.9053 - val_auc_14: 0.9720 - val_recall_14: 0.8356
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.0667 - acc: 0.9723 - precision_14: 0.9418 - auc_14: 0.9954 - recall_14: 0.9297Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 216ms/step - loss: 0.0667 - acc: 0.9723 - precision_14: 0.9418 - auc_14: 0.9954 - recall_14: 0.9297 - val_loss: 0.1865 - val_acc: 0.9445 - val_precision_14: 0.9426 - val_auc_14: 0.9655 - val_recall_14: 0.7865
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1403 - acc: 0.9460 - precision_14: 0.8980 - auc_14: 0.9798 - recall_14: 0.8556
---------------TEST METRICS----------------------
jaccard_index 0.7823572866289026
test_sensitivity 0.8664804316509507
test_specifitivity 0.967171331222904
test_accuracy 0.944656696725399
test_precision 0.8837403019222771
test_jaccard_score 0.7823572866289026
test_dicecoef 0.8750252623434804
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-224728.h5
[1.88336761 1.52519616 1.73183489 1.75159259 1.92131859] [0.9446567  0.78235729 0.8837403  0.86648043 0.96717133]

-------------------------
Averaged metrics for Baseline + Gaussian Blur - lesion: [0.94267477 0.76918448 0.8718584  0.87269101 0.96282997]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Histogram Equalization - lesion, PARAMS: {'histogram_equalization': True, 'augumentation': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-24 22:47:29.162720: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 22:47:29.162818: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_31"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_16 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_285 (Conv2D)             (None, 256, 256, 32) 320         input_16[0][0]
__________________________________________________________________________________________________
conv2d_286 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_285[0][0]
__________________________________________________________________________________________________
max_pooling2d_60 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_286[0][0]
__________________________________________________________________________________________________
conv2d_287 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_60[0][0]
__________________________________________________________________________________________________
conv2d_288 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_287[0][0]
__________________________________________________________________________________________________
max_pooling2d_61 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_288[0][0]
__________________________________________________________________________________________________
conv2d_289 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_61[0][0]
__________________________________________________________________________________________________
conv2d_290 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_289[0][0]
__________________________________________________________________________________________________
max_pooling2d_62 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_290[0][0]
__________________________________________________________________________________________________
conv2d_291 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_62[0][0]
__________________________________________________________________________________________________
conv2d_292 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_291[0][0]
__________________________________________________________________________________________________
max_pooling2d_63 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_292[0][0]
__________________________________________________________________________________________________
conv2d_293 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_63[0][0]
__________________________________________________________________________________________________
conv2d_294 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_293[0][0]
__________________________________________________________________________________________________
up_sampling2d_60 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_294[0][0]
__________________________________________________________________________________________________
concatenate_60 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_60[0][0]
                                                                 conv2d_292[0][0]
__________________________________________________________________________________________________
conv2d_295 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_60[0][0]
__________________________________________________________________________________________________
conv2d_296 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_295[0][0]
__________________________________________________________________________________________________
up_sampling2d_61 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_296[0][0]
__________________________________________________________________________________________________
concatenate_61 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_61[0][0]
                                                                 conv2d_290[0][0]
__________________________________________________________________________________________________
conv2d_297 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_61[0][0]
__________________________________________________________________________________________________
conv2d_298 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_297[0][0]
__________________________________________________________________________________________________
up_sampling2d_62 (UpSampling2D) (None, 128, 128, 128 0           conv2d_298[0][0]
__________________________________________________________________________________________________
concatenate_62 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_62[0][0]
                                                                 conv2d_288[0][0]
__________________________________________________________________________________________________
conv2d_299 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_62[0][0]
__________________________________________________________________________________________________
conv2d_300 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_299[0][0]
__________________________________________________________________________________________________
up_sampling2d_63 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_300[0][0]
__________________________________________________________________________________________________
concatenate_63 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_63[0][0]
                                                                 conv2d_286[0][0]
__________________________________________________________________________________________________
conv2d_301 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_63[0][0]
__________________________________________________________________________________________________
conv2d_302 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_301[0][0]
__________________________________________________________________________________________________
conv2d_303 (Conv2D)             (None, 256, 256, 1)  33          conv2d_302[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.7006 - acc: 0.1913 - precision_15: 0.1719 - auc_15: 0.3991 - recall_15: 0.98612021-09-24 22:48:05.465345: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 22:48:05.465945: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 22:48:05.951573: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 22:48:05.964172: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05
2021-09-24 22:48:05.984163: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.trace.json.gz
2021-09-24 22:48:06.009775: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05
2021-09-24 22:48:06.018336: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.memory_profile.json.gz
2021-09-24 22:48:06.038282: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05Dumped tool data for xplane.pb to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-224729/train/plugins/profile/2021_09_24_22_48_05/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:12 - loss: 0.6923 - acc: 0.4127 - precision_15: 0.1719 - auc_15: 0.3828 - recall_15: 0.3090WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1971s vs `on_train_batch_end` time: 0.3769s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4221 - acc: 0.8210 - precision_15: 0.7547 - auc_15: 0.8336 - recall_15: 0.2629 - val_loss: 0.3564 - val_acc: 0.8560 - val_precision_15: 0.6442 - val_auc_15: 0.8902 - val_recall_15: 0.7193
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2994 - acc: 0.8745 - precision_15: 0.8217 - auc_15: 0.8970 - recall_15: 0.5463 - val_loss: 0.3099 - val_acc: 0.8758 - val_precision_15: 0.8932 - val_auc_15: 0.8944 - val_recall_15: 0.4703
Epoch 3/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2921 - acc: 0.8779 - precision_15: 0.8217 - auc_15: 0.9019 - recall_15: 0.5666 - val_loss: 0.2973 - val_acc: 0.8832 - val_precision_15: 0.8286 - val_auc_15: 0.8987 - val_recall_15: 0.5727
Epoch 4/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2881 - acc: 0.8798 - precision_15: 0.8186 - auc_15: 0.9038 - recall_15: 0.5832 - val_loss: 0.2811 - val_acc: 0.8931 - val_precision_15: 0.8293 - val_auc_15: 0.9047 - val_recall_15: 0.6220
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2928 - acc: 0.8801 - precision_15: 0.8282 - auc_15: 0.9005 - recall_15: 0.5761 - val_loss: 0.2947 - val_acc: 0.8855 - val_precision_15: 0.8183 - val_auc_15: 0.8937 - val_recall_15: 0.5702
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2795 - acc: 0.8853 - precision_15: 0.8405 - auc_15: 0.9081 - recall_15: 0.5901 - val_loss: 0.3268 - val_acc: 0.8699 - val_precision_15: 0.6641 - val_auc_15: 0.9215 - val_recall_15: 0.8095
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2909 - acc: 0.8795 - precision_15: 0.8207 - auc_15: 0.9018 - recall_15: 0.5784 - val_loss: 0.2961 - val_acc: 0.8833 - val_precision_15: 0.7980 - val_auc_15: 0.8944 - val_recall_15: 0.5925
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2708 - acc: 0.8906 - precision_15: 0.8524 - auc_15: 0.9121 - recall_15: 0.6085 - val_loss: 0.2725 - val_acc: 0.9034 - val_precision_15: 0.7998 - val_auc_15: 0.9139 - val_recall_15: 0.7467
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2267 - acc: 0.9091 - precision_15: 0.8659 - auc_15: 0.9401 - recall_15: 0.7033 - val_loss: 0.2182 - val_acc: 0.9255 - val_precision_15: 0.8934 - val_auc_15: 0.9497 - val_recall_15: 0.7309
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2101 - acc: 0.9142 - precision_15: 0.8737 - auc_15: 0.9507 - recall_15: 0.7218 - val_loss: 0.2413 - val_acc: 0.9058 - val_precision_15: 0.7900 - val_auc_15: 0.9408 - val_recall_15: 0.7611
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2015 - acc: 0.9171 - precision_15: 0.8797 - auc_15: 0.9545 - recall_15: 0.7342 - val_loss: 0.2067 - val_acc: 0.9185 - val_precision_15: 0.7842 - val_auc_15: 0.9631 - val_recall_15: 0.8477
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1904 - acc: 0.9210 - precision_15: 0.8770 - auc_15: 0.9607 - recall_15: 0.7581 - val_loss: 0.1861 - val_acc: 0.9269 - val_precision_15: 0.8053 - val_auc_15: 0.9687 - val_recall_15: 0.8620
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1817 - acc: 0.9246 - precision_15: 0.8857 - auc_15: 0.9645 - recall_15: 0.7669 - val_loss: 0.1695 - val_acc: 0.9320 - val_precision_15: 0.8263 - val_auc_15: 0.9730 - val_recall_15: 0.8650
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1753 - acc: 0.9264 - precision_15: 0.8848 - auc_15: 0.9671 - recall_15: 0.7786 - val_loss: 0.1737 - val_acc: 0.9328 - val_precision_15: 0.8826 - val_auc_15: 0.9653 - val_recall_15: 0.7779
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1708 - acc: 0.9288 - precision_15: 0.8899 - auc_15: 0.9691 - recall_15: 0.7842 - val_loss: 0.2016 - val_acc: 0.9196 - val_precision_15: 0.7751 - val_auc_15: 0.9722 - val_recall_15: 0.8999
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1656 - acc: 0.9291 - precision_15: 0.8853 - auc_15: 0.9713 - recall_15: 0.7926 - val_loss: 0.1569 - val_acc: 0.9389 - val_precision_15: 0.9088 - val_auc_15: 0.9703 - val_recall_15: 0.7687
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1704 - acc: 0.9283 - precision_15: 0.8891 - auc_15: 0.9696 - recall_15: 0.7840 - val_loss: 0.1870 - val_acc: 0.9280 - val_precision_15: 0.8370 - val_auc_15: 0.9699 - val_recall_15: 0.8352
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1674 - acc: 0.9301 - precision_15: 0.8933 - auc_15: 0.9706 - recall_15: 0.7885 - val_loss: 0.1529 - val_acc: 0.9444 - val_precision_15: 0.8769 - val_auc_15: 0.9771 - val_recall_15: 0.8508
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1596 - acc: 0.9323 - precision_15: 0.8936 - auc_15: 0.9733 - recall_15: 0.7989 - val_loss: 0.1582 - val_acc: 0.9366 - val_precision_15: 0.8667 - val_auc_15: 0.9744 - val_recall_15: 0.8417
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1632 - acc: 0.9310 - precision_15: 0.8897 - auc_15: 0.9723 - recall_15: 0.7984 - val_loss: 0.1453 - val_acc: 0.9421 - val_precision_15: 0.8502 - val_auc_15: 0.9750 - val_recall_15: 0.8469
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1618 - acc: 0.9312 - precision_15: 0.8910 - auc_15: 0.9727 - recall_15: 0.7981 - val_loss: 0.1654 - val_acc: 0.9370 - val_precision_15: 0.8548 - val_auc_15: 0.9790 - val_recall_15: 0.8780
Epoch 22/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1556 - acc: 0.9339 - precision_15: 0.8985 - auc_15: 0.9746 - recall_15: 0.8032 - val_loss: 0.1784 - val_acc: 0.9376 - val_precision_15: 0.8451 - val_auc_15: 0.9732 - val_recall_15: 0.8482
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1533 - acc: 0.9349 - precision_15: 0.9000 - auc_15: 0.9748 - recall_15: 0.8061 - val_loss: 0.1573 - val_acc: 0.9395 - val_precision_15: 0.8485 - val_auc_15: 0.9784 - val_recall_15: 0.8691
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1499 - acc: 0.9361 - precision_15: 0.9048 - auc_15: 0.9765 - recall_15: 0.8070 - val_loss: 0.1931 - val_acc: 0.9191 - val_precision_15: 0.7574 - val_auc_15: 0.9768 - val_recall_15: 0.9198
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1559 - acc: 0.9341 - precision_15: 0.8945 - auc_15: 0.9748 - recall_15: 0.8097 - val_loss: 0.1501 - val_acc: 0.9434 - val_precision_15: 0.8389 - val_auc_15: 0.9833 - val_recall_15: 0.9002
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1481 - acc: 0.9368 - precision_15: 0.9023 - auc_15: 0.9769 - recall_15: 0.8133 - val_loss: 0.1469 - val_acc: 0.9430 - val_precision_15: 0.8999 - val_auc_15: 0.9767 - val_recall_15: 0.8205
Epoch 27/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1480 - acc: 0.9372 - precision_15: 0.9092 - auc_15: 0.9770 - recall_15: 0.8088 - val_loss: 0.1414 - val_acc: 0.9433 - val_precision_15: 0.8518 - val_auc_15: 0.9819 - val_recall_15: 0.8877
Epoch 28/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1447 - acc: 0.9376 - precision_15: 0.9045 - auc_15: 0.9782 - recall_15: 0.8164 - val_loss: 0.1634 - val_acc: 0.9354 - val_precision_15: 0.8100 - val_auc_15: 0.9812 - val_recall_15: 0.9097
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1487 - acc: 0.9367 - precision_15: 0.9060 - auc_15: 0.9768 - recall_15: 0.8100 - val_loss: 0.2167 - val_acc: 0.9043 - val_precision_15: 0.7224 - val_auc_15: 0.9782 - val_recall_15: 0.9356
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1456 - acc: 0.9374 - precision_15: 0.9022 - auc_15: 0.9778 - recall_15: 0.8194 - val_loss: 0.1436 - val_acc: 0.9442 - val_precision_15: 0.9176 - val_auc_15: 0.9793 - val_recall_15: 0.8090
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1439 - acc: 0.9383 - precision_15: 0.9071 - auc_15: 0.9783 - recall_15: 0.8171 - val_loss: 0.1617 - val_acc: 0.9364 - val_precision_15: 0.8115 - val_auc_15: 0.9821 - val_recall_15: 0.9073
Epoch 32/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1449 - acc: 0.9378 - precision_15: 0.9056 - auc_15: 0.9782 - recall_15: 0.8172 - val_loss: 0.1387 - val_acc: 0.9456 - val_precision_15: 0.8796 - val_auc_15: 0.9815 - val_recall_15: 0.8641
Epoch 33/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1436 - acc: 0.9383 - precision_15: 0.9032 - auc_15: 0.9786 - recall_15: 0.8212 - val_loss: 0.1314 - val_acc: 0.9494 - val_precision_15: 0.8870 - val_auc_15: 0.9828 - val_recall_15: 0.8709
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1423 - acc: 0.9387 - precision_15: 0.9079 - auc_15: 0.9790 - recall_15: 0.8174 - val_loss: 0.1332 - val_acc: 0.9500 - val_precision_15: 0.8996 - val_auc_15: 0.9815 - val_recall_15: 0.8384
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1385 - acc: 0.9402 - precision_15: 0.9095 - auc_15: 0.9799 - recall_15: 0.8233 - val_loss: 0.1407 - val_acc: 0.9434 - val_precision_15: 0.8632 - val_auc_15: 0.9834 - val_recall_15: 0.8896
Epoch 36/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1401 - acc: 0.9401 - precision_15: 0.9010 - auc_15: 0.9797 - recall_15: 0.8331 - val_loss: 0.1362 - val_acc: 0.9447 - val_precision_15: 0.8759 - val_auc_15: 0.9807 - val_recall_15: 0.8623
Epoch 37/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1384 - acc: 0.9411 - precision_15: 0.9020 - auc_15: 0.9800 - recall_15: 0.8380 - val_loss: 0.1810 - val_acc: 0.9393 - val_precision_15: 0.9704 - val_auc_15: 0.9805 - val_recall_15: 0.7413
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1353 - acc: 0.9419 - precision_15: 0.9091 - auc_15: 0.9810 - recall_15: 0.8350 - val_loss: 0.1299 - val_acc: 0.9502 - val_precision_15: 0.8821 - val_auc_15: 0.9834 - val_recall_15: 0.8800
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1351 - acc: 0.9416 - precision_15: 0.9092 - auc_15: 0.9810 - recall_15: 0.8320 - val_loss: 0.1340 - val_acc: 0.9458 - val_precision_15: 0.9216 - val_auc_15: 0.9819 - val_recall_15: 0.8227
Epoch 40/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1337 - acc: 0.9423 - precision_15: 0.9107 - auc_15: 0.9814 - recall_15: 0.8354 - val_loss: 0.1270 - val_acc: 0.9510 - val_precision_15: 0.9000 - val_auc_15: 0.9826 - val_recall_15: 0.8478
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1392 - acc: 0.9404 - precision_15: 0.9030 - auc_15: 0.9800 - recall_15: 0.8335 - val_loss: 0.1480 - val_acc: 0.9381 - val_precision_15: 0.8538 - val_auc_15: 0.9799 - val_recall_15: 0.8659
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1356 - acc: 0.9414 - precision_15: 0.9091 - auc_15: 0.9810 - recall_15: 0.8329 - val_loss: 0.1413 - val_acc: 0.9468 - val_precision_15: 0.9078 - val_auc_15: 0.9807 - val_recall_15: 0.8312
Epoch 43/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1343 - acc: 0.9421 - precision_15: 0.9104 - auc_15: 0.9814 - recall_15: 0.8352 - val_loss: 0.1542 - val_acc: 0.9411 - val_precision_15: 0.8698 - val_auc_15: 0.9783 - val_recall_15: 0.8572
Epoch 44/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1351 - acc: 0.9418 - precision_15: 0.9090 - auc_15: 0.9812 - recall_15: 0.8337 - val_loss: 0.1383 - val_acc: 0.9467 - val_precision_15: 0.9595 - val_auc_15: 0.9819 - val_recall_15: 0.7818
Epoch 45/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1340 - acc: 0.9420 - precision_15: 0.9081 - auc_15: 0.9814 - recall_15: 0.8355 - val_loss: 0.1362 - val_acc: 0.9460 - val_precision_15: 0.8914 - val_auc_15: 0.9817 - val_recall_15: 0.8518
Epoch 46/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1302 - acc: 0.9426 - precision_15: 0.9085 - auc_15: 0.9826 - recall_15: 0.8382 - val_loss: 0.1350 - val_acc: 0.9454 - val_precision_15: 0.8611 - val_auc_15: 0.9828 - val_recall_15: 0.8826
Epoch 47/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1306 - acc: 0.9432 - precision_15: 0.9114 - auc_15: 0.9824 - recall_15: 0.8380 - val_loss: 0.1341 - val_acc: 0.9454 - val_precision_15: 0.8635 - val_auc_15: 0.9829 - val_recall_15: 0.8853
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1317 - acc: 0.9427 - precision_15: 0.9078 - auc_15: 0.9819 - recall_15: 0.8406 - val_loss: 0.1229 - val_acc: 0.9512 - val_precision_15: 0.9256 - val_auc_15: 0.9832 - val_recall_15: 0.8343
Epoch 49/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1304 - acc: 0.9434 - precision_15: 0.9134 - auc_15: 0.9822 - recall_15: 0.8363 - val_loss: 0.1589 - val_acc: 0.9394 - val_precision_15: 0.8417 - val_auc_15: 0.9806 - val_recall_15: 0.8804
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1306 - acc: 0.9434 - precision_15: 0.9107 - auc_15: 0.9824 - recall_15: 0.8392 - val_loss: 0.1326 - val_acc: 0.9439 - val_precision_15: 0.8481 - val_auc_15: 0.9832 - val_recall_15: 0.8973
Epoch 51/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1319 - acc: 0.9421 - precision_15: 0.9084 - auc_15: 0.9820 - recall_15: 0.8367 - val_loss: 0.1301 - val_acc: 0.9482 - val_precision_15: 0.8847 - val_auc_15: 0.9833 - val_recall_15: 0.8663
Epoch 52/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1293 - acc: 0.9430 - precision_15: 0.9051 - auc_15: 0.9830 - recall_15: 0.8443 - val_loss: 0.1342 - val_acc: 0.9517 - val_precision_15: 0.9026 - val_auc_15: 0.9842 - val_recall_15: 0.8678
Epoch 53/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1273 - acc: 0.9440 - precision_15: 0.9143 - auc_15: 0.9835 - recall_15: 0.8385 - val_loss: 0.1282 - val_acc: 0.9477 - val_precision_15: 0.8791 - val_auc_15: 0.9841 - val_recall_15: 0.8720
Epoch 54/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1292 - acc: 0.9437 - precision_15: 0.9130 - auc_15: 0.9826 - recall_15: 0.8398 - val_loss: 0.1328 - val_acc: 0.9466 - val_precision_15: 0.9177 - val_auc_15: 0.9807 - val_recall_15: 0.8234
Epoch 55/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1299 - acc: 0.9428 - precision_15: 0.9122 - auc_15: 0.9826 - recall_15: 0.8352 - val_loss: 0.1249 - val_acc: 0.9511 - val_precision_15: 0.9412 - val_auc_15: 0.9843 - val_recall_15: 0.8174
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1290 - acc: 0.9438 - precision_15: 0.9134 - auc_15: 0.9827 - recall_15: 0.8381 - val_loss: 0.1404 - val_acc: 0.9416 - val_precision_15: 0.8399 - val_auc_15: 0.9834 - val_recall_15: 0.9035
Epoch 57/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1297 - acc: 0.9437 - precision_15: 0.9114 - auc_15: 0.9824 - recall_15: 0.8407 - val_loss: 0.1315 - val_acc: 0.9474 - val_precision_15: 0.8662 - val_auc_15: 0.9840 - val_recall_15: 0.8860
Epoch 58/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1288 - acc: 0.9440 - precision_15: 0.9152 - auc_15: 0.9831 - recall_15: 0.8387 - val_loss: 0.1423 - val_acc: 0.9455 - val_precision_15: 0.9455 - val_auc_15: 0.9804 - val_recall_15: 0.7784
Epoch 59/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1250 - acc: 0.9448 - precision_15: 0.9120 - auc_15: 0.9839 - recall_15: 0.8454 - val_loss: 0.1436 - val_acc: 0.9413 - val_precision_15: 0.8434 - val_auc_15: 0.9827 - val_recall_15: 0.9052
Epoch 60/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1289 - acc: 0.9438 - precision_15: 0.9087 - auc_15: 0.9830 - recall_15: 0.8446 - val_loss: 0.1386 - val_acc: 0.9476 - val_precision_15: 0.9566 - val_auc_15: 0.9836 - val_recall_15: 0.7894
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1274 - acc: 0.9441 - precision_15: 0.9084 - auc_15: 0.9832 - recall_15: 0.8471 - val_loss: 0.1229 - val_acc: 0.9527 - val_precision_15: 0.9068 - val_auc_15: 0.9837 - val_recall_15: 0.8665
Epoch 62/100
254/254 [==============================] - ETA: 0s - loss: 0.1279 - acc: 0.9442 - precision_15: 0.9131 - auc_15: 0.9831 - recall_15: 0.8417Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1279 - acc: 0.9442 - precision_15: 0.9131 - auc_15: 0.9831 - recall_15: 0.8417 - val_loss: 0.1257 - val_acc: 0.9477 - val_precision_15: 0.8867 - val_auc_15: 0.9828 - val_recall_15: 0.8597
Epoch 00062: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1277 - acc: 0.9504 - precision_15: 0.9164 - auc_15: 0.9831 - recall_15: 0.8563
---------------TEST METRICS----------------------
jaccard_index 0.8013608874450395
test_sensitivity 0.865869892481705
test_specifitivity 0.9745267510963855
test_accuracy 0.9502309163411459
test_precision 0.9073168678386293
test_jaccard_score 0.8013608874450395
test_dicecoef 0.8861089834218543
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_24092021-234629.h5
[0. 0. 0. 0. 0.] [0.95023092 0.80136089 0.90731687 0.86586989 0.97452675]

-------------------------
Rep: 1
-------------------------

2021-09-24 23:46:30.466521: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 23:46:30.466650: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_33"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_17 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_304 (Conv2D)             (None, 256, 256, 32) 320         input_17[0][0]
__________________________________________________________________________________________________
conv2d_305 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_304[0][0]
__________________________________________________________________________________________________
max_pooling2d_64 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_305[0][0]
__________________________________________________________________________________________________
conv2d_306 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_64[0][0]
__________________________________________________________________________________________________
conv2d_307 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_306[0][0]
__________________________________________________________________________________________________
max_pooling2d_65 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_307[0][0]
__________________________________________________________________________________________________
conv2d_308 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_65[0][0]
__________________________________________________________________________________________________
conv2d_309 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_308[0][0]
__________________________________________________________________________________________________
max_pooling2d_66 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_309[0][0]
__________________________________________________________________________________________________
conv2d_310 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_66[0][0]
__________________________________________________________________________________________________
conv2d_311 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_310[0][0]
__________________________________________________________________________________________________
max_pooling2d_67 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_311[0][0]
__________________________________________________________________________________________________
conv2d_312 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_67[0][0]
__________________________________________________________________________________________________
conv2d_313 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_312[0][0]
__________________________________________________________________________________________________
up_sampling2d_64 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_313[0][0]
__________________________________________________________________________________________________
concatenate_64 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_64[0][0]
                                                                 conv2d_311[0][0]
__________________________________________________________________________________________________
conv2d_314 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_64[0][0]
__________________________________________________________________________________________________
conv2d_315 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_314[0][0]
__________________________________________________________________________________________________
up_sampling2d_65 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_315[0][0]
__________________________________________________________________________________________________
concatenate_65 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_65[0][0]
                                                                 conv2d_309[0][0]
__________________________________________________________________________________________________
conv2d_316 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_65[0][0]
__________________________________________________________________________________________________
conv2d_317 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_316[0][0]
__________________________________________________________________________________________________
up_sampling2d_66 (UpSampling2D) (None, 128, 128, 128 0           conv2d_317[0][0]
__________________________________________________________________________________________________
concatenate_66 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_66[0][0]
                                                                 conv2d_307[0][0]
__________________________________________________________________________________________________
conv2d_318 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_66[0][0]
__________________________________________________________________________________________________
conv2d_319 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_318[0][0]
__________________________________________________________________________________________________
up_sampling2d_67 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_319[0][0]
__________________________________________________________________________________________________
concatenate_67 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_67[0][0]
                                                                 conv2d_305[0][0]
__________________________________________________________________________________________________
conv2d_320 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_67[0][0]
__________________________________________________________________________________________________
conv2d_321 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_320[0][0]
__________________________________________________________________________________________________
conv2d_322 (Conv2D)             (None, 256, 256, 1)  33          conv2d_321[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6864 - acc: 0.7742 - precision_16: 0.0065 - auc_16: 0.5835 - recall_16: 0.00232021-09-24 23:47:06.405966: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-24 23:47:06.406603: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-24 23:47:06.923065: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-24 23:47:06.933586: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06
2021-09-24 23:47:06.936832: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.trace.json.gz
2021-09-24 23:47:06.964034: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06
2021-09-24 23:47:06.972454: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.memory_profile.json.gz
2021-09-24 23:47:06.985560: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06Dumped tool data for xplane.pb to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210924-234630/train/plugins/profile/2021_09_24_23_47_06/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:13 - loss: 0.6788 - acc: 0.7042 - precision_16: 0.0065 - auc_16: 0.4099 - recall_16: 7.2681e-04WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1994s vs `on_train_batch_end` time: 0.3809s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.3841 - acc: 0.8269 - precision_16: 0.8161 - auc_16: 0.8355 - recall_16: 0.2635 - val_loss: 0.3797 - val_acc: 0.8586 - val_precision_16: 0.8569 - val_auc_16: 0.8451 - val_recall_16: 0.4014
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3088 - acc: 0.8718 - precision_16: 0.8213 - auc_16: 0.8905 - recall_16: 0.5299 - val_loss: 0.2969 - val_acc: 0.8840 - val_precision_16: 0.9008 - val_auc_16: 0.8970 - val_recall_16: 0.5089
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2971 - acc: 0.8799 - precision_16: 0.8203 - auc_16: 0.9001 - recall_16: 0.5806 - val_loss: 0.2786 - val_acc: 0.8941 - val_precision_16: 0.8713 - val_auc_16: 0.9081 - val_recall_16: 0.5929
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2747 - acc: 0.8891 - precision_16: 0.8203 - auc_16: 0.9132 - recall_16: 0.6409 - val_loss: 0.2455 - val_acc: 0.9080 - val_precision_16: 0.8778 - val_auc_16: 0.9294 - val_recall_16: 0.6557
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2478 - acc: 0.9007 - precision_16: 0.8406 - auc_16: 0.9318 - recall_16: 0.6882 - val_loss: 0.3500 - val_acc: 0.8634 - val_precision_16: 0.6955 - val_auc_16: 0.8745 - val_recall_16: 0.5983
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2636 - acc: 0.8930 - precision_16: 0.8207 - auc_16: 0.9236 - recall_16: 0.6639 - val_loss: 0.1908 - val_acc: 0.9272 - val_precision_16: 0.8214 - val_auc_16: 0.9680 - val_recall_16: 0.8488
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2034 - acc: 0.9173 - precision_16: 0.8578 - auc_16: 0.9556 - recall_16: 0.7615 - val_loss: 0.1917 - val_acc: 0.9241 - val_precision_16: 0.8398 - val_auc_16: 0.9615 - val_recall_16: 0.7881
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1847 - acc: 0.9233 - precision_16: 0.8676 - auc_16: 0.9639 - recall_16: 0.7829 - val_loss: 0.1905 - val_acc: 0.9259 - val_precision_16: 0.8288 - val_auc_16: 0.9682 - val_recall_16: 0.8344
Epoch 9/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1898 - acc: 0.9217 - precision_16: 0.8647 - auc_16: 0.9621 - recall_16: 0.7790 - val_loss: 0.1633 - val_acc: 0.9413 - val_precision_16: 0.9186 - val_auc_16: 0.9760 - val_recall_16: 0.7893
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1684 - acc: 0.9288 - precision_16: 0.8778 - auc_16: 0.9705 - recall_16: 0.8005 - val_loss: 0.1802 - val_acc: 0.9346 - val_precision_16: 0.8682 - val_auc_16: 0.9656 - val_recall_16: 0.8178
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1651 - acc: 0.9299 - precision_16: 0.8833 - auc_16: 0.9717 - recall_16: 0.8018 - val_loss: 0.1540 - val_acc: 0.9372 - val_precision_16: 0.8518 - val_auc_16: 0.9765 - val_recall_16: 0.8508
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1657 - acc: 0.9299 - precision_16: 0.8811 - auc_16: 0.9711 - recall_16: 0.8028 - val_loss: 0.1762 - val_acc: 0.9248 - val_precision_16: 0.7774 - val_auc_16: 0.9759 - val_recall_16: 0.9020
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1620 - acc: 0.9318 - precision_16: 0.8850 - auc_16: 0.9728 - recall_16: 0.8088 - val_loss: 0.1462 - val_acc: 0.9405 - val_precision_16: 0.8434 - val_auc_16: 0.9819 - val_recall_16: 0.8876
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1533 - acc: 0.9342 - precision_16: 0.8910 - auc_16: 0.9758 - recall_16: 0.8141 - val_loss: 0.1595 - val_acc: 0.9339 - val_precision_16: 0.8234 - val_auc_16: 0.9758 - val_recall_16: 0.8650
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1508 - acc: 0.9357 - precision_16: 0.8941 - auc_16: 0.9766 - recall_16: 0.8176 - val_loss: 0.1698 - val_acc: 0.9316 - val_precision_16: 0.8368 - val_auc_16: 0.9730 - val_recall_16: 0.8607
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1522 - acc: 0.9346 - precision_16: 0.8893 - auc_16: 0.9762 - recall_16: 0.8180 - val_loss: 0.1331 - val_acc: 0.9490 - val_precision_16: 0.9225 - val_auc_16: 0.9801 - val_recall_16: 0.8108
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1562 - acc: 0.9330 - precision_16: 0.8889 - auc_16: 0.9751 - recall_16: 0.8103 - val_loss: 0.1539 - val_acc: 0.9377 - val_precision_16: 0.8568 - val_auc_16: 0.9786 - val_recall_16: 0.8605
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1516 - acc: 0.9356 - precision_16: 0.8954 - auc_16: 0.9762 - recall_16: 0.8163 - val_loss: 0.1385 - val_acc: 0.9494 - val_precision_16: 0.9172 - val_auc_16: 0.9806 - val_recall_16: 0.8302
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1462 - acc: 0.9367 - precision_16: 0.8910 - auc_16: 0.9786 - recall_16: 0.8260 - val_loss: 0.1497 - val_acc: 0.9421 - val_precision_16: 0.9235 - val_auc_16: 0.9800 - val_recall_16: 0.8036
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1441 - acc: 0.9377 - precision_16: 0.8959 - auc_16: 0.9787 - recall_16: 0.8276 - val_loss: 0.1335 - val_acc: 0.9472 - val_precision_16: 0.8603 - val_auc_16: 0.9787 - val_recall_16: 0.8645
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1453 - acc: 0.9372 - precision_16: 0.8914 - auc_16: 0.9786 - recall_16: 0.8300 - val_loss: 0.1423 - val_acc: 0.9432 - val_precision_16: 0.8711 - val_auc_16: 0.9824 - val_recall_16: 0.8867
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1405 - acc: 0.9395 - precision_16: 0.9034 - auc_16: 0.9791 - recall_16: 0.8275 - val_loss: 0.1563 - val_acc: 0.9426 - val_precision_16: 0.8496 - val_auc_16: 0.9800 - val_recall_16: 0.8720
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1425 - acc: 0.9388 - precision_16: 0.8971 - auc_16: 0.9789 - recall_16: 0.8311 - val_loss: 0.1567 - val_acc: 0.9379 - val_precision_16: 0.8383 - val_auc_16: 0.9796 - val_recall_16: 0.8752
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1394 - acc: 0.9398 - precision_16: 0.9058 - auc_16: 0.9801 - recall_16: 0.8258 - val_loss: 0.1649 - val_acc: 0.9285 - val_precision_16: 0.7884 - val_auc_16: 0.9789 - val_recall_16: 0.9142
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1418 - acc: 0.9390 - precision_16: 0.9001 - auc_16: 0.9796 - recall_16: 0.8290 - val_loss: 0.1580 - val_acc: 0.9350 - val_precision_16: 0.8025 - val_auc_16: 0.9818 - val_recall_16: 0.9114
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1386 - acc: 0.9400 - precision_16: 0.9011 - auc_16: 0.9805 - recall_16: 0.8321 - val_loss: 0.1477 - val_acc: 0.9404 - val_precision_16: 0.8746 - val_auc_16: 0.9768 - val_recall_16: 0.8370
Epoch 27/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1354 - acc: 0.9416 - precision_16: 0.9103 - auc_16: 0.9811 - recall_16: 0.8311 - val_loss: 0.1284 - val_acc: 0.9497 - val_precision_16: 0.8907 - val_auc_16: 0.9840 - val_recall_16: 0.8706
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1367 - acc: 0.9400 - precision_16: 0.9018 - auc_16: 0.9813 - recall_16: 0.8322 - val_loss: 0.1865 - val_acc: 0.9192 - val_precision_16: 0.7504 - val_auc_16: 0.9803 - val_recall_16: 0.9286
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1386 - acc: 0.9399 - precision_16: 0.8993 - auc_16: 0.9803 - recall_16: 0.8357 - val_loss: 0.2136 - val_acc: 0.9040 - val_precision_16: 0.7204 - val_auc_16: 0.9790 - val_recall_16: 0.9389
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1376 - acc: 0.9402 - precision_16: 0.9004 - auc_16: 0.9808 - recall_16: 0.8363 - val_loss: 0.1290 - val_acc: 0.9496 - val_precision_16: 0.8992 - val_auc_16: 0.9848 - val_recall_16: 0.8581
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1346 - acc: 0.9413 - precision_16: 0.9070 - auc_16: 0.9816 - recall_16: 0.8335 - val_loss: 0.1486 - val_acc: 0.9412 - val_precision_16: 0.8344 - val_auc_16: 0.9801 - val_recall_16: 0.8979
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1333 - acc: 0.9419 - precision_16: 0.9046 - auc_16: 0.9820 - recall_16: 0.8399 - val_loss: 0.1365 - val_acc: 0.9467 - val_precision_16: 0.8897 - val_auc_16: 0.9822 - val_recall_16: 0.8568
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1361 - acc: 0.9408 - precision_16: 0.9034 - auc_16: 0.9809 - recall_16: 0.8340 - val_loss: 0.1395 - val_acc: 0.9432 - val_precision_16: 0.8426 - val_auc_16: 0.9827 - val_recall_16: 0.8987
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1355 - acc: 0.9414 - precision_16: 0.9059 - auc_16: 0.9812 - recall_16: 0.8337 - val_loss: 0.1393 - val_acc: 0.9438 - val_precision_16: 0.8430 - val_auc_16: 0.9809 - val_recall_16: 0.8763
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1314 - acc: 0.9419 - precision_16: 0.9082 - auc_16: 0.9825 - recall_16: 0.8341 - val_loss: 0.1526 - val_acc: 0.9377 - val_precision_16: 0.8322 - val_auc_16: 0.9826 - val_recall_16: 0.9055
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1381 - acc: 0.9402 - precision_16: 0.9035 - auc_16: 0.9803 - recall_16: 0.8301 - val_loss: 0.1457 - val_acc: 0.9391 - val_precision_16: 0.8347 - val_auc_16: 0.9802 - val_recall_16: 0.8903
Epoch 37/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1330 - acc: 0.9429 - precision_16: 0.9081 - auc_16: 0.9818 - recall_16: 0.8401 - val_loss: 0.1946 - val_acc: 0.9313 - val_precision_16: 0.9754 - val_auc_16: 0.9768 - val_recall_16: 0.6993
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1329 - acc: 0.9430 - precision_16: 0.9076 - auc_16: 0.9822 - recall_16: 0.8424 - val_loss: 0.1312 - val_acc: 0.9463 - val_precision_16: 0.8672 - val_auc_16: 0.9825 - val_recall_16: 0.8784
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1326 - acc: 0.9424 - precision_16: 0.9074 - auc_16: 0.9822 - recall_16: 0.8383 - val_loss: 0.1338 - val_acc: 0.9473 - val_precision_16: 0.9236 - val_auc_16: 0.9823 - val_recall_16: 0.8278
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1274 - acc: 0.9444 - precision_16: 0.9132 - auc_16: 0.9835 - recall_16: 0.8434 - val_loss: 0.1318 - val_acc: 0.9493 - val_precision_16: 0.8928 - val_auc_16: 0.9812 - val_recall_16: 0.8464
Epoch 41/100
254/254 [==============================] - ETA: 0s - loss: 0.1282 - acc: 0.9437 - precision_16: 0.9088 - auc_16: 0.9831 - recall_16: 0.8439Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1282 - acc: 0.9437 - precision_16: 0.9088 - auc_16: 0.9831 - recall_16: 0.8439 - val_loss: 0.1401 - val_acc: 0.9421 - val_precision_16: 0.8664 - val_auc_16: 0.9818 - val_recall_16: 0.8700
Epoch 00041: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1423 - acc: 0.9440 - precision_16: 0.8770 - auc_16: 0.9812 - recall_16: 0.8718
---------------TEST METRICS----------------------
jaccard_index 0.7615084777632753
test_sensitivity 0.88891647567656
test_specifitivity 0.9524459954285879
test_accuracy 0.9382407005797041
test_precision 0.8433458609503387
test_jaccard_score 0.7615084777632753
test_dicecoef 0.8655317553715939
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-002553.h5
[0.95023092 0.80136089 0.90731687 0.86586989 0.97452675] [0.9382407  0.76150848 0.84334586 0.88891648 0.952446  ]

-------------------------
Rep: 2
-------------------------

2021-09-25 00:25:53.918447: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 00:25:53.918573: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied histogram equalization on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.6746797118553127,
Validation samples: 383, channel mean: 0.6808880061846456
Model built.
Model: "functional_35"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_18 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_323 (Conv2D)             (None, 256, 256, 32) 320         input_18[0][0]
__________________________________________________________________________________________________
conv2d_324 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_323[0][0]
__________________________________________________________________________________________________
max_pooling2d_68 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_324[0][0]
__________________________________________________________________________________________________
conv2d_325 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_68[0][0]
__________________________________________________________________________________________________
conv2d_326 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_325[0][0]
__________________________________________________________________________________________________
max_pooling2d_69 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_326[0][0]
__________________________________________________________________________________________________
conv2d_327 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_69[0][0]
__________________________________________________________________________________________________
conv2d_328 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_327[0][0]
__________________________________________________________________________________________________
max_pooling2d_70 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_328[0][0]
__________________________________________________________________________________________________
conv2d_329 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_70[0][0]
__________________________________________________________________________________________________
conv2d_330 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_329[0][0]
__________________________________________________________________________________________________
max_pooling2d_71 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_330[0][0]
__________________________________________________________________________________________________
conv2d_331 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_71[0][0]
__________________________________________________________________________________________________
conv2d_332 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_331[0][0]
__________________________________________________________________________________________________
up_sampling2d_68 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_332[0][0]
__________________________________________________________________________________________________
concatenate_68 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_68[0][0]
                                                                 conv2d_330[0][0]
__________________________________________________________________________________________________
conv2d_333 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_68[0][0]
__________________________________________________________________________________________________
conv2d_334 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_333[0][0]
__________________________________________________________________________________________________
up_sampling2d_69 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_334[0][0]
__________________________________________________________________________________________________
concatenate_69 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_69[0][0]
                                                                 conv2d_328[0][0]
__________________________________________________________________________________________________
conv2d_335 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_69[0][0]
__________________________________________________________________________________________________
conv2d_336 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_335[0][0]
__________________________________________________________________________________________________
up_sampling2d_70 (UpSampling2D) (None, 128, 128, 128 0           conv2d_336[0][0]
__________________________________________________________________________________________________
concatenate_70 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_70[0][0]
                                                                 conv2d_326[0][0]
__________________________________________________________________________________________________
conv2d_337 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_70[0][0]
__________________________________________________________________________________________________
conv2d_338 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_337[0][0]
__________________________________________________________________________________________________
up_sampling2d_71 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_338[0][0]
__________________________________________________________________________________________________
concatenate_71 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_71[0][0]
                                                                 conv2d_324[0][0]
__________________________________________________________________________________________________
conv2d_339 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_71[0][0]
__________________________________________________________________________________________________
conv2d_340 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_339[0][0]
__________________________________________________________________________________________________
conv2d_341 (Conv2D)             (None, 256, 256, 1)  33          conv2d_340[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6629 - acc: 0.8051 - precision_17: 6.8162e-04 - auc_17: 0.7510 - recall_17: 1.1427e-042021-09-25 00:26:29.786751: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 00:26:29.788442: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 00:26:30.301972: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 00:26:30.311856: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30
2021-09-25 00:26:30.314814: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.trace.json.gz
2021-09-25 00:26:30.342193: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30
2021-09-25 00:26:30.352498: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.memory_profile.json.gz
2021-09-25 00:26:30.367157: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30Dumped tool data for xplane.pb to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-002553/train/plugins/profile/2021_09_25_00_26_30/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:13 - loss: 0.6657 - acc: 0.7197 - precision_17: 6.8162e-04 - auc_17: 0.4915 - recall_17: 3.5804e-05WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1987s vs `on_train_batch_end` time: 0.3817s). Check your callbacks.
  3/254 [..............................] - ETA: 1:05 - loss: 0.5919 - acc: 0.7611 - precision_17: 6.8162e-04 - auc_17: 0.5751 - recall_17: 2.7692e  4/254 [..............................] - ETA: 1:01 - loss: 3.5546 - acc: 0.7680 - precision_17: 6.8162e-04 - auc_17: 0.5519 - recall_17: 2.1187e  5/254 [..............................] - ETA: 59s - loss: 2.9578 - acc: 0.7664 - precision_17: 6.8162e-04 - auc_17: 0.5458 - recall_17: 1.6727e-  6/254 [..............................] - ETA: 57s - loss: 2.5630 - acc: 0.7598 - precision_17: 6.8162e-04 - auc_17: 0.5470 - recall_17: 1.3497e-  7/254 [..............................] - ETA: 56s - loss: 2.2809 - acc: 0.7603 - precision_17: 6.8162e-04 - auc_17: 0.5524 - recall_17: 1.1560e-  8/254 [..............................] - ETA: 55s - loss: 2.0672 - acc: 0.7643 - precision_17: 6.8162e-04 - auc_17: 0.5618 - recall_17: 1.0269e-  9/254 [>.............................] - ETA: 54s - loss: 1.9014 - acc: 0.7681 - precision_17: 6.8162e-04 - auc_17: 0.5682 - recall_17: 9.2617e- 10/254 [>.............................] - ETA: 53s - loss: 1.7650 - acc: 0.7811 - precision_17: 6.8162e-04 - auc_17: 0.5699 - recall_17: 8.8255e- 11/254 [>.............................] - ETA: 53s - loss: 1.6582 - acc: 0.7780 - precision_17: 6.8162e-04 - auc_17: 0.5731 - recall_17: 7.9012e- 12/254 [>.............................] - ETA: 52s - loss: 1.5686 - acc: 0.7728 - precision_17: 6.8162e-04 - auc_17: 0.5710 - recall_17: 7.0691e- 13/254 [>.............................] - ETA: 52s - loss: 1.4812 - acc: 0.7804 - precision_17: 6.8162e-04 - auc_17: 0.5745 - recall_17: 6.7480e- 14/254 [>.............................] - ETA: 51s - loss: 1.4199 - acc: 0.7783 - precision_17: 6.8162e-04 - auc_17: 0.5605 - recall_17: 6.2002e- 15/254 [>.............................] - ETA: 51s - loss: 1.3636 - acc: 0.7749 - precision_17: 6.8162e-04 - auc_17: 0.5509 - recall_17: 5.6948e- 16/254 [>.............................] - ETA: 50s - loss: 1.3142 - acc: 0.7723 - precision_17: 6.8162e-04 - auc_17: 0.5451 - recall_17: 5.2752e- 17/254 [=>............................] - ETA: 50s - loss: 1.2632 - acc: 0.7766 - precision_17: 6.8162e-04 - auc_17: 0.5502 - recall_17: 5.0596e- 18/254 [=>............................] - ETA: 50s - loss: 1.2234 - acc: 0.7753 - precision_17: 6.8162e-04 - auc_17: 0.5486 - recall_17: 4.7477e- 19/254 [=>............................] - ETA: 50s - loss: 1.1803 - acc: 0.7799 - precision_17: 6.8162e-04 - auc_17: 0.5558 - recall_17: 4.5913e- 20/254 [=>............................] - ETA: 49s - loss: 1.1492 - acc: 0.7770 - precision_17: 6.8162e-04 - auc_17: 0.5530 - recall_17: 4.3028e- 21/254 [=>............................] - ETA: 49s - loss: 1.1106 - acc: 0.7817 - precision_17: 6.8162e-04 - auc_17: 0.5640 - recall_17: 4.1857e- 22/254 [=>............................] - ETA: 49s - loss: 1.0824 - acc: 0.7811 - precision_17: 6.8162e-04 - auc_17: 0.5635 - recall_17: 3.9829e- 23/254 [=>............................] - ETA: 48s - loss: 1.0501 - acc: 0.7848 - precision_17: 6.8162e-04 - auc_17: 0.5722 - recall_17: 3.8750e- 24/254 [=>............................] - ETA: 48s - loss: 1.0373 - acc: 0.7771 - precision_17: 6.8162e-04 - auc_17: 0.5639 - recall_17: 3.5841e- 25/254 [=>............................] - ETA: 48s - loss: 1.0165 - acc: 0.7763 - precision_17: 6.8162e-04 - auc_17: 0.5659 - recall_17: 3.4272e- 26/254 [==>...........................] - ETA: 47s - loss: 0.9938 - acc: 0.7771 - precision_17: 6.8162e-04 - auc_17: 0.5731 - recall_17: 3.3076e- 27/254 [==>...........................] - ETA: 47s - loss: 0.9735 - acc: 0.7775 - precision_17: 6.8162e-04 - auc_17: 0.5792 - recall_17: 3.1902e- 28/254 [==>...........................] - ETA: 47s - loss: 0.9519 - acc: 0.7803 - precision_17: 6.8162e-04 - auc_17: 0.5860 - recall_17: 3.1150e- 29/254 [==>...........................] - ETA: 47s - loss: 0.9342 - acc: 0.7811 - precision_17: 6.8162e-04 - auc_17: 0.5905 - recall_17: 3.0177e- 30/254 [==>...........................] - ETA: 46s - loss: 0.9179 - acc: 0.7814 - precision_17: 6.8162e-04 - auc_17: 0.5946 - recall_17: 2.9214e- 31/254 [==>...........................] - ETA: 46s - loss: 0.8997 - acc: 0.7839 - precision_17: 6.8162e-04 - auc_17: 0.6010 - recall_17: 2.8586e- 32/254 [==>...........................] - ETA: 46s - loss: 0.8882 - acc: 0.7814 - precision_17: 6.8162e-04 - auc_17: 0.6049 - recall_17: 2.7380e- 33/254 [==>...........................] - ETA: 46s - loss: 0.8742 - acc: 0.7818 - precision_17: 6.8162e-04 - auc_17: 0.6101 - recall_17: 2.6589e- 34/254 [===>..........................] - ETA: 45s - loss: 0.8578 - acc: 0.7839 - precision_17: 6.8162e-04 - auc_17: 0.6179 - recall_17: 2.6058e- 35/254 [===>..........................] - ETA: 45s - loss: 0.8423 - acc: 0.7868 - precision_17: 6.8162e-04 - auc_17: 0.6231 - recall_17: 2.5656e- 36/254 [===>..........................] - ETA: 45s - loss: 0.8343 - acc: 0.7838 - precision_17: 6.8162e-04 - auc_17: 0.6251 - recall_17: 2.4598e- 37/254 [===>..........................] - ETA: 45s - loss: 0.8257 - acc: 0.7820 - precision_17: 6.8162e-04 - auc_17: 0.6273 - recall_17: 2.3733e- 38/254 [===>..........................] - ETA: 44s - loss: 0.8156 - acc: 0.7809 - precision_17: 6.8162e-04 - auc_17: 0.6339 - recall_17: 2.2989e- 39/254 [===>..........................] - ETA: 44s - loss: 0.8045 - acc: 0.7814 - precision_17: 6.8162e-04 - auc_17: 0.6402 - recall_17: 2.2442e- 40/254 [===>..........................] - ETA: 44s - loss: 0.7948 - acc: 0.7801 - precision_17: 6.8162e-04 - auc_17: 0.6481 - recall_17: 2.1757e- 41/254 [===>..........................] - ETA: 44s - loss: 0.7877 - acc: 0.7786 - precision_17: 6.8162e-04 - auc_17: 0.6524 - recall_17: 2.1076e- 42/254 [===>..........................] - ETA: 44s - loss: 0.7792 - acc: 0.7770 - precision_17: 6.8162e-04 - auc_17: 0.6607 - recall_17: 2.0421e- 43/254 [====>.........................] - ETA: 43s - loss: 0.7721 - acc: 0.7757 - precision_17: 6.8162e-04 - auc_17: 0.6656 - recall_17: 1.9836e- 44/254 [====>.........................] - ETA: 43s - loss: 0.7646 - acc: 0.7760 - precision_17: 6.8162e-04 - auc_17: 0.6681 - recall_17: 1.9409e- 45/254 [====>.........................] - ETA: 43s - loss: 0.7558 - acc: 0.7776 - precision_17: 6.8162e-04 - auc_17: 0.6711 - recall_17: 1.9108e- 46/254 [====>.........................] - ETA: 43s - loss: 0.7492 - acc: 0.7774 - precision_17: 6.8162e-04 - auc_17: 0.6734 - recall_17: 1.8682e- 47/254 [====>.........................] - ETA: 42s - loss: 0.7410 - acc: 0.7790 - precision_17: 6.8162e-04 - auc_17: 0.6758 - recall_17: 1.8410e- 48/254 [====>.........................] - ETA: 42s - loss: 0.7357 - acc: 0.7776 - precision_17: 6.8162e-04 - auc_17: 0.6795 - recall_17: 1.7913e- 49/254 [====>.........................] - ETA: 42s - loss: 0.7292 - acc: 0.7772 - precision_17: 6.8162e-04 - auc_17: 0.6839 - recall_17: 1.7519e- 50/254 [====>.........................] - ETA: 42s - loss: 0.7208 - acc: 0.7794 - precision_17: 6.8162e-04 - auc_17: 0.6874 - recall_17: 1.7334e- 51/254 [=====>........................] - ETA: 42s - loss: 0.7150 - acc: 0.7800 - precision_17: 6.8162e-04 - auc_17: 0.6903 - recall_17: 1.7045e- 52/254 [=====>........................] - ETA: 41s - loss: 0.7093 - acc: 0.7792 - precision_17: 6.8162e-04 - auc_17: 0.6950 - recall_17: 1.6650e- 53/254 [=====>........................] - ETA: 41s - loss: 0.7047 - acc: 0.7778 - precision_17: 6.8162e-04 - auc_17: 0.6987 - recall_17: 1.6238e- 54/254 [=====>........................] - ETA: 41s - loss: 0.6978 - acc: 0.7793 - precision_17: 6.8162e-04 - auc_17: 0.7015 - recall_17: 1.6042e- 55/254 [=====>........................] - ETA: 41s - loss: 0.6911 - acc: 0.7809 - precision_17: 6.8162e-04 - auc_17: 0.7039 - recall_17: 1.5868e- 56/254 [=====>........................] - ETA: 40s - loss: 0.6860 - acc: 0.7814 - precision_17: 6.8162e-04 - auc_17: 0.7049 - recall_17: 1.5615e- 57/254 [=====>........................] - ETA: 40s - loss: 0.6818 - acc: 0.7806 - precision_17: 6.8162e-04 - auc_17: 0.7076 - recall_17: 1.5285e- 58/254 [=====>........................] - ETA: 40s - loss: 0.6770 - acc: 0.7809 - precision_17: 6.8162e-04 - auc_17: 0.7094 - recall_17: 1.5045e- 59/254 [=====>........................] - ETA: 40s - loss: 0.6718 - acc: 0.7815 - precision_17: 6.8162e-04 - auc_17: 0.7114 - recall_17: 1.4828e- 60/254 [======>.......................] - ETA: 40s - loss: 0.6676 - acc: 0.7810 - precision_17: 6.8162e-04 - auc_17: 0.7146 - recall_17: 1.4547e- 61/254 [======>.......................] - ETA: 39s - loss: 0.6686 - acc: 0.7771 - precision_17: 6.8162e-04 - auc_17: 0.7142 - recall_17: 1.4054e- 62/254 [======>.......................] - ETA: 39s - loss: 0.6649 - acc: 0.7768 - precision_17: 6.8162e-04 - auc_17: 0.7160 - recall_17: 1.3812e- 63/254 [======>.......................] - ETA: 39s - loss: 0.6613 - acc: 0.7766 - precision_17: 6.8162e-04 - auc_17: 0.7176 - recall_17: 1.3578e- 64/254 [======>.......................] - ETA: 39s - loss: 0.6573 - acc: 0.7771 - precision_17: 6.8162e-04 - auc_17: 0.7185 - recall_17: 1.3396e- 65/254 [======>.......................] - ETA: 39s - loss: 0.6581 - acc: 0.7728 - precision_17: 6.8162e-04 - auc_17: 0.7219 - recall_17: 1.2939e- 66/254 [======>.......................] - ETA: 38s - loss: 0.6536 - acc: 0.7737 - precision_17: 6.8162e-04 - auc_17: 0.7229 - recall_17: 1.2796e- 67/254 [======>.......................] - ETA: 38s - loss: 0.6499 - acc: 0.7743 - precision_17: 6.8162e-04 - auc_17: 0.7235 - recall_17: 1.2634e- 68/254 [=======>......................] - ETA: 38s - loss: 0.6457 - acc: 0.7761 - precision_17: 6.8162e-04 - auc_17: 0.7221 - recall_17: 1.2549e- 69/254 [=======>......................] - ETA: 38s - loss: 0.6436 - acc: 0.7755 - precision_17: 6.8162e-04 - auc_17: 0.7222 - recall_17: 1.2337e- 70/254 [=======>......................] - ETA: 38s - loss: 0.6396 - acc: 0.7766 - precision_17: 6.8162e-04 - auc_17: 0.7227 - recall_17: 1.2219e- 71/254 [=======>......................] - ETA: 37s - loss: 0.6354 - acc: 0.7779 - precision_17: 6.8162e-04 - auc_17: 0.7236 - recall_17: 1.2118e- 72/254 [=======>......................] - ETA: 37s - loss: 0.6313 - acc: 0.7790 - precision_17: 6.8162e-04 - auc_17: 0.7253 - recall_17: 1.2006e- 73/254 [=======>......................] - ETA: 37s - loss: 0.6279 - acc: 0.7786 - precision_17: 6.8162e-04 - auc_17: 0.7286 - recall_17: 1.1823e- 74/254 [=======>......................] - ETA: 37s - loss: 0.6236 - acc: 0.7799 - precision_17: 6.8162e-04 - auc_17: 0.7293 - recall_17: 1.1731e- 75/254 [=======>......................] - ETA: 36s - loss: 0.6248 - acc: 0.7788 - precision_17: 6.8162e-04 - auc_17: 0.7250 - recall_17: 1.1516e- 76/254 [=======>......................] - ETA: 36s - loss: 0.6205 - acc: 0.7798 - precision_17: 6.8162e-04 - auc_17: 0.7277 - recall_17: 1.1414e- 77/254 [========>.....................] - ETA: 36s - loss: 0.6196 - acc: 0.7786 - precision_17: 6.8162e-04 - auc_17: 0.7283 - recall_17: 1.1205e- 78/254 [========>.....................] - ETA: 36s - loss: 0.6183 - acc: 0.7766 - precision_17: 6.8162e-04 - auc_17: 0.7317 - recall_17: 1.0963e- 79/254 [========>.....................] - ETA: 36s - loss: 0.6155 - acc: 0.7769 - precision_17: 6.8162e-04 - auc_17: 0.7331 - recall_17: 1.0839e- 80/254 [========>.....................] - ETA: 35s - loss: 0.6141 - acc: 0.7762 - precision_17: 6.8162e-04 - auc_17: 0.7338 - recall_17: 1.0670e- 81/254 [========>.....................] - ETA: 35s - loss: 0.6120 - acc: 0.7759 - precision_17: 6.8162e-04 - auc_17: 0.7348 - recall_17: 1.0523e- 82/254 [========>.....................] - ETA: 35s - loss: 0.6093 - acc: 0.7764 - precision_17: 6.8162e-04 - auc_17: 0.7358 - recall_17: 1.0418e- 83/254 [========>.....................] - ETA: 35s - loss: 0.6062 - acc: 0.7774 - precision_17: 6.8162e-04 - auc_17: 0.7364 - recall_17: 1.0338e- 84/254 [========>.....................] - ETA: 34s - loss: 0.6031 - acc: 0.7779 - precision_17: 6.8162e-04 - auc_17: 0.7383 - recall_17: 1.0238e- 85/254 [=========>....................] - ETA: 34s - loss: 0.6019 - acc: 0.7772 - precision_17: 6.8162e-04 - auc_17: 0.7391 - recall_17: 1.0085e- 86/254 [=========>....................] - ETA: 34s - loss: 0.5981 - acc: 0.7788 - precision_17: 6.8162e-04 - auc_17: 0.7405 - recall_17: 1.0042e- 87/254 [=========>....................] - ETA: 34s - loss: 0.5945 - acc: 0.7797 - precision_17: 6.8162e-04 - auc_17: 0.7428 - recall_17: 9.9671e- 88/254 [=========>....................] - ETA: 34s - loss: 0.5935 - acc: 0.7791 - precision_17: 6.8162e-04 - auc_17: 0.7424 - recall_17: 9.8270e- 89/254 [=========>....................] - ETA: 33s - loss: 0.5911 - acc: 0.7791 - precision_17: 6.8162e-04 - auc_17: 0.7434 - recall_17: 9.7174e- 90/254 [=========>....................] - ETA: 33s - loss: 0.5890 - acc: 0.7798 - precision_17: 6.8162e-04 - auc_17: 0.7447 - recall_17: 9.6376e- 91/254 [=========>....................] - ETA: 33s - loss: 0.5861 - acc: 0.7807 - precision_17: 6.8162e-04 - auc_17: 0.7456 - recall_17: 9.5715e- 92/254 [=========>....................] - ETA: 33s - loss: 0.5847 - acc: 0.7803 - precision_17: 6.8162e-04 - auc_ 93/254 [=========>....................] - ETA: 33s - loss: 0.5819 - acc: 0.7809 - precision_17: 6.8162e-04 - auc_ 94/254 [==========>...................] - ETA: 32s - loss: 0.5809 - acc: 0.7808 - precision_17: 6.8162e-04 - auc_ 95/254 [==========>...................] - ETA: 32s - loss: 0.5782 - acc: 0.7815 - precision_17: 6.8162e-04 - auc_ 96/254 [==========>...................] - ETA: 32s - loss: 0.5781 - acc: 0.7798 - precision_17: 6.8162e-04 - auc_ 97/254 [==========>...................] - ETA: 32s - loss: 0.5754 - acc: 0.7805 - precision_17: 6.8162e-04 - auc_ 98/254 [==========>...................] - ETA: 32s - loss: 0.5731 - acc: 0.7810 - precision_17: 6.8162e-04 - auc_ 99/254 [==========>...................] - ETA: 31s - loss: 0.5713 - acc: 0.7808 - precision_17: 6.8162e-04 - auc_100/254 [==========>...................] - ETA: 31s - loss: 0.5701 - acc: 0.7801 - precision_17: 6.8162e-04 - auc_101/254 [==========>...................] - ETA: 31s - loss: 0.5687 - acc: 0.7800 - precision_17: 6.8162e-04 - auc_102/254 [===========>..................] - ETA: 31s - loss: 0.5666 - acc: 0.7801 - precision_17: 6.8162e-04 - auc_103/254 [===========>..................] - ETA: 31s - loss: 0.5649 - acc: 0.7802 - precision_17: 6.8162e-04 - auc_104/254 [===========>..................] - ETA: 30s - loss: 0.5629 - acc: 0.7803 - precision_17: 7.4973e-04 - auc_105/254 [===========>..................] - ETA: 30s - loss: 0.5608 - acc: 0.7802 - precision_17: 8.1783e-04 - auc_106/254 [===========>..................] - ETA: 30s - loss: 0.5587 - acc: 0.7809 - precision_17: 8.1783e-04 - auc_107/254 [===========>..................] - ETA: 30s - loss: 0.5572 - acc: 0.7805 - precision_17: 9.5400e-04 - auc_108/254 [===========>..................] - ETA: 29s - loss: 0.5565 - acc: 0.7799 - precision_17: 0.0021 - auc_17: 109/254 [===========>..................] - ETA: 29s - loss: 0.5537 - acc: 0.7810 - precision_17: 0.0074 - auc_17: 110/254 [===========>..................] - ETA: 29s - loss: 0.5527 - acc: 0.7809 - precision_17: 0.0656 - auc_17: 111/254 [============>.................] - ETA: 29s - loss: 0.5526 - acc: 0.7803 - precision_17: 0.0657 - auc_17: 112/254 [============>.................] - ETA: 29s - loss: 0.5511 - acc: 0.7803 - precision_17: 0.1181 - auc_17: 254/254 [==============================] - 57s 224ms/step - loss: 0.4388 - acc: 0.8152 - precision_17: 0.7803 - auc_17: 0.8324 - recall_17: 0.2087 - val_loss: 0.3367 - val_acc: 0.8641 - val_precision_17: 0.6696 - val_auc_17: 0.8939 - val_recall_17: 0.7113
Epoch 2/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2997 - acc: 0.8741 - precision_17: 0.7938 - auc_17: 0.8977 - recall_17: 0.5774 - val_loss: 0.3029 - val_acc: 0.8826 - val_precision_17: 0.8777 - val_auc_17: 0.8950 - val_recall_17: 0.5183
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3039 - acc: 0.8738 - precision_17: 0.8054 - auc_17: 0.8954 - recall_17: 0.5596 - val_loss: 0.3220 - val_acc: 0.8692 - val_precision_17: 0.7096 - val_auc_17: 0.8921 - val_recall_17: 0.6581
Epoch 4/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2935 - acc: 0.8773 - precision_17: 0.8112 - auc_17: 0.9005 - recall_17: 0.5771 - val_loss: 0.2923 - val_acc: 0.8860 - val_precision_17: 0.7820 - val_auc_17: 0.8988 - val_recall_17: 0.6380
Epoch 5/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2937 - acc: 0.8792 - precision_17: 0.8213 - auc_17: 0.9004 - recall_17: 0.5786 - val_loss: 0.2911 - val_acc: 0.8872 - val_precision_17: 0.8045 - val_auc_17: 0.8951 - val_recall_17: 0.5975
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2825 - acc: 0.8840 - precision_17: 0.8292 - auc_17: 0.9075 - recall_17: 0.5958 - val_loss: 0.2751 - val_acc: 0.8952 - val_precision_17: 0.8062 - val_auc_17: 0.9142 - val_recall_17: 0.6801
Epoch 7/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2750 - acc: 0.8906 - precision_17: 0.8332 - auc_17: 0.9116 - recall_17: 0.6332 - val_loss: 0.2437 - val_acc: 0.9032 - val_precision_17: 0.7850 - val_auc_17: 0.9352 - val_recall_17: 0.7408
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2226 - acc: 0.9097 - precision_17: 0.8485 - auc_17: 0.9453 - recall_17: 0.7295 - val_loss: 0.2183 - val_acc: 0.9205 - val_precision_17: 0.9034 - val_auc_17: 0.9480 - val_recall_17: 0.7141
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2016 - acc: 0.9167 - precision_17: 0.8591 - auc_17: 0.9562 - recall_17: 0.7573 - val_loss: 0.1960 - val_acc: 0.9267 - val_precision_17: 0.9405 - val_auc_17: 0.9652 - val_recall_17: 0.6936
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1785 - acc: 0.9255 - precision_17: 0.8755 - auc_17: 0.9663 - recall_17: 0.7844 - val_loss: 0.1890 - val_acc: 0.9320 - val_precision_17: 0.8636 - val_auc_17: 0.9571 - val_recall_17: 0.8095
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1749 - acc: 0.9257 - precision_17: 0.8787 - auc_17: 0.9678 - recall_17: 0.7844 - val_loss: 0.1666 - val_acc: 0.9326 - val_precision_17: 0.8788 - val_auc_17: 0.9714 - val_recall_17: 0.7898
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1600 - acc: 0.9327 - precision_17: 0.8890 - auc_17: 0.9732 - recall_17: 0.8086 - val_loss: 0.1834 - val_acc: 0.9248 - val_precision_17: 0.7965 - val_auc_17: 0.9688 - val_recall_17: 0.8644
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1645 - acc: 0.9310 - precision_17: 0.8856 - auc_17: 0.9722 - recall_17: 0.8032 - val_loss: 0.1523 - val_acc: 0.9382 - val_precision_17: 0.8301 - val_auc_17: 0.9804 - val_recall_17: 0.8950
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1578 - acc: 0.9332 - precision_17: 0.8892 - auc_17: 0.9739 - recall_17: 0.8109 - val_loss: 0.1428 - val_acc: 0.9429 - val_precision_17: 0.8591 - val_auc_17: 0.9783 - val_recall_17: 0.8653
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1540 - acc: 0.9350 - precision_17: 0.8938 - auc_17: 0.9755 - recall_17: 0.8137 - val_loss: 0.1560 - val_acc: 0.9340 - val_precision_17: 0.8227 - val_auc_17: 0.9797 - val_recall_17: 0.8972
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1499 - acc: 0.9354 - precision_17: 0.8913 - auc_17: 0.9770 - recall_17: 0.8199 - val_loss: 0.1326 - val_acc: 0.9473 - val_precision_17: 0.9303 - val_auc_17: 0.9816 - val_recall_17: 0.7934
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1538 - acc: 0.9344 - precision_17: 0.8917 - auc_17: 0.9756 - recall_17: 0.8146 - val_loss: 0.1422 - val_acc: 0.9422 - val_precision_17: 0.8955 - val_auc_17: 0.9795 - val_recall_17: 0.8345
Epoch 18/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1495 - acc: 0.9368 - precision_17: 0.8995 - auc_17: 0.9766 - recall_17: 0.8178 - val_loss: 0.1252 - val_acc: 0.9528 - val_precision_17: 0.9269 - val_auc_17: 0.9838 - val_recall_17: 0.8380
Epoch 19/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1439 - acc: 0.9384 - precision_17: 0.8984 - auc_17: 0.9788 - recall_17: 0.8265 - val_loss: 0.1335 - val_acc: 0.9463 - val_precision_17: 0.9117 - val_auc_17: 0.9824 - val_recall_17: 0.8373
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1452 - acc: 0.9378 - precision_17: 0.8975 - auc_17: 0.9786 - recall_17: 0.8261 - val_loss: 0.1356 - val_acc: 0.9421 - val_precision_17: 0.8184 - val_auc_17: 0.9825 - val_recall_17: 0.8963
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1461 - acc: 0.9375 - precision_17: 0.8966 - auc_17: 0.9781 - recall_17: 0.8251 - val_loss: 0.1520 - val_acc: 0.9412 - val_precision_17: 0.8461 - val_auc_17: 0.9848 - val_recall_17: 0.9131
Epoch 22/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1392 - acc: 0.9405 - precision_17: 0.9044 - auc_17: 0.9799 - recall_17: 0.8323 - val_loss: 0.1557 - val_acc: 0.9377 - val_precision_17: 0.8221 - val_auc_17: 0.9801 - val_recall_17: 0.8849
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1408 - acc: 0.9398 - precision_17: 0.9018 - auc_17: 0.9794 - recall_17: 0.8306 - val_loss: 0.1607 - val_acc: 0.9310 - val_precision_17: 0.7971 - val_auc_17: 0.9810 - val_recall_17: 0.9035
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1401 - acc: 0.9394 - precision_17: 0.9015 - auc_17: 0.9800 - recall_17: 0.8288 - val_loss: 0.1337 - val_acc: 0.9424 - val_precision_17: 0.8408 - val_auc_17: 0.9840 - val_recall_17: 0.9043
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1430 - acc: 0.9393 - precision_17: 0.9009 - auc_17: 0.9792 - recall_17: 0.8297 - val_loss: 0.1466 - val_acc: 0.9427 - val_precision_17: 0.8492 - val_auc_17: 0.9802 - val_recall_17: 0.8806
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1383 - acc: 0.9403 - precision_17: 0.9064 - auc_17: 0.9802 - recall_17: 0.8276 - val_loss: 0.1305 - val_acc: 0.9457 - val_precision_17: 0.8762 - val_auc_17: 0.9823 - val_recall_17: 0.8644
Epoch 27/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1373 - acc: 0.9414 - precision_17: 0.9074 - auc_17: 0.9803 - recall_17: 0.8338 - val_loss: 0.1289 - val_acc: 0.9515 - val_precision_17: 0.9115 - val_auc_17: 0.9850 - val_recall_17: 0.8549
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1356 - acc: 0.9413 - precision_17: 0.9094 - auc_17: 0.9813 - recall_17: 0.8299 - val_loss: 0.2088 - val_acc: 0.9192 - val_precision_17: 0.7494 - val_auc_17: 0.9800 - val_recall_17: 0.9312
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1354 - acc: 0.9411 - precision_17: 0.9042 - auc_17: 0.9811 - recall_17: 0.8363 - val_loss: 0.1827 - val_acc: 0.9197 - val_precision_17: 0.7634 - val_auc_17: 0.9826 - val_recall_17: 0.9336
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1330 - acc: 0.9421 - precision_17: 0.9049 - auc_17: 0.9817 - recall_17: 0.8412 - val_loss: 0.1203 - val_acc: 0.9513 - val_precision_17: 0.9033 - val_auc_17: 0.9856 - val_recall_17: 0.8626
Epoch 31/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1299 - acc: 0.9431 - precision_17: 0.9107 - auc_17: 0.9829 - recall_17: 0.8385 - val_loss: 0.1565 - val_acc: 0.9395 - val_precision_17: 0.8181 - val_auc_17: 0.9836 - val_recall_17: 0.9148
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1320 - acc: 0.9428 - precision_17: 0.9084 - auc_17: 0.9822 - recall_17: 0.8406 - val_loss: 0.1285 - val_acc: 0.9483 - val_precision_17: 0.8646 - val_auc_17: 0.9859 - val_recall_17: 0.8989
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1419 - acc: 0.9405 - precision_17: 0.9048 - auc_17: 0.9783 - recall_17: 0.8309 - val_loss: 0.1265 - val_acc: 0.9466 - val_precision_17: 0.8567 - val_auc_17: 0.9855 - val_recall_17: 0.8965
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1364 - acc: 0.9411 - precision_17: 0.9056 - auc_17: 0.9809 - recall_17: 0.8331 - val_loss: 0.1290 - val_acc: 0.9472 - val_precision_17: 0.8521 - val_auc_17: 0.9837 - val_recall_17: 0.8839
Epoch 35/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1309 - acc: 0.9437 - precision_17: 0.9110 - auc_17: 0.9823 - recall_17: 0.8406 - val_loss: 0.1307 - val_acc: 0.9456 - val_precision_17: 0.8613 - val_auc_17: 0.9854 - val_recall_17: 0.9036
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1295 - acc: 0.9444 - precision_17: 0.9129 - auc_17: 0.9825 - recall_17: 0.8417 - val_loss: 0.1293 - val_acc: 0.9466 - val_precision_17: 0.8647 - val_auc_17: 0.9838 - val_recall_17: 0.8879
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1316 - acc: 0.9428 - precision_17: 0.9082 - auc_17: 0.9822 - recall_17: 0.8394 - val_loss: 0.1579 - val_acc: 0.9471 - val_precision_17: 0.9482 - val_auc_17: 0.9820 - val_recall_17: 0.7983
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1301 - acc: 0.9437 - precision_17: 0.9110 - auc_17: 0.9826 - recall_17: 0.8417 - val_loss: 0.1257 - val_acc: 0.9521 - val_precision_17: 0.9064 - val_auc_17: 0.9835 - val_recall_17: 0.8606
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1297 - acc: 0.9441 - precision_17: 0.9126 - auc_17: 0.9826 - recall_17: 0.8411 - val_loss: 0.1272 - val_acc: 0.9489 - val_precision_17: 0.8930 - val_auc_17: 0.9849 - val_recall_17: 0.8710
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1282 - acc: 0.9442 - precision_17: 0.9116 - auc_17: 0.9836 - recall_17: 0.8442 - val_loss: 0.1192 - val_acc: 0.9549 - val_precision_17: 0.9104 - val_auc_17: 0.9857 - val_recall_17: 0.8573
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1301 - acc: 0.9439 - precision_17: 0.9102 - auc_17: 0.9822 - recall_17: 0.8432 - val_loss: 0.1370 - val_acc: 0.9433 - val_precision_17: 0.8642 - val_auc_17: 0.9834 - val_recall_17: 0.8796
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1260 - acc: 0.9452 - precision_17: 0.9154 - auc_17: 0.9836 - recall_17: 0.8453 - val_loss: 0.1305 - val_acc: 0.9479 - val_precision_17: 0.8810 - val_auc_17: 0.9842 - val_recall_17: 0.8696
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1245 - acc: 0.9449 - precision_17: 0.9119 - auc_17: 0.9845 - recall_17: 0.8483 - val_loss: 0.1376 - val_acc: 0.9460 - val_precision_17: 0.8928 - val_auc_17: 0.9821 - val_recall_17: 0.8541
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1246 - acc: 0.9456 - precision_17: 0.9135 - auc_17: 0.9843 - recall_17: 0.8481 - val_loss: 0.1203 - val_acc: 0.9520 - val_precision_17: 0.9233 - val_auc_17: 0.9843 - val_recall_17: 0.8442
Epoch 45/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1266 - acc: 0.9447 - precision_17: 0.9141 - auc_17: 0.9835 - recall_17: 0.8427 - val_loss: 0.1294 - val_acc: 0.9458 - val_precision_17: 0.8629 - val_auc_17: 0.9844 - val_recall_17: 0.8882
Epoch 46/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1237 - acc: 0.9453 - precision_17: 0.9121 - auc_17: 0.9842 - recall_17: 0.8481 - val_loss: 0.1245 - val_acc: 0.9494 - val_precision_17: 0.8748 - val_auc_17: 0.9856 - val_recall_17: 0.8863
Epoch 47/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1276 - acc: 0.9442 - precision_17: 0.9111 - auc_17: 0.9834 - recall_17: 0.8434 - val_loss: 0.1183 - val_acc: 0.9524 - val_precision_17: 0.9096 - val_auc_17: 0.9845 - val_recall_17: 0.8640
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1237 - acc: 0.9464 - precision_17: 0.9168 - auc_17: 0.9841 - recall_17: 0.8491 - val_loss: 0.1194 - val_acc: 0.9533 - val_precision_17: 0.9447 - val_auc_17: 0.9852 - val_recall_17: 0.8255
Epoch 49/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1268 - acc: 0.9447 - precision_17: 0.9166 - auc_17: 0.9836 - recall_17: 0.8394 - val_loss: 0.1443 - val_acc: 0.9505 - val_precision_17: 0.8867 - val_auc_17: 0.9851 - val_recall_17: 0.8790
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1277 - acc: 0.9443 - precision_17: 0.9110 - auc_17: 0.9831 - recall_17: 0.8440 - val_loss: 0.1208 - val_acc: 0.9497 - val_precision_17: 0.8713 - val_auc_17: 0.9847 - val_recall_17: 0.8964
Epoch 51/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1283 - acc: 0.9442 - precision_17: 0.9117 - auc_17: 0.9828 - recall_17: 0.8437 - val_loss: 0.1253 - val_acc: 0.9505 - val_precision_17: 0.8983 - val_auc_17: 0.9840 - val_recall_17: 0.8618
Epoch 52/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1258 - acc: 0.9453 - precision_17: 0.9104 - auc_17: 0.9837 - recall_17: 0.8504 - val_loss: 0.1814 - val_acc: 0.9280 - val_precision_17: 0.7846 - val_auc_17: 0.9808 - val_recall_17: 0.9138
Epoch 53/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1203 - acc: 0.9471 - precision_17: 0.9151 - auc_17: 0.9851 - recall_17: 0.8538 - val_loss: 0.1248 - val_acc: 0.9493 - val_precision_17: 0.8837 - val_auc_17: 0.9837 - val_recall_17: 0.8750
Epoch 54/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1231 - acc: 0.9461 - precision_17: 0.9157 - auc_17: 0.9845 - recall_17: 0.8495 - val_loss: 0.1285 - val_acc: 0.9473 - val_precision_17: 0.8745 - val_auc_17: 0.9839 - val_recall_17: 0.8794
Epoch 55/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1217 - acc: 0.9469 - precision_17: 0.9172 - auc_17: 0.9845 - recall_17: 0.8503 - val_loss: 0.1090 - val_acc: 0.9563 - val_precision_17: 0.9132 - val_auc_17: 0.9875 - val_recall_17: 0.8742
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1184 - acc: 0.9474 - precision_17: 0.9183 - auc_17: 0.9855 - recall_17: 0.8512 - val_loss: 0.1340 - val_acc: 0.9450 - val_precision_17: 0.8471 - val_auc_17: 0.9842 - val_recall_17: 0.9113
Epoch 57/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1216 - acc: 0.9464 - precision_17: 0.9159 - auc_17: 0.9847 - recall_17: 0.8496 - val_loss: 0.1319 - val_acc: 0.9449 - val_precision_17: 0.8372 - val_auc_17: 0.9860 - val_recall_17: 0.9147
Epoch 58/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1197 - acc: 0.9470 - precision_17: 0.9159 - auc_17: 0.9854 - recall_17: 0.8536 - val_loss: 0.1309 - val_acc: 0.9493 - val_precision_17: 0.9391 - val_auc_17: 0.9822 - val_recall_17: 0.8044
Epoch 59/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1196 - acc: 0.9472 - precision_17: 0.9179 - auc_17: 0.9853 - recall_17: 0.8510 - val_loss: 0.1598 - val_acc: 0.9307 - val_precision_17: 0.8072 - val_auc_17: 0.9812 - val_recall_17: 0.9060
Epoch 60/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1207 - acc: 0.9469 - precision_17: 0.9166 - auc_17: 0.9851 - recall_17: 0.8514 - val_loss: 0.1179 - val_acc: 0.9522 - val_precision_17: 0.8969 - val_auc_17: 0.9866 - val_recall_17: 0.8762
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1186 - acc: 0.9481 - precision_17: 0.9166 - auc_17: 0.9856 - recall_17: 0.8579 - val_loss: 0.1194 - val_acc: 0.9541 - val_precision_17: 0.8998 - val_auc_17: 0.9853 - val_recall_17: 0.8824
Epoch 62/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1246 - acc: 0.9462 - precision_17: 0.9145 - auc_17: 0.9844 - recall_17: 0.8503 - val_loss: 0.1163 - val_acc: 0.9516 - val_precision_17: 0.9023 - val_auc_17: 0.9857 - val_recall_17: 0.8621
Epoch 63/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1204 - acc: 0.9475 - precision_17: 0.9174 - auc_17: 0.9851 - recall_17: 0.8520 - val_loss: 0.1172 - val_acc: 0.9543 - val_precision_17: 0.9335 - val_auc_17: 0.9849 - val_recall_17: 0.8419
Epoch 64/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1180 - acc: 0.9481 - precision_17: 0.9174 - auc_17: 0.9855 - recall_17: 0.8558 - val_loss: 0.1502 - val_acc: 0.9451 - val_precision_17: 0.9600 - val_auc_17: 0.9797 - val_recall_17: 0.7834
Epoch 65/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1185 - acc: 0.9483 - precision_17: 0.9194 - auc_17: 0.9856 - recall_17: 0.8550 - val_loss: 0.1228 - val_acc: 0.9523 - val_precision_17: 0.9247 - val_auc_17: 0.9813 - val_recall_17: 0.8282
Epoch 66/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1169 - acc: 0.9486 - precision_17: 0.9194 - auc_17: 0.9859 - recall_17: 0.8576 - val_loss: 0.1178 - val_acc: 0.9522 - val_precision_17: 0.9419 - val_auc_17: 0.9867 - val_recall_17: 0.8370
Epoch 67/100
254/254 [==============================] - 55s 219ms/step - loss: 0.1185 - acc: 0.9475 - precision_17: 0.9161 - auc_17: 0.9856 - recall_17: 0.8547 - val_loss: 0.1133 - val_acc: 0.9539 - val_precision_17: 0.9020 - val_auc_17: 0.9855 - val_recall_17: 0.8702
Epoch 68/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1207 - acc: 0.9466 - precision_17: 0.9171 - auc_17: 0.9852 - recall_17: 0.8483 - val_loss: 0.1235 - val_acc: 0.9515 - val_precision_17: 0.8947 - val_auc_17: 0.9856 - val_recall_17: 0.8703
Epoch 69/100
254/254 [==============================] - ETA: 0s - loss: 0.1169 - acc: 0.9482 - precision_17: 0.9211 - auc_17: 0.9859 - recall_17: 0.8535Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1169 - acc: 0.9482 - precision_17: 0.9211 - auc_17: 0.9859 - recall_17: 0.8535 - val_loss: 0.1392 - val_acc: 0.9441 - val_precision_17: 0.8498 - val_auc_17: 0.9831 - val_recall_17: 0.8932
Epoch 00069: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1307 - acc: 0.9487 - precision_17: 0.8931 - auc_17: 0.9832 - recall_17: 0.8754
---------------TEST METRICS----------------------
jaccard_index 0.7988270489271152
test_sensitivity 0.8819808184709514
test_specifitivity 0.9675992433899592
test_accuracy 0.9484548365816157
test_precision 0.886872724697837
test_jaccard_score 0.7988270489271152
test_dicecoef 0.884420007105044
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-013122.h5
[1.88847162 1.56286937 1.75066273 1.75478637 1.92697275] [0.94845484 0.79882705 0.88687272 0.88198082 0.96759924]

-------------------------
Averaged metrics for Baseline + Augumentations + Histogram Equalization - lesion: [0.94564215 0.78723214 0.87917848 0.8789224  0.96485733]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Per Channel Normalization - lesion, PARAMS: {'augumentation': True, 'per_channel_normalization': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 01:31:23.835200: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 01:31:23.835315: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_37"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_19 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_342 (Conv2D)             (None, 256, 256, 32) 320         input_19[0][0]
__________________________________________________________________________________________________
conv2d_343 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_342[0][0]
__________________________________________________________________________________________________
max_pooling2d_72 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_343[0][0]
__________________________________________________________________________________________________
conv2d_344 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_72[0][0]
__________________________________________________________________________________________________
conv2d_345 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_344[0][0]
__________________________________________________________________________________________________
max_pooling2d_73 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_345[0][0]
__________________________________________________________________________________________________
conv2d_346 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_73[0][0]
__________________________________________________________________________________________________
conv2d_347 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_346[0][0]
__________________________________________________________________________________________________
max_pooling2d_74 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_347[0][0]
__________________________________________________________________________________________________
conv2d_348 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_74[0][0]
__________________________________________________________________________________________________
conv2d_349 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_348[0][0]
__________________________________________________________________________________________________
max_pooling2d_75 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_349[0][0]
__________________________________________________________________________________________________
conv2d_350 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_75[0][0]
__________________________________________________________________________________________________
conv2d_351 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_350[0][0]
__________________________________________________________________________________________________
up_sampling2d_72 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_351[0][0]
__________________________________________________________________________________________________
concatenate_72 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_72[0][0]
                                                                 conv2d_349[0][0]
__________________________________________________________________________________________________
conv2d_352 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_72[0][0]
__________________________________________________________________________________________________
conv2d_353 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_352[0][0]
__________________________________________________________________________________________________
up_sampling2d_73 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_353[0][0]
__________________________________________________________________________________________________
concatenate_73 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_73[0][0]
                                                                 conv2d_347[0][0]
__________________________________________________________________________________________________
conv2d_354 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_73[0][0]
__________________________________________________________________________________________________
conv2d_355 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_354[0][0]
__________________________________________________________________________________________________
up_sampling2d_74 (UpSampling2D) (None, 128, 128, 128 0           conv2d_355[0][0]
__________________________________________________________________________________________________
concatenate_74 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_74[0][0]
                                                                 conv2d_345[0][0]
__________________________________________________________________________________________________
conv2d_356 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_74[0][0]
__________________________________________________________________________________________________
conv2d_357 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_356[0][0]
__________________________________________________________________________________________________
up_sampling2d_75 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_357[0][0]
__________________________________________________________________________________________________
concatenate_75 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_75[0][0]
                                                                 conv2d_343[0][0]
__________________________________________________________________________________________________
conv2d_358 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_75[0][0]
__________________________________________________________________________________________________
conv2d_359 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_358[0][0]
__________________________________________________________________________________________________
conv2d_360 (Conv2D)             (None, 256, 256, 1)  33          conv2d_359[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6925 - acc: 0.7630 - precision_18: 0.0767 - auc_18: 0.4588 - recall_18: 0.03752021-09-25 01:31:32.840546: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 01:31:32.840663: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 01:31:33.373685: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 01:31:33.387086: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33
2021-09-25 01:31:33.390222: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.trace.json.gz
2021-09-25 01:31:33.417878: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33
2021-09-25 01:31:33.425846: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.memory_profile.json.gz
2021-09-25 01:31:33.438370: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33Dumped tool data for xplane.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-013123/train/plugins/profile/2021_09_25_01_31_33/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6923 - acc: 0.6986 - precision_18: 0.0767 - auc_18: 0.3100 - recall_18: 0.0117WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1994s vs `on_train_batch_end` time: 0.4000s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4260 - acc: 0.8350 - precision_18: 0.7082 - auc_18: 0.7986 - recall_18: 0.4130 - val_loss: 0.3998 - val_acc: 0.8185 - val_precision_18: 0.5755 - val_auc_18: 0.8362 - val_recall_18: 0.5544
Epoch 2/100
254/254 [==============================] - 56s 219ms/step - loss: 0.3578 - acc: 0.8479 - precision_18: 0.7301 - auc_18: 0.8572 - recall_18: 0.4846 - val_loss: 0.3080 - val_acc: 0.8750 - val_precision_18: 0.8082 - val_auc_18: 0.9095 - val_recall_18: 0.5382
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2596 - acc: 0.8931 - precision_18: 0.8247 - auc_18: 0.9262 - recall_18: 0.6608 - val_loss: 0.2155 - val_acc: 0.9237 - val_precision_18: 0.8328 - val_auc_18: 0.9524 - val_recall_18: 0.8054
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2293 - acc: 0.9069 - precision_18: 0.8433 - auc_18: 0.9442 - recall_18: 0.7203 - val_loss: 0.1996 - val_acc: 0.9216 - val_precision_18: 0.8402 - val_auc_18: 0.9556 - val_recall_18: 0.7764
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1988 - acc: 0.9189 - precision_18: 0.8689 - auc_18: 0.9574 - recall_18: 0.7579 - val_loss: 0.1800 - val_acc: 0.9302 - val_precision_18: 0.8164 - val_auc_18: 0.9694 - val_recall_18: 0.8526
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1857 - acc: 0.9225 - precision_18: 0.8715 - auc_18: 0.9633 - recall_18: 0.7736 - val_loss: 0.1750 - val_acc: 0.9349 - val_precision_18: 0.9012 - val_auc_18: 0.9736 - val_recall_18: 0.7859
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1824 - acc: 0.9237 - precision_18: 0.8719 - auc_18: 0.9645 - recall_18: 0.7801 - val_loss: 0.1768 - val_acc: 0.9305 - val_precision_18: 0.9005 - val_auc_18: 0.9652 - val_recall_18: 0.7508
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1734 - acc: 0.9274 - precision_18: 0.8782 - auc_18: 0.9678 - recall_18: 0.7918 - val_loss: 0.1720 - val_acc: 0.9339 - val_precision_18: 0.9345 - val_auc_18: 0.9711 - val_recall_18: 0.7510
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1710 - acc: 0.9285 - precision_18: 0.8808 - auc_18: 0.9688 - recall_18: 0.7961 - val_loss: 0.1767 - val_acc: 0.9368 - val_precision_18: 0.9411 - val_auc_18: 0.9731 - val_recall_18: 0.7444
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1700 - acc: 0.9290 - precision_18: 0.8848 - auc_18: 0.9690 - recall_18: 0.7919 - val_loss: 0.1843 - val_acc: 0.9344 - val_precision_18: 0.9168 - val_auc_18: 0.9599 - val_recall_18: 0.7619
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1664 - acc: 0.9296 - precision_18: 0.8841 - auc_18: 0.9711 - recall_18: 0.7987 - val_loss: 0.1572 - val_acc: 0.9397 - val_precision_18: 0.9319 - val_auc_18: 0.9759 - val_recall_18: 0.7710
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1759 - acc: 0.9267 - precision_18: 0.8797 - auc_18: 0.9670 - recall_18: 0.7866 - val_loss: 0.1491 - val_acc: 0.9351 - val_precision_18: 0.8171 - val_auc_18: 0.9796 - val_recall_18: 0.8921
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1645 - acc: 0.9310 - precision_18: 0.8875 - auc_18: 0.9713 - recall_18: 0.8010 - val_loss: 0.1588 - val_acc: 0.9373 - val_precision_18: 0.8181 - val_auc_18: 0.9813 - val_recall_18: 0.9104
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1615 - acc: 0.9315 - precision_18: 0.8884 - auc_18: 0.9722 - recall_18: 0.8025 - val_loss: 0.1476 - val_acc: 0.9414 - val_precision_18: 0.8955 - val_auc_18: 0.9756 - val_recall_18: 0.8103
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1535 - acc: 0.9342 - precision_18: 0.8938 - auc_18: 0.9756 - recall_18: 0.8099 - val_loss: 0.1754 - val_acc: 0.9386 - val_precision_18: 0.9097 - val_auc_18: 0.9640 - val_recall_18: 0.8041
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1533 - acc: 0.9345 - precision_18: 0.8949 - auc_18: 0.9758 - recall_18: 0.8109 - val_loss: 0.1382 - val_acc: 0.9462 - val_precision_18: 0.9152 - val_auc_18: 0.9784 - val_recall_18: 0.8026
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1549 - acc: 0.9345 - precision_18: 0.8987 - auc_18: 0.9750 - recall_18: 0.8067 - val_loss: 0.1700 - val_acc: 0.9344 - val_precision_18: 0.8376 - val_auc_18: 0.9770 - val_recall_18: 0.8702
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1559 - acc: 0.9340 - precision_18: 0.8934 - auc_18: 0.9745 - recall_18: 0.8102 - val_loss: 0.1308 - val_acc: 0.9490 - val_precision_18: 0.9017 - val_auc_18: 0.9809 - val_recall_18: 0.8457
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1528 - acc: 0.9347 - precision_18: 0.8933 - auc_18: 0.9755 - recall_18: 0.8130 - val_loss: 0.1449 - val_acc: 0.9417 - val_precision_18: 0.9215 - val_auc_18: 0.9797 - val_recall_18: 0.8038
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1552 - acc: 0.9335 - precision_18: 0.8930 - auc_18: 0.9747 - recall_18: 0.8084 - val_loss: 0.1312 - val_acc: 0.9472 - val_precision_18: 0.8607 - val_auc_18: 0.9791 - val_recall_18: 0.8640
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1563 - acc: 0.9338 - precision_18: 0.8909 - auc_18: 0.9751 - recall_18: 0.8121 - val_loss: 0.1525 - val_acc: 0.9421 - val_precision_18: 0.9063 - val_auc_18: 0.9806 - val_recall_18: 0.8377
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1492 - acc: 0.9362 - precision_18: 0.8997 - auc_18: 0.9767 - recall_18: 0.8142 - val_loss: 0.1465 - val_acc: 0.9420 - val_precision_18: 0.8542 - val_auc_18: 0.9789 - val_recall_18: 0.8615
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1493 - acc: 0.9368 - precision_18: 0.9011 - auc_18: 0.9763 - recall_18: 0.8156 - val_loss: 0.1364 - val_acc: 0.9467 - val_precision_18: 0.8881 - val_auc_18: 0.9805 - val_recall_18: 0.8556
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1507 - acc: 0.9359 - precision_18: 0.8992 - auc_18: 0.9763 - recall_18: 0.8130 - val_loss: 0.1411 - val_acc: 0.9435 - val_precision_18: 0.8603 - val_auc_18: 0.9806 - val_recall_18: 0.8818
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1451 - acc: 0.9380 - precision_18: 0.9002 - auc_18: 0.9783 - recall_18: 0.8238 - val_loss: 0.1445 - val_acc: 0.9429 - val_precision_18: 0.8287 - val_auc_18: 0.9847 - val_recall_18: 0.9140
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1430 - acc: 0.9385 - precision_18: 0.9026 - auc_18: 0.9790 - recall_18: 0.8223 - val_loss: 0.1360 - val_acc: 0.9453 - val_precision_18: 0.8793 - val_auc_18: 0.9798 - val_recall_18: 0.8580
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1468 - acc: 0.9378 - precision_18: 0.9030 - auc_18: 0.9773 - recall_18: 0.8196 - val_loss: 0.1262 - val_acc: 0.9510 - val_precision_18: 0.9196 - val_auc_18: 0.9843 - val_recall_18: 0.8435
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1407 - acc: 0.9398 - precision_18: 0.9066 - auc_18: 0.9797 - recall_18: 0.8256 - val_loss: 0.1633 - val_acc: 0.9351 - val_precision_18: 0.8046 - val_auc_18: 0.9806 - val_recall_18: 0.9174
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1424 - acc: 0.9382 - precision_18: 0.8990 - auc_18: 0.9789 - recall_18: 0.8268 - val_loss: 0.1804 - val_acc: 0.9223 - val_precision_18: 0.7755 - val_auc_18: 0.9793 - val_recall_18: 0.9232
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1420 - acc: 0.9386 - precision_18: 0.8989 - auc_18: 0.9792 - recall_18: 0.8293 - val_loss: 0.1213 - val_acc: 0.9512 - val_precision_18: 0.9403 - val_auc_18: 0.9856 - val_recall_18: 0.8215
Epoch 31/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1418 - acc: 0.9391 - precision_18: 0.9059 - auc_18: 0.9790 - recall_18: 0.8233 - val_loss: 0.1384 - val_acc: 0.9481 - val_precision_18: 0.8717 - val_auc_18: 0.9797 - val_recall_18: 0.8825
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1382 - acc: 0.9406 - precision_18: 0.9063 - auc_18: 0.9800 - recall_18: 0.8310 - val_loss: 0.1343 - val_acc: 0.9495 - val_precision_18: 0.8808 - val_auc_18: 0.9855 - val_recall_18: 0.8832
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1372 - acc: 0.9407 - precision_18: 0.9032 - auc_18: 0.9806 - recall_18: 0.8338 - val_loss: 0.1276 - val_acc: 0.9487 - val_precision_18: 0.8753 - val_auc_18: 0.9841 - val_recall_18: 0.8824
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1454 - acc: 0.9379 - precision_18: 0.9006 - auc_18: 0.9780 - recall_18: 0.8221 - val_loss: 0.1284 - val_acc: 0.9500 - val_precision_18: 0.8730 - val_auc_18: 0.9830 - val_recall_18: 0.8715
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1342 - acc: 0.9418 - precision_18: 0.9063 - auc_18: 0.9812 - recall_18: 0.8362 - val_loss: 0.1320 - val_acc: 0.9480 - val_precision_18: 0.8879 - val_auc_18: 0.9838 - val_recall_18: 0.8803
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1360 - acc: 0.9418 - precision_18: 0.9088 - auc_18: 0.9806 - recall_18: 0.8327 - val_loss: 0.1290 - val_acc: 0.9486 - val_precision_18: 0.8902 - val_auc_18: 0.9819 - val_recall_18: 0.8650
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1348 - acc: 0.9417 - precision_18: 0.9072 - auc_18: 0.9812 - recall_18: 0.8347 - val_loss: 0.1425 - val_acc: 0.9456 - val_precision_18: 0.9679 - val_auc_18: 0.9842 - val_recall_18: 0.7737
Epoch 38/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1322 - acc: 0.9426 - precision_18: 0.9069 - auc_18: 0.9818 - recall_18: 0.8412 - val_loss: 0.1276 - val_acc: 0.9514 - val_precision_18: 0.9159 - val_auc_18: 0.9833 - val_recall_18: 0.8461
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1327 - acc: 0.9426 - precision_18: 0.9115 - auc_18: 0.9817 - recall_18: 0.8340 - val_loss: 0.1405 - val_acc: 0.9447 - val_precision_18: 0.9088 - val_auc_18: 0.9814 - val_recall_18: 0.8311
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1328 - acc: 0.9424 - precision_18: 0.9067 - auc_18: 0.9819 - recall_18: 0.8406 - val_loss: 0.1334 - val_acc: 0.9498 - val_precision_18: 0.9512 - val_auc_18: 0.9819 - val_recall_18: 0.7877
Epoch 41/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1317 - acc: 0.9431 - precision_18: 0.9091 - auc_18: 0.9819 - recall_18: 0.8403 - val_loss: 0.1396 - val_acc: 0.9433 - val_precision_18: 0.8786 - val_auc_18: 0.9796 - val_recall_18: 0.8599
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1390 - acc: 0.9397 - precision_18: 0.9065 - auc_18: 0.9802 - recall_18: 0.8268 - val_loss: 0.1293 - val_acc: 0.9500 - val_precision_18: 0.8995 - val_auc_18: 0.9837 - val_recall_18: 0.8579
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1333 - acc: 0.9413 - precision_18: 0.9075 - auc_18: 0.9820 - recall_18: 0.8342 - val_loss: 0.1373 - val_acc: 0.9467 - val_precision_18: 0.8870 - val_auc_18: 0.9813 - val_recall_18: 0.8647
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1302 - acc: 0.9439 - precision_18: 0.9096 - auc_18: 0.9827 - recall_18: 0.8435Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1302 - acc: 0.9439 - precision_18: 0.9096 - auc_18: 0.9827 - recall_18: 0.8435 - val_loss: 0.1225 - val_acc: 0.9514 - val_precision_18: 0.9258 - val_auc_18: 0.9827 - val_recall_18: 0.8381
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1347 - acc: 0.9476 - precision_18: 0.9290 - auc_18: 0.9822 - recall_18: 0.8288
---------------TEST METRICS----------------------
jaccard_index 0.7829513311599599
test_sensitivity 0.840265239703776
test_specifitivity 0.975969874425899
test_accuracy 0.9456261168134973
test_precision 0.9096697121524707
test_jaccard_score 0.7829513311599599
test_dicecoef 0.8735911445419847
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-021309.h5
[0. 0. 0. 0. 0.] [0.94562612 0.78295133 0.90966971 0.84026524 0.97596987]

-------------------------
Rep: 1
-------------------------

2021-09-25 02:13:10.651657: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 02:13:10.651782: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_39"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_20 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_361 (Conv2D)             (None, 256, 256, 32) 320         input_20[0][0]
__________________________________________________________________________________________________
conv2d_362 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_361[0][0]
__________________________________________________________________________________________________
max_pooling2d_76 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_363 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_76[0][0]
__________________________________________________________________________________________________
conv2d_364 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_363[0][0]
__________________________________________________________________________________________________
max_pooling2d_77 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_365 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_77[0][0]
__________________________________________________________________________________________________
conv2d_366 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_365[0][0]
__________________________________________________________________________________________________
max_pooling2d_78 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_367 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_78[0][0]
__________________________________________________________________________________________________
conv2d_368 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_367[0][0]
__________________________________________________________________________________________________
max_pooling2d_79 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_369 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_79[0][0]
__________________________________________________________________________________________________
conv2d_370 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_369[0][0]
__________________________________________________________________________________________________
up_sampling2d_76 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_370[0][0]
__________________________________________________________________________________________________
concatenate_76 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_76[0][0]
                                                                 conv2d_368[0][0]
__________________________________________________________________________________________________
conv2d_371 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_76[0][0]
__________________________________________________________________________________________________
conv2d_372 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_371[0][0]
__________________________________________________________________________________________________
up_sampling2d_77 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_372[0][0]
__________________________________________________________________________________________________
concatenate_77 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_77[0][0]
                                                                 conv2d_366[0][0]
__________________________________________________________________________________________________
conv2d_373 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_77[0][0]
__________________________________________________________________________________________________
conv2d_374 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_373[0][0]
__________________________________________________________________________________________________
up_sampling2d_78 (UpSampling2D) (None, 128, 128, 128 0           conv2d_374[0][0]
__________________________________________________________________________________________________
concatenate_78 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_78[0][0]
                                                                 conv2d_364[0][0]
__________________________________________________________________________________________________
conv2d_375 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_78[0][0]
__________________________________________________________________________________________________
conv2d_376 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_375[0][0]
__________________________________________________________________________________________________
up_sampling2d_79 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_376[0][0]
__________________________________________________________________________________________________
concatenate_79 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_79[0][0]
                                                                 conv2d_362[0][0]
__________________________________________________________________________________________________
conv2d_377 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_79[0][0]
__________________________________________________________________________________________________
conv2d_378 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_377[0][0]
__________________________________________________________________________________________________
conv2d_379 (Conv2D)             (None, 256, 256, 1)  33          conv2d_378[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6929 - acc: 0.7967 - precision_19: 0.1574 - auc_19: 0.5000 - recall_19: 0.04872021-09-25 02:13:19.698195: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 02:13:19.698306: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 02:13:20.205399: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 02:13:20.224579: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20
2021-09-25 02:13:20.229105: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.trace.json.gz
2021-09-25 02:13:20.256197: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20
2021-09-25 02:13:20.265111: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.memory_profile.json.gz
2021-09-25 02:13:20.284886: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20Dumped tool data for xplane.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-021310/train/plugins/profile/2021_09_25_02_13_20/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6913 - acc: 0.7154 - precision_19: 0.1599 - auc_19: 0.4029 - recall_19: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1946s vs `on_train_batch_end` time: 0.3937s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4237 - acc: 0.8356 - precision_19: 0.7004 - auc_19: 0.7863 - recall_19: 0.4290 - val_loss: 0.4165 - val_acc: 0.8483 - val_precision_19: 0.7955 - val_auc_19: 0.8087 - val_recall_19: 0.3847
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3876 - acc: 0.8411 - precision_19: 0.7092 - auc_19: 0.8230 - recall_19: 0.4617 - val_loss: 0.3782 - val_acc: 0.8467 - val_precision_19: 0.7071 - val_auc_19: 0.8299 - val_recall_19: 0.4727
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3490 - acc: 0.8620 - precision_19: 0.7745 - auc_19: 0.8505 - recall_19: 0.5213 - val_loss: 0.3266 - val_acc: 0.8822 - val_precision_19: 0.7170 - val_auc_19: 0.8898 - val_recall_19: 0.7424
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2738 - acc: 0.8943 - precision_19: 0.8412 - auc_19: 0.9116 - recall_19: 0.6455 - val_loss: 0.2154 - val_acc: 0.9205 - val_precision_19: 0.8800 - val_auc_19: 0.9457 - val_recall_19: 0.7221
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2428 - acc: 0.9027 - precision_19: 0.8557 - auc_19: 0.9330 - recall_19: 0.6800 - val_loss: 0.2108 - val_acc: 0.9232 - val_precision_19: 0.8640 - val_auc_19: 0.9505 - val_recall_19: 0.7439
Epoch 6/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2205 - acc: 0.9105 - precision_19: 0.8637 - auc_19: 0.9464 - recall_19: 0.7136 - val_loss: 0.2043 - val_acc: 0.9258 - val_precision_19: 0.9348 - val_auc_19: 0.9594 - val_recall_19: 0.7072
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2081 - acc: 0.9159 - precision_19: 0.8712 - auc_19: 0.9521 - recall_19: 0.7366 - val_loss: 0.1830 - val_acc: 0.9285 - val_precision_19: 0.9115 - val_auc_19: 0.9621 - val_recall_19: 0.7295
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1994 - acc: 0.9176 - precision_19: 0.8724 - auc_19: 0.9560 - recall_19: 0.7437 - val_loss: 0.2023 - val_acc: 0.9232 - val_precision_19: 0.8751 - val_auc_19: 0.9554 - val_recall_19: 0.7578
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1979 - acc: 0.9181 - precision_19: 0.8707 - auc_19: 0.9581 - recall_19: 0.7502 - val_loss: 0.2208 - val_acc: 0.9106 - val_precision_19: 0.9874 - val_auc_19: 0.9653 - val_recall_19: 0.5798
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1902 - acc: 0.9207 - precision_19: 0.8751 - auc_19: 0.9617 - recall_19: 0.7576 - val_loss: 0.1925 - val_acc: 0.9261 - val_precision_19: 0.9355 - val_auc_19: 0.9609 - val_recall_19: 0.7022
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1925 - acc: 0.9205 - precision_19: 0.8780 - auc_19: 0.9603 - recall_19: 0.7562 - val_loss: 0.1858 - val_acc: 0.9272 - val_precision_19: 0.9505 - val_auc_19: 0.9676 - val_recall_19: 0.6916
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1860 - acc: 0.9227 - precision_19: 0.8776 - auc_19: 0.9638 - recall_19: 0.7664 - val_loss: 0.1837 - val_acc: 0.9396 - val_precision_19: 0.8865 - val_auc_19: 0.9723 - val_recall_19: 0.8183
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1780 - acc: 0.9258 - precision_19: 0.8805 - auc_19: 0.9671 - recall_19: 0.7814 - val_loss: 0.1593 - val_acc: 0.9373 - val_precision_19: 0.8405 - val_auc_19: 0.9768 - val_recall_19: 0.8733
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1932 - acc: 0.9192 - precision_19: 0.8663 - auc_19: 0.9595 - recall_19: 0.7617 - val_loss: 0.1750 - val_acc: 0.9347 - val_precision_19: 0.9499 - val_auc_19: 0.9695 - val_recall_19: 0.7215
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1737 - acc: 0.9274 - precision_19: 0.8839 - auc_19: 0.9682 - recall_19: 0.7844 - val_loss: 0.1593 - val_acc: 0.9393 - val_precision_19: 0.8560 - val_auc_19: 0.9778 - val_recall_19: 0.8745
Epoch 16/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1681 - acc: 0.9288 - precision_19: 0.8843 - auc_19: 0.9705 - recall_19: 0.7932 - val_loss: 0.1417 - val_acc: 0.9459 - val_precision_19: 0.8995 - val_auc_19: 0.9750 - val_recall_19: 0.8183
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1774 - acc: 0.9265 - precision_19: 0.8886 - auc_19: 0.9670 - recall_19: 0.7756 - val_loss: 0.1657 - val_acc: 0.9390 - val_precision_19: 0.8699 - val_auc_19: 0.9762 - val_recall_19: 0.8494
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1714 - acc: 0.9279 - precision_19: 0.8858 - auc_19: 0.9691 - recall_19: 0.7870 - val_loss: 0.1504 - val_acc: 0.9436 - val_precision_19: 0.8638 - val_auc_19: 0.9771 - val_recall_19: 0.8633
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1665 - acc: 0.9303 - precision_19: 0.8871 - auc_19: 0.9709 - recall_19: 0.7967 - val_loss: 0.1499 - val_acc: 0.9431 - val_precision_19: 0.9243 - val_auc_19: 0.9766 - val_recall_19: 0.8078
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1653 - acc: 0.9305 - precision_19: 0.8895 - auc_19: 0.9713 - recall_19: 0.7974 - val_loss: 0.1320 - val_acc: 0.9498 - val_precision_19: 0.8967 - val_auc_19: 0.9774 - val_recall_19: 0.8338
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1762 - acc: 0.9268 - precision_19: 0.8792 - auc_19: 0.9679 - recall_19: 0.7894 - val_loss: 0.1563 - val_acc: 0.9399 - val_precision_19: 0.8874 - val_auc_19: 0.9762 - val_recall_19: 0.8495
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1618 - acc: 0.9313 - precision_19: 0.8912 - auc_19: 0.9725 - recall_19: 0.7985 - val_loss: 0.1524 - val_acc: 0.9419 - val_precision_19: 0.8780 - val_auc_19: 0.9745 - val_recall_19: 0.8291
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1580 - acc: 0.9332 - precision_19: 0.8934 - auc_19: 0.9732 - recall_19: 0.8059 - val_loss: 0.1411 - val_acc: 0.9450 - val_precision_19: 0.8816 - val_auc_19: 0.9794 - val_recall_19: 0.8547
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1774 - acc: 0.9262 - precision_19: 0.8845 - auc_19: 0.9663 - recall_19: 0.7780 - val_loss: 0.1468 - val_acc: 0.9430 - val_precision_19: 0.8865 - val_auc_19: 0.9755 - val_recall_19: 0.8441
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1736 - acc: 0.9283 - precision_19: 0.8847 - auc_19: 0.9679 - recall_19: 0.7904 - val_loss: 0.1606 - val_acc: 0.9376 - val_precision_19: 0.8150 - val_auc_19: 0.9813 - val_recall_19: 0.9053
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1683 - acc: 0.9300 - precision_19: 0.8825 - auc_19: 0.9696 - recall_19: 0.8012 - val_loss: 0.1460 - val_acc: 0.9438 - val_precision_19: 0.8974 - val_auc_19: 0.9761 - val_recall_19: 0.8278
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1620 - acc: 0.9324 - precision_19: 0.8940 - auc_19: 0.9719 - recall_19: 0.8014 - val_loss: 0.1355 - val_acc: 0.9471 - val_precision_19: 0.9064 - val_auc_19: 0.9819 - val_recall_19: 0.8382
Epoch 28/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1615 - acc: 0.9316 - precision_19: 0.8918 - auc_19: 0.9725 - recall_19: 0.7998 - val_loss: 0.1604 - val_acc: 0.9383 - val_precision_19: 0.8291 - val_auc_19: 0.9803 - val_recall_19: 0.8940
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1616 - acc: 0.9316 - precision_19: 0.8899 - auc_19: 0.9727 - recall_19: 0.8027 - val_loss: 0.1741 - val_acc: 0.9282 - val_precision_19: 0.7957 - val_auc_19: 0.9788 - val_recall_19: 0.9173
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1579 - acc: 0.9333 - precision_19: 0.8916 - auc_19: 0.9734 - recall_19: 0.8106 - val_loss: 0.1357 - val_acc: 0.9474 - val_precision_19: 0.9362 - val_auc_19: 0.9814 - val_recall_19: 0.8067
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1555 - acc: 0.9343 - precision_19: 0.8980 - auc_19: 0.9739 - recall_19: 0.8072 - val_loss: 0.1425 - val_acc: 0.9431 - val_precision_19: 0.8460 - val_auc_19: 0.9810 - val_recall_19: 0.8907
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1570 - acc: 0.9338 - precision_19: 0.8936 - auc_19: 0.9741 - recall_19: 0.8104 - val_loss: 0.1359 - val_acc: 0.9481 - val_precision_19: 0.8927 - val_auc_19: 0.9814 - val_recall_19: 0.8609
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1501 - acc: 0.9359 - precision_19: 0.8933 - auc_19: 0.9765 - recall_19: 0.8206 - val_loss: 0.1260 - val_acc: 0.9506 - val_precision_19: 0.8942 - val_auc_19: 0.9837 - val_recall_19: 0.8685
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1511 - acc: 0.9358 - precision_19: 0.8976 - auc_19: 0.9759 - recall_19: 0.8144 - val_loss: 0.1302 - val_acc: 0.9499 - val_precision_19: 0.8795 - val_auc_19: 0.9813 - val_recall_19: 0.8627
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1479 - acc: 0.9370 - precision_19: 0.9012 - auc_19: 0.9770 - recall_19: 0.8165 - val_loss: 0.1379 - val_acc: 0.9442 - val_precision_19: 0.8708 - val_auc_19: 0.9829 - val_recall_19: 0.8831
Epoch 36/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1451 - acc: 0.9382 - precision_19: 0.9006 - auc_19: 0.9779 - recall_19: 0.8239 - val_loss: 0.1345 - val_acc: 0.9471 - val_precision_19: 0.9024 - val_auc_19: 0.9797 - val_recall_19: 0.8425
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1505 - acc: 0.9363 - precision_19: 0.8969 - auc_19: 0.9760 - recall_19: 0.8189 - val_loss: 0.1779 - val_acc: 0.9349 - val_precision_19: 0.9728 - val_auc_19: 0.9790 - val_recall_19: 0.7183
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1421 - acc: 0.9393 - precision_19: 0.9024 - auc_19: 0.9790 - recall_19: 0.8290 - val_loss: 0.1364 - val_acc: 0.9477 - val_precision_19: 0.8679 - val_auc_19: 0.9828 - val_recall_19: 0.8855
Epoch 39/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1426 - acc: 0.9384 - precision_19: 0.9034 - auc_19: 0.9789 - recall_19: 0.8220 - val_loss: 0.1378 - val_acc: 0.9451 - val_precision_19: 0.9067 - val_auc_19: 0.9802 - val_recall_19: 0.8353
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1406 - acc: 0.9393 - precision_19: 0.9028 - auc_19: 0.9797 - recall_19: 0.8292 - val_loss: 0.1474 - val_acc: 0.9446 - val_precision_19: 0.9720 - val_auc_19: 0.9811 - val_recall_19: 0.7428
Epoch 41/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1411 - acc: 0.9392 - precision_19: 0.8997 - auc_19: 0.9794 - recall_19: 0.8311 - val_loss: 0.1561 - val_acc: 0.9335 - val_precision_19: 0.8283 - val_auc_19: 0.9785 - val_recall_19: 0.8785
Epoch 42/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1405 - acc: 0.9393 - precision_19: 0.9032 - auc_19: 0.9796 - recall_19: 0.8284 - val_loss: 0.1344 - val_acc: 0.9477 - val_precision_19: 0.8842 - val_auc_19: 0.9824 - val_recall_19: 0.8643
Epoch 43/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1390 - acc: 0.9398 - precision_19: 0.9053 - auc_19: 0.9799 - recall_19: 0.8296 - val_loss: 0.1537 - val_acc: 0.9383 - val_precision_19: 0.8449 - val_auc_19: 0.9800 - val_recall_19: 0.8773
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1385 - acc: 0.9405 - precision_19: 0.9040 - auc_19: 0.9802 - recall_19: 0.8331 - val_loss: 0.1246 - val_acc: 0.9514 - val_precision_19: 0.9318 - val_auc_19: 0.9819 - val_recall_19: 0.8321
Epoch 45/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1417 - acc: 0.9395 - precision_19: 0.9063 - auc_19: 0.9792 - recall_19: 0.8245 - val_loss: 0.1344 - val_acc: 0.9460 - val_precision_19: 0.9227 - val_auc_19: 0.9811 - val_recall_19: 0.8167
Epoch 46/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1463 - acc: 0.9372 - precision_19: 0.9012 - auc_19: 0.9781 - recall_19: 0.8181 - val_loss: 0.1297 - val_acc: 0.9478 - val_precision_19: 0.8783 - val_auc_19: 0.9827 - val_recall_19: 0.8725
Epoch 47/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1351 - acc: 0.9414 - precision_19: 0.9070 - auc_19: 0.9811 - recall_19: 0.8347 - val_loss: 0.1198 - val_acc: 0.9521 - val_precision_19: 0.9111 - val_auc_19: 0.9844 - val_recall_19: 0.8606
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1352 - acc: 0.9414 - precision_19: 0.9043 - auc_19: 0.9807 - recall_19: 0.8384 - val_loss: 0.1275 - val_acc: 0.9507 - val_precision_19: 0.9406 - val_auc_19: 0.9830 - val_recall_19: 0.8164
Epoch 49/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1346 - acc: 0.9416 - precision_19: 0.9081 - auc_19: 0.9813 - recall_19: 0.8335 - val_loss: 0.1573 - val_acc: 0.9395 - val_precision_19: 0.8278 - val_auc_19: 0.9820 - val_recall_19: 0.9029
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1329 - acc: 0.9422 - precision_19: 0.9075 - auc_19: 0.9816 - recall_19: 0.8370 - val_loss: 0.1344 - val_acc: 0.9424 - val_precision_19: 0.8416 - val_auc_19: 0.9827 - val_recall_19: 0.8987
Epoch 51/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1423 - acc: 0.9394 - precision_19: 0.9042 - auc_19: 0.9785 - recall_19: 0.8275 - val_loss: 0.1276 - val_acc: 0.9479 - val_precision_19: 0.8717 - val_auc_19: 0.9839 - val_recall_19: 0.8821
Epoch 52/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1385 - acc: 0.9402 - precision_19: 0.9020 - auc_19: 0.9801 - recall_19: 0.8335 - val_loss: 0.1658 - val_acc: 0.9357 - val_precision_19: 0.8079 - val_auc_19: 0.9822 - val_recall_19: 0.9172
Epoch 53/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1305 - acc: 0.9431 - precision_19: 0.9097 - auc_19: 0.9823 - recall_19: 0.8395 - val_loss: 0.1243 - val_acc: 0.9494 - val_precision_19: 0.8809 - val_auc_19: 0.9843 - val_recall_19: 0.8788
Epoch 54/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1347 - acc: 0.9417 - precision_19: 0.9051 - auc_19: 0.9817 - recall_19: 0.8390 - val_loss: 0.1341 - val_acc: 0.9475 - val_precision_19: 0.9281 - val_auc_19: 0.9807 - val_recall_19: 0.8175
Epoch 55/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1348 - acc: 0.9416 - precision_19: 0.9075 - auc_19: 0.9811 - recall_19: 0.8344 - val_loss: 0.1115 - val_acc: 0.9558 - val_precision_19: 0.9148 - val_auc_19: 0.9861 - val_recall_19: 0.8698
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1318 - acc: 0.9422 - precision_19: 0.9087 - auc_19: 0.9820 - recall_19: 0.8357 - val_loss: 0.1307 - val_acc: 0.9474 - val_precision_19: 0.8720 - val_auc_19: 0.9824 - val_recall_19: 0.8886
Epoch 57/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1318 - acc: 0.9425 - precision_19: 0.9061 - auc_19: 0.9818 - recall_19: 0.8408 - val_loss: 0.1217 - val_acc: 0.9512 - val_precision_19: 0.8842 - val_auc_19: 0.9860 - val_recall_19: 0.8826
Epoch 58/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1328 - acc: 0.9428 - precision_19: 0.9085 - auc_19: 0.9818 - recall_19: 0.8409 - val_loss: 0.1452 - val_acc: 0.9456 - val_precision_19: 0.9663 - val_auc_19: 0.9814 - val_recall_19: 0.7603
Epoch 59/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1350 - acc: 0.9416 - precision_19: 0.9086 - auc_19: 0.9810 - recall_19: 0.8326 - val_loss: 0.1542 - val_acc: 0.9362 - val_precision_19: 0.8271 - val_auc_19: 0.9803 - val_recall_19: 0.9029
Epoch 60/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1332 - acc: 0.9419 - precision_19: 0.9083 - auc_19: 0.9818 - recall_19: 0.8351 - val_loss: 0.1133 - val_acc: 0.9541 - val_precision_19: 0.9117 - val_auc_19: 0.9867 - val_recall_19: 0.8683
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1294 - acc: 0.9434 - precision_19: 0.9098 - auc_19: 0.9825 - recall_19: 0.8416 - val_loss: 0.1254 - val_acc: 0.9518 - val_precision_19: 0.9159 - val_auc_19: 0.9832 - val_recall_19: 0.8511
Epoch 62/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1357 - acc: 0.9422 - precision_19: 0.9071 - auc_19: 0.9812 - recall_19: 0.8386 - val_loss: 0.1207 - val_acc: 0.9500 - val_precision_19: 0.8904 - val_auc_19: 0.9842 - val_recall_19: 0.8678
Epoch 63/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1301 - acc: 0.9433 - precision_19: 0.9085 - auc_19: 0.9823 - recall_19: 0.8409 - val_loss: 0.1153 - val_acc: 0.9536 - val_precision_19: 0.9330 - val_auc_19: 0.9862 - val_recall_19: 0.8389
Epoch 64/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1359 - acc: 0.9416 - precision_19: 0.9078 - auc_19: 0.9803 - recall_19: 0.8333 - val_loss: 0.1251 - val_acc: 0.9502 - val_precision_19: 0.9448 - val_auc_19: 0.9848 - val_recall_19: 0.8220
Epoch 65/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1327 - acc: 0.9429 - precision_19: 0.9102 - auc_19: 0.9813 - recall_19: 0.8383 - val_loss: 0.1254 - val_acc: 0.9511 - val_precision_19: 0.9043 - val_auc_19: 0.9805 - val_recall_19: 0.8443
Epoch 66/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9441 - precision_19: 0.9107 - auc_19: 0.9819 - recall_19: 0.8443 - val_loss: 0.1212 - val_acc: 0.9509 - val_precision_19: 0.9456 - val_auc_19: 0.9865 - val_recall_19: 0.8270
Epoch 67/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1267 - acc: 0.9443 - precision_19: 0.9107 - auc_19: 0.9836 - recall_19: 0.8448 - val_loss: 0.1161 - val_acc: 0.9557 - val_precision_19: 0.9249 - val_auc_19: 0.9836 - val_recall_19: 0.8538
Epoch 68/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9435 - precision_19: 0.9124 - auc_19: 0.9821 - recall_19: 0.8380 - val_loss: 0.1267 - val_acc: 0.9460 - val_precision_19: 0.8472 - val_auc_19: 0.9854 - val_recall_19: 0.9047
Epoch 69/100
254/254 [==============================] - ETA: 0s - loss: 0.1263 - acc: 0.9444 - precision_19: 0.9095 - auc_19: 0.9835 - recall_19: 0.8473Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 219ms/step - loss: 0.1263 - acc: 0.9444 - precision_19: 0.9095 - auc_19: 0.9835 - recall_19: 0.8473 - val_loss: 0.1376 - val_acc: 0.9420 - val_precision_19: 0.8409 - val_auc_19: 0.9818 - val_recall_19: 0.8949
Epoch 00069: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1303 - acc: 0.9494 - precision_19: 0.9036 - auc_19: 0.9830 - recall_19: 0.8663
---------------TEST METRICS----------------------
jaccard_index 0.8028049092620341
test_sensitivity 0.8752965882161342
test_specifitivity 0.9711306338487306
test_accuracy 0.9497019990961603
test_precision 0.8972449759706855
test_jaccard_score 0.8028049092620341
test_dicecoef 0.8861348948073916
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-031812.h5
[0.94562612 0.78295133 0.90966971 0.84026524 0.97596987] [0.949702   0.80280491 0.89724498 0.87529659 0.97113063]

-------------------------
Rep: 2
-------------------------

2021-09-25 03:18:13.487523: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:18:13.487656: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Normalized per channel
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 4.962215403927742e-17,
Validation samples: 383, channel mean: 0.0053282898964007895
Model built.
Model: "functional_41"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_21 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_380 (Conv2D)             (None, 256, 256, 32) 320         input_21[0][0]
__________________________________________________________________________________________________
conv2d_381 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_380[0][0]
__________________________________________________________________________________________________
max_pooling2d_80 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_382 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_80[0][0]
__________________________________________________________________________________________________
conv2d_383 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_382[0][0]
__________________________________________________________________________________________________
max_pooling2d_81 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_384 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_81[0][0]
__________________________________________________________________________________________________
conv2d_385 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_384[0][0]
__________________________________________________________________________________________________
max_pooling2d_82 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_386 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_82[0][0]
__________________________________________________________________________________________________
conv2d_387 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_386[0][0]
__________________________________________________________________________________________________
max_pooling2d_83 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_388 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_83[0][0]
__________________________________________________________________________________________________
conv2d_389 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_388[0][0]
__________________________________________________________________________________________________
up_sampling2d_80 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_389[0][0]
__________________________________________________________________________________________________
concatenate_80 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_80[0][0]
                                                                 conv2d_387[0][0]
__________________________________________________________________________________________________
conv2d_390 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_80[0][0]
__________________________________________________________________________________________________
conv2d_391 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_390[0][0]
__________________________________________________________________________________________________
up_sampling2d_81 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_391[0][0]
__________________________________________________________________________________________________
concatenate_81 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_81[0][0]
                                                                 conv2d_385[0][0]
__________________________________________________________________________________________________
conv2d_392 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_81[0][0]
__________________________________________________________________________________________________
conv2d_393 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_392[0][0]
__________________________________________________________________________________________________
up_sampling2d_82 (UpSampling2D) (None, 128, 128, 128 0           conv2d_393[0][0]
__________________________________________________________________________________________________
concatenate_82 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_82[0][0]
                                                                 conv2d_383[0][0]
__________________________________________________________________________________________________
conv2d_394 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_82[0][0]
__________________________________________________________________________________________________
conv2d_395 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_394[0][0]
__________________________________________________________________________________________________
up_sampling2d_83 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_395[0][0]
__________________________________________________________________________________________________
concatenate_83 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_83[0][0]
                                                                 conv2d_381[0][0]
__________________________________________________________________________________________________
conv2d_396 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_83[0][0]
__________________________________________________________________________________________________
conv2d_397 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_396[0][0]
__________________________________________________________________________________________________
conv2d_398 (Conv2D)             (None, 256, 256, 1)  33          conv2d_397[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6927 - acc: 0.7240 - precision_20: 0.3299 - auc_20: 0.5007 - recall_20: 0.60152021-09-25 03:18:22.581701: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:18:22.581784: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 03:18:23.105046: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 03:18:23.121102: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23
2021-09-25 03:18:23.127235: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.trace.json.gz
2021-09-25 03:18:23.154952: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23
2021-09-25 03:18:23.164300: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.memory_profile.json.gz
2021-09-25 03:18:23.181490: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23Dumped tool data for xplane.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-031813/train/plugins/profile/2021_09_25_03_18_23/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6906 - acc: 0.6791 - precision_20: 0.3299 - auc_20: 0.3783 - recall_20: 0.1885WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1976s vs `on_train_batch_end` time: 0.4037s). Check your callbacks.
254/254 [==============================] - 57s 223ms/step - loss: 0.4150 - acc: 0.8397 - precision_20: 0.7101 - auc_20: 0.8020 - recall_20: 0.4485 - val_loss: 0.4143 - val_acc: 0.8606 - val_precision_20: 0.8569 - val_auc_20: 0.8562 - val_recall_20: 0.4129
Epoch 2/100
254/254 [==============================] - 56s 219ms/step - loss: 0.3777 - acc: 0.8502 - precision_20: 0.7378 - auc_20: 0.8262 - recall_20: 0.4886 - val_loss: 0.2617 - val_acc: 0.9062 - val_precision_20: 0.9120 - val_auc_20: 0.9172 - val_recall_20: 0.6170
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2908 - acc: 0.8859 - precision_20: 0.8194 - auc_20: 0.8986 - recall_20: 0.6206 - val_loss: 0.2345 - val_acc: 0.9138 - val_precision_20: 0.8138 - val_auc_20: 0.9415 - val_recall_20: 0.7741
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2504 - acc: 0.8997 - precision_20: 0.8430 - auc_20: 0.9297 - recall_20: 0.6781 - val_loss: 0.2136 - val_acc: 0.9211 - val_precision_20: 0.8213 - val_auc_20: 0.9565 - val_recall_20: 0.8008
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2442 - acc: 0.9022 - precision_20: 0.8452 - auc_20: 0.9346 - recall_20: 0.6911 - val_loss: 0.1943 - val_acc: 0.9261 - val_precision_20: 0.8523 - val_auc_20: 0.9552 - val_recall_20: 0.7756
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2255 - acc: 0.9089 - precision_20: 0.8571 - auc_20: 0.9431 - recall_20: 0.7134 - val_loss: 0.1931 - val_acc: 0.9267 - val_precision_20: 0.9016 - val_auc_20: 0.9615 - val_recall_20: 0.7430
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2190 - acc: 0.9118 - precision_20: 0.8592 - auc_20: 0.9472 - recall_20: 0.7282 - val_loss: 0.1784 - val_acc: 0.9328 - val_precision_20: 0.9163 - val_auc_20: 0.9647 - val_recall_20: 0.7474
Epoch 8/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2038 - acc: 0.9168 - precision_20: 0.8654 - auc_20: 0.9543 - recall_20: 0.7483 - val_loss: 0.1879 - val_acc: 0.9272 - val_precision_20: 0.8783 - val_auc_20: 0.9645 - val_recall_20: 0.7753
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1947 - acc: 0.9194 - precision_20: 0.8680 - auc_20: 0.9591 - recall_20: 0.7615 - val_loss: 0.2113 - val_acc: 0.9188 - val_precision_20: 0.9765 - val_auc_20: 0.9579 - val_recall_20: 0.6267
Epoch 10/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1851 - acc: 0.9227 - precision_20: 0.8748 - auc_20: 0.9631 - recall_20: 0.7696 - val_loss: 0.1758 - val_acc: 0.9334 - val_precision_20: 0.9216 - val_auc_20: 0.9657 - val_recall_20: 0.7520
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1854 - acc: 0.9226 - precision_20: 0.8792 - auc_20: 0.9629 - recall_20: 0.7663 - val_loss: 0.1765 - val_acc: 0.9353 - val_precision_20: 0.8702 - val_auc_20: 0.9685 - val_recall_20: 0.8157
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1800 - acc: 0.9253 - precision_20: 0.8786 - auc_20: 0.9655 - recall_20: 0.7804 - val_loss: 0.1584 - val_acc: 0.9407 - val_precision_20: 0.8974 - val_auc_20: 0.9714 - val_recall_20: 0.8115
Epoch 13/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1728 - acc: 0.9282 - precision_20: 0.8854 - auc_20: 0.9683 - recall_20: 0.7882 - val_loss: 0.1522 - val_acc: 0.9400 - val_precision_20: 0.8444 - val_auc_20: 0.9786 - val_recall_20: 0.8828
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1717 - acc: 0.9282 - precision_20: 0.8842 - auc_20: 0.9678 - recall_20: 0.7905 - val_loss: 0.1519 - val_acc: 0.9399 - val_precision_20: 0.9135 - val_auc_20: 0.9741 - val_recall_20: 0.7830
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1659 - acc: 0.9305 - precision_20: 0.8906 - auc_20: 0.9709 - recall_20: 0.7933 - val_loss: 0.1668 - val_acc: 0.9302 - val_precision_20: 0.8077 - val_auc_20: 0.9780 - val_recall_20: 0.9010
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1622 - acc: 0.9312 - precision_20: 0.8865 - auc_20: 0.9723 - recall_20: 0.8037 - val_loss: 0.1286 - val_acc: 0.9497 - val_precision_20: 0.9174 - val_auc_20: 0.9810 - val_recall_20: 0.8199
Epoch 17/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1640 - acc: 0.9304 - precision_20: 0.8909 - auc_20: 0.9716 - recall_20: 0.7939 - val_loss: 0.1625 - val_acc: 0.9351 - val_precision_20: 0.8433 - val_auc_20: 0.9758 - val_recall_20: 0.8658
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1641 - acc: 0.9308 - precision_20: 0.8874 - auc_20: 0.9714 - recall_20: 0.8004 - val_loss: 0.1346 - val_acc: 0.9476 - val_precision_20: 0.9048 - val_auc_20: 0.9808 - val_recall_20: 0.8346
Epoch 19/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1595 - acc: 0.9324 - precision_20: 0.8870 - auc_20: 0.9735 - recall_20: 0.8079 - val_loss: 0.1456 - val_acc: 0.9440 - val_precision_20: 0.9348 - val_auc_20: 0.9798 - val_recall_20: 0.8017
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1607 - acc: 0.9322 - precision_20: 0.8914 - auc_20: 0.9730 - recall_20: 0.8037 - val_loss: 0.1315 - val_acc: 0.9474 - val_precision_20: 0.8642 - val_auc_20: 0.9789 - val_recall_20: 0.8605
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1637 - acc: 0.9307 - precision_20: 0.8844 - auc_20: 0.9718 - recall_20: 0.8043 - val_loss: 0.1388 - val_acc: 0.9466 - val_precision_20: 0.8993 - val_auc_20: 0.9833 - val_recall_20: 0.8672
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1557 - acc: 0.9340 - precision_20: 0.8947 - auc_20: 0.9748 - recall_20: 0.8087 - val_loss: 0.1472 - val_acc: 0.9419 - val_precision_20: 0.8620 - val_auc_20: 0.9772 - val_recall_20: 0.8502
Epoch 23/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1597 - acc: 0.9335 - precision_20: 0.8922 - auc_20: 0.9720 - recall_20: 0.8087 - val_loss: 0.1493 - val_acc: 0.9393 - val_precision_20: 0.8369 - val_auc_20: 0.9799 - val_recall_20: 0.8853
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1531 - acc: 0.9348 - precision_20: 0.8988 - auc_20: 0.9755 - recall_20: 0.8074 - val_loss: 0.1477 - val_acc: 0.9395 - val_precision_20: 0.8342 - val_auc_20: 0.9798 - val_recall_20: 0.8985
Epoch 25/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1548 - acc: 0.9347 - precision_20: 0.8918 - auc_20: 0.9751 - recall_20: 0.8164 - val_loss: 0.1633 - val_acc: 0.9385 - val_precision_20: 0.8021 - val_auc_20: 0.9863 - val_recall_20: 0.9346
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1470 - acc: 0.9370 - precision_20: 0.8943 - auc_20: 0.9772 - recall_20: 0.8246 - val_loss: 0.1387 - val_acc: 0.9450 - val_precision_20: 0.8926 - val_auc_20: 0.9797 - val_recall_20: 0.8398
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1605 - acc: 0.9333 - precision_20: 0.8945 - auc_20: 0.9723 - recall_20: 0.8054 - val_loss: 0.1361 - val_acc: 0.9460 - val_precision_20: 0.9174 - val_auc_20: 0.9814 - val_recall_20: 0.8201
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1537 - acc: 0.9349 - precision_20: 0.8977 - auc_20: 0.9751 - recall_20: 0.8099 - val_loss: 0.1844 - val_acc: 0.9187 - val_precision_20: 0.7442 - val_auc_20: 0.9826 - val_recall_20: 0.9413
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1481 - acc: 0.9365 - precision_20: 0.8952 - auc_20: 0.9770 - recall_20: 0.8226 - val_loss: 0.1685 - val_acc: 0.9287 - val_precision_20: 0.7939 - val_auc_20: 0.9814 - val_recall_20: 0.9241
Epoch 30/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1447 - acc: 0.9375 - precision_20: 0.8968 - auc_20: 0.9782 - recall_20: 0.8269 - val_loss: 0.1261 - val_acc: 0.9521 - val_precision_20: 0.9321 - val_auc_20: 0.9836 - val_recall_20: 0.8347
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1444 - acc: 0.9382 - precision_20: 0.9011 - auc_20: 0.9778 - recall_20: 0.8249 - val_loss: 0.1434 - val_acc: 0.9431 - val_precision_20: 0.8426 - val_auc_20: 0.9803 - val_recall_20: 0.8961
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1460 - acc: 0.9374 - precision_20: 0.8969 - auc_20: 0.9777 - recall_20: 0.8257 - val_loss: 0.1346 - val_acc: 0.9488 - val_precision_20: 0.8808 - val_auc_20: 0.9838 - val_recall_20: 0.8797
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1437 - acc: 0.9383 - precision_20: 0.8983 - auc_20: 0.9784 - recall_20: 0.8274 - val_loss: 0.1293 - val_acc: 0.9490 - val_precision_20: 0.8794 - val_auc_20: 0.9823 - val_recall_20: 0.8788
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1446 - acc: 0.9381 - precision_20: 0.9012 - auc_20: 0.9778 - recall_20: 0.8223 - val_loss: 0.1316 - val_acc: 0.9499 - val_precision_20: 0.8729 - val_auc_20: 0.9814 - val_recall_20: 0.8715
Epoch 35/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1400 - acc: 0.9393 - precision_20: 0.9025 - auc_20: 0.9796 - recall_20: 0.8273 - val_loss: 0.1390 - val_acc: 0.9429 - val_precision_20: 0.8485 - val_auc_20: 0.9847 - val_recall_20: 0.9086
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1405 - acc: 0.9400 - precision_20: 0.9013 - auc_20: 0.9791 - recall_20: 0.8323 - val_loss: 0.1353 - val_acc: 0.9456 - val_precision_20: 0.9009 - val_auc_20: 0.9795 - val_recall_20: 0.8366
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1788 - acc: 0.9278 - precision_20: 0.8828 - auc_20: 0.9677 - recall_20: 0.7892 - val_loss: 0.1755 - val_acc: 0.9354 - val_precision_20: 0.9791 - val_auc_20: 0.9789 - val_recall_20: 0.7158
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1416 - acc: 0.9394 - precision_20: 0.9033 - auc_20: 0.9789 - recall_20: 0.8287 - val_loss: 0.1297 - val_acc: 0.9519 - val_precision_20: 0.8824 - val_auc_20: 0.9832 - val_recall_20: 0.8891
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1410 - acc: 0.9395 - precision_20: 0.9054 - auc_20: 0.9790 - recall_20: 0.8248 - val_loss: 0.1462 - val_acc: 0.9415 - val_precision_20: 0.8664 - val_auc_20: 0.9800 - val_recall_20: 0.8667
Epoch 40/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1381 - acc: 0.9397 - precision_20: 0.9043 - auc_20: 0.9804 - recall_20: 0.8291 - val_loss: 0.1367 - val_acc: 0.9473 - val_precision_20: 0.9694 - val_auc_20: 0.9824 - val_recall_20: 0.7590
Epoch 41/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1359 - acc: 0.9406 - precision_20: 0.9032 - auc_20: 0.9806 - recall_20: 0.8340 - val_loss: 0.1450 - val_acc: 0.9382 - val_precision_20: 0.8437 - val_auc_20: 0.9804 - val_recall_20: 0.8814
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1378 - acc: 0.9399 - precision_20: 0.9054 - auc_20: 0.9803 - recall_20: 0.8292 - val_loss: 0.1309 - val_acc: 0.9491 - val_precision_20: 0.8851 - val_auc_20: 0.9833 - val_recall_20: 0.8710
Epoch 43/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1371 - acc: 0.9402 - precision_20: 0.9043 - auc_20: 0.9805 - recall_20: 0.8326 - val_loss: 0.1410 - val_acc: 0.9437 - val_precision_20: 0.8633 - val_auc_20: 0.9823 - val_recall_20: 0.8805
Epoch 44/100
254/254 [==============================] - ETA: 0s - loss: 0.1357 - acc: 0.9413 - precision_20: 0.9058 - auc_20: 0.9811 - recall_20: 0.8344Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1357 - acc: 0.9413 - precision_20: 0.9058 - auc_20: 0.9811 - recall_20: 0.8344 - val_loss: 0.1352 - val_acc: 0.9472 - val_precision_20: 0.9471 - val_auc_20: 0.9809 - val_recall_20: 0.7956
Epoch 00044: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1398 - acc: 0.9469 - precision_20: 0.9207 - auc_20: 0.9800 - recall_20: 0.8346
---------------TEST METRICS----------------------
jaccard_index 0.7770268811251573
test_sensitivity 0.849272809857098
test_specifitivity 0.9724030801591694
test_accuracy 0.9448709690824468
test_precision 0.8986100979458086
test_jaccard_score 0.7770268811251573
test_dicecoef 0.8732451349475114
isic_eval_score 0.9929078014184397
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-035955.h5
[1.89532812 1.58575624 1.80691469 1.71556183 1.94710051] [0.94487097 0.77702688 0.8986101  0.84927281 0.97240308]

-------------------------
Averaged metrics for Baseline + Augumentations + Per Channel Normalization - lesion: [0.94673303 0.78759437 0.9018416  0.85494488 0.97316786]
-------------------------


-------------------------
RUN: Baseline + Augumentations + Gaussian Blur - lesion, PARAMS: {'augumentation': True, 'gaussian_blur': True}
-------------------------


-------------------------
Rep: 0
-------------------------

2021-09-25 03:59:55.984366: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 03:59:55.984485: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_43"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_22 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_399 (Conv2D)             (None, 256, 256, 32) 320         input_22[0][0]
__________________________________________________________________________________________________
conv2d_400 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_399[0][0]
__________________________________________________________________________________________________
max_pooling2d_84 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_401 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_84[0][0]
__________________________________________________________________________________________________
conv2d_402 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_401[0][0]
__________________________________________________________________________________________________
max_pooling2d_85 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_403 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_85[0][0]
__________________________________________________________________________________________________
conv2d_404 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_403[0][0]
__________________________________________________________________________________________________
max_pooling2d_86 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_405 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_86[0][0]
__________________________________________________________________________________________________
conv2d_406 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_405[0][0]
__________________________________________________________________________________________________
max_pooling2d_87 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_407 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_87[0][0]
__________________________________________________________________________________________________
conv2d_408 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_407[0][0]
__________________________________________________________________________________________________
up_sampling2d_84 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_408[0][0]
__________________________________________________________________________________________________
concatenate_84 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_84[0][0]
                                                                 conv2d_406[0][0]
__________________________________________________________________________________________________
conv2d_409 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_84[0][0]
__________________________________________________________________________________________________
conv2d_410 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_409[0][0]
__________________________________________________________________________________________________
up_sampling2d_85 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_410[0][0]
__________________________________________________________________________________________________
concatenate_85 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_85[0][0]
                                                                 conv2d_404[0][0]
__________________________________________________________________________________________________
conv2d_411 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_85[0][0]
__________________________________________________________________________________________________
conv2d_412 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_411[0][0]
__________________________________________________________________________________________________
up_sampling2d_86 (UpSampling2D) (None, 128, 128, 128 0           conv2d_412[0][0]
__________________________________________________________________________________________________
concatenate_86 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_86[0][0]
                                                                 conv2d_402[0][0]
__________________________________________________________________________________________________
conv2d_413 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_86[0][0]
__________________________________________________________________________________________________
conv2d_414 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_413[0][0]
__________________________________________________________________________________________________
up_sampling2d_87 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_414[0][0]
__________________________________________________________________________________________________
concatenate_87 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_87[0][0]
                                                                 conv2d_400[0][0]
__________________________________________________________________________________________________
conv2d_415 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_87[0][0]
__________________________________________________________________________________________________
conv2d_416 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_415[0][0]
__________________________________________________________________________________________________
conv2d_417 (Conv2D)             (None, 256, 256, 1)  33          conv2d_416[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6926 - acc: 0.7683 - precision_21: 0.2681 - auc_21: 0.5074 - recall_21: 0.21442021-09-25 04:00:34.794123: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 04:00:34.794763: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 04:00:35.319430: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 04:00:35.330743: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35
2021-09-25 04:00:35.334084: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.trace.json.gz
2021-09-25 04:00:35.361324: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35
2021-09-25 04:00:35.369348: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.memory_profile.json.gz
2021-09-25 04:00:35.383945: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35Dumped tool data for xplane.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-035955/train/plugins/profile/2021_09_25_04_00_35/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:14 - loss: 0.6859 - acc: 0.7013 - precision_21: 0.2681 - auc_21: 0.4188 - recall_21: 0.0672WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2071s vs `on_train_batch_end` time: 0.3834s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4908 - acc: 0.7966 - precision_21: 0.7713 - auc_21: 0.7735 - recall_21: 0.0889 - val_loss: 0.3949 - val_acc: 0.8518 - val_precision_21: 0.7913 - val_auc_21: 0.7990 - val_recall_21: 0.4102
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3587 - acc: 0.8499 - precision_21: 0.8039 - auc_21: 0.8476 - recall_21: 0.4126 - val_loss: 0.3437 - val_acc: 0.8728 - val_precision_21: 0.8441 - val_auc_21: 0.8580 - val_recall_21: 0.4906
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3394 - acc: 0.8548 - precision_21: 0.8001 - auc_21: 0.8676 - recall_21: 0.4468 - val_loss: 0.3473 - val_acc: 0.8622 - val_precision_21: 0.7526 - val_auc_21: 0.8610 - val_recall_21: 0.5307
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3210 - acc: 0.8652 - precision_21: 0.8199 - auc_21: 0.8785 - recall_21: 0.4950 - val_loss: 0.3288 - val_acc: 0.8728 - val_precision_21: 0.8135 - val_auc_21: 0.8704 - val_recall_21: 0.5162
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3077 - acc: 0.8748 - precision_21: 0.8438 - auc_21: 0.8852 - recall_21: 0.5297 - val_loss: 0.3189 - val_acc: 0.8883 - val_precision_21: 0.7793 - val_auc_21: 0.8890 - val_recall_21: 0.6384
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2873 - acc: 0.8861 - precision_21: 0.8562 - auc_21: 0.8985 - recall_21: 0.5809 - val_loss: 0.2923 - val_acc: 0.8904 - val_precision_21: 0.7825 - val_auc_21: 0.9021 - val_recall_21: 0.6851
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2667 - acc: 0.8952 - precision_21: 0.8496 - auc_21: 0.9137 - recall_21: 0.6435 - val_loss: 0.2637 - val_acc: 0.8999 - val_precision_21: 0.7720 - val_auc_21: 0.9164 - val_recall_21: 0.7406
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2408 - acc: 0.9058 - precision_21: 0.8619 - auc_21: 0.9315 - recall_21: 0.6900 - val_loss: 0.2185 - val_acc: 0.9220 - val_precision_21: 0.8678 - val_auc_21: 0.9452 - val_recall_21: 0.7600
Epoch 9/100
254/254 [==============================] - 56s 219ms/step - loss: 0.2168 - acc: 0.9135 - precision_21: 0.8649 - auc_21: 0.9474 - recall_21: 0.7320 - val_loss: 0.2343 - val_acc: 0.9119 - val_precision_21: 0.9600 - val_auc_21: 0.9488 - val_recall_21: 0.6038
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2036 - acc: 0.9178 - precision_21: 0.8725 - auc_21: 0.9536 - recall_21: 0.7445 - val_loss: 0.1806 - val_acc: 0.9319 - val_precision_21: 0.8522 - val_auc_21: 0.9635 - val_recall_21: 0.8243
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1985 - acc: 0.9191 - precision_21: 0.8787 - auc_21: 0.9572 - recall_21: 0.7477 - val_loss: 0.1853 - val_acc: 0.9291 - val_precision_21: 0.8577 - val_auc_21: 0.9628 - val_recall_21: 0.7966
Epoch 12/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1894 - acc: 0.9226 - precision_21: 0.8855 - auc_21: 0.9610 - recall_21: 0.7571 - val_loss: 0.2183 - val_acc: 0.9091 - val_precision_21: 0.7333 - val_auc_21: 0.9672 - val_recall_21: 0.8945
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1760 - acc: 0.9271 - precision_21: 0.8858 - auc_21: 0.9674 - recall_21: 0.7816 - val_loss: 0.1659 - val_acc: 0.9376 - val_precision_21: 0.8443 - val_auc_21: 0.9770 - val_recall_21: 0.8696
Epoch 14/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1742 - acc: 0.9267 - precision_21: 0.8834 - auc_21: 0.9682 - recall_21: 0.7825 - val_loss: 0.1575 - val_acc: 0.9394 - val_precision_21: 0.8666 - val_auc_21: 0.9728 - val_recall_21: 0.8347
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1687 - acc: 0.9289 - precision_21: 0.8891 - auc_21: 0.9704 - recall_21: 0.7859 - val_loss: 0.1768 - val_acc: 0.9286 - val_precision_21: 0.8144 - val_auc_21: 0.9740 - val_recall_21: 0.8795
Epoch 16/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1634 - acc: 0.9315 - precision_21: 0.8913 - auc_21: 0.9722 - recall_21: 0.7984 - val_loss: 0.1404 - val_acc: 0.9459 - val_precision_21: 0.9128 - val_auc_21: 0.9770 - val_recall_21: 0.8035
Epoch 17/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1680 - acc: 0.9299 - precision_21: 0.8915 - auc_21: 0.9701 - recall_21: 0.7903 - val_loss: 0.1575 - val_acc: 0.9393 - val_precision_21: 0.8834 - val_auc_21: 0.9758 - val_recall_21: 0.8339
Epoch 18/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1693 - acc: 0.9293 - precision_21: 0.8877 - auc_21: 0.9701 - recall_21: 0.7914 - val_loss: 0.1593 - val_acc: 0.9407 - val_precision_21: 0.8567 - val_auc_21: 0.9751 - val_recall_21: 0.8566
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1613 - acc: 0.9316 - precision_21: 0.8869 - auc_21: 0.9731 - recall_21: 0.8038 - val_loss: 0.1419 - val_acc: 0.9451 - val_precision_21: 0.9191 - val_auc_21: 0.9790 - val_recall_21: 0.8232
Epoch 20/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1613 - acc: 0.9314 - precision_21: 0.8939 - auc_21: 0.9732 - recall_21: 0.7964 - val_loss: 0.1449 - val_acc: 0.9405 - val_precision_21: 0.8172 - val_auc_21: 0.9785 - val_recall_21: 0.8880
Epoch 21/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1590 - acc: 0.9328 - precision_21: 0.8903 - auc_21: 0.9736 - recall_21: 0.8077 - val_loss: 0.1529 - val_acc: 0.9412 - val_precision_21: 0.8727 - val_auc_21: 0.9806 - val_recall_21: 0.8746
Epoch 22/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1571 - acc: 0.9338 - precision_21: 0.8942 - auc_21: 0.9742 - recall_21: 0.8084 - val_loss: 0.1557 - val_acc: 0.9409 - val_precision_21: 0.8578 - val_auc_21: 0.9763 - val_recall_21: 0.8503
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1535 - acc: 0.9356 - precision_21: 0.8988 - auc_21: 0.9748 - recall_21: 0.8115 - val_loss: 0.1747 - val_acc: 0.9266 - val_precision_21: 0.7909 - val_auc_21: 0.9739 - val_recall_21: 0.8876
Epoch 24/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1532 - acc: 0.9348 - precision_21: 0.8977 - auc_21: 0.9760 - recall_21: 0.8093 - val_loss: 0.1679 - val_acc: 0.9320 - val_precision_21: 0.8084 - val_auc_21: 0.9785 - val_recall_21: 0.8981
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1571 - acc: 0.9338 - precision_21: 0.8935 - auc_21: 0.9738 - recall_21: 0.8095 - val_loss: 0.1719 - val_acc: 0.9379 - val_precision_21: 0.8198 - val_auc_21: 0.9798 - val_recall_21: 0.8983
Epoch 26/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1498 - acc: 0.9358 - precision_21: 0.8983 - auc_21: 0.9771 - recall_21: 0.8130 - val_loss: 0.1406 - val_acc: 0.9443 - val_precision_21: 0.8895 - val_auc_21: 0.9776 - val_recall_21: 0.8399
Epoch 27/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1558 - acc: 0.9346 - precision_21: 0.8974 - auc_21: 0.9743 - recall_21: 0.8085 - val_loss: 0.1343 - val_acc: 0.9476 - val_precision_21: 0.8855 - val_auc_21: 0.9820 - val_recall_21: 0.8657
Epoch 28/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1501 - acc: 0.9360 - precision_21: 0.8984 - auc_21: 0.9768 - recall_21: 0.8149 - val_loss: 0.1555 - val_acc: 0.9445 - val_precision_21: 0.8740 - val_auc_21: 0.9772 - val_recall_21: 0.8635
Epoch 29/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1514 - acc: 0.9359 - precision_21: 0.9013 - auc_21: 0.9761 - recall_21: 0.8117 - val_loss: 0.1777 - val_acc: 0.9267 - val_precision_21: 0.8089 - val_auc_21: 0.9737 - val_recall_21: 0.8842
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1482 - acc: 0.9365 - precision_21: 0.9021 - auc_21: 0.9773 - recall_21: 0.8148 - val_loss: 0.1298 - val_acc: 0.9492 - val_precision_21: 0.9108 - val_auc_21: 0.9827 - val_recall_21: 0.8425
Epoch 31/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1472 - acc: 0.9374 - precision_21: 0.9024 - auc_21: 0.9776 - recall_21: 0.8181 - val_loss: 0.1624 - val_acc: 0.9347 - val_precision_21: 0.8054 - val_auc_21: 0.9798 - val_recall_21: 0.9081
Epoch 32/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1521 - acc: 0.9357 - precision_21: 0.8974 - auc_21: 0.9756 - recall_21: 0.8161 - val_loss: 0.1449 - val_acc: 0.9424 - val_precision_21: 0.8538 - val_auc_21: 0.9811 - val_recall_21: 0.8815
Epoch 33/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1561 - acc: 0.9344 - precision_21: 0.8971 - auc_21: 0.9744 - recall_21: 0.8072 - val_loss: 0.1338 - val_acc: 0.9506 - val_precision_21: 0.9140 - val_auc_21: 0.9819 - val_recall_21: 0.8456
Epoch 34/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1482 - acc: 0.9373 - precision_21: 0.9005 - auc_21: 0.9770 - recall_21: 0.8188 - val_loss: 0.1327 - val_acc: 0.9505 - val_precision_21: 0.8956 - val_auc_21: 0.9807 - val_recall_21: 0.8458
Epoch 35/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1411 - acc: 0.9390 - precision_21: 0.9030 - auc_21: 0.9798 - recall_21: 0.8253 - val_loss: 0.1466 - val_acc: 0.9388 - val_precision_21: 0.8440 - val_auc_21: 0.9820 - val_recall_21: 0.8933
Epoch 36/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1461 - acc: 0.9375 - precision_21: 0.9010 - auc_21: 0.9779 - recall_21: 0.8188 - val_loss: 0.1310 - val_acc: 0.9472 - val_precision_21: 0.8868 - val_auc_21: 0.9815 - val_recall_21: 0.8618
Epoch 37/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1443 - acc: 0.9381 - precision_21: 0.9024 - auc_21: 0.9785 - recall_21: 0.8216 - val_loss: 0.1736 - val_acc: 0.9386 - val_precision_21: 0.9214 - val_auc_21: 0.9713 - val_recall_21: 0.7824
Epoch 38/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1418 - acc: 0.9393 - precision_21: 0.9038 - auc_21: 0.9792 - recall_21: 0.8272 - val_loss: 0.1363 - val_acc: 0.9473 - val_precision_21: 0.8888 - val_auc_21: 0.9786 - val_recall_21: 0.8558
Epoch 39/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1415 - acc: 0.9390 - precision_21: 0.9047 - auc_21: 0.9794 - recall_21: 0.8234 - val_loss: 0.1457 - val_acc: 0.9411 - val_precision_21: 0.8906 - val_auc_21: 0.9782 - val_recall_21: 0.8335
Epoch 40/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1397 - acc: 0.9398 - precision_21: 0.9057 - auc_21: 0.9803 - recall_21: 0.8283 - val_loss: 0.1269 - val_acc: 0.9516 - val_precision_21: 0.9214 - val_auc_21: 0.9810 - val_recall_21: 0.8271
Epoch 41/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1428 - acc: 0.9390 - precision_21: 0.9003 - auc_21: 0.9788 - recall_21: 0.8291 - val_loss: 0.1593 - val_acc: 0.9327 - val_precision_21: 0.8229 - val_auc_21: 0.9779 - val_recall_21: 0.8829
Epoch 42/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1424 - acc: 0.9384 - precision_21: 0.9046 - auc_21: 0.9795 - recall_21: 0.8224 - val_loss: 0.1392 - val_acc: 0.9466 - val_precision_21: 0.8746 - val_auc_21: 0.9827 - val_recall_21: 0.8710
Epoch 43/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1435 - acc: 0.9388 - precision_21: 0.9031 - auc_21: 0.9791 - recall_21: 0.8264 - val_loss: 0.1380 - val_acc: 0.9472 - val_precision_21: 0.9025 - val_auc_21: 0.9796 - val_recall_21: 0.8490
Epoch 44/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1383 - acc: 0.9406 - precision_21: 0.9073 - auc_21: 0.9805 - recall_21: 0.8291 - val_loss: 0.1234 - val_acc: 0.9520 - val_precision_21: 0.9280 - val_auc_21: 0.9825 - val_recall_21: 0.8388
Epoch 45/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1409 - acc: 0.9397 - precision_21: 0.9051 - auc_21: 0.9794 - recall_21: 0.8277 - val_loss: 0.1313 - val_acc: 0.9479 - val_precision_21: 0.9004 - val_auc_21: 0.9814 - val_recall_21: 0.8511
Epoch 46/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1374 - acc: 0.9408 - precision_21: 0.9065 - auc_21: 0.9806 - recall_21: 0.8310 - val_loss: 0.1307 - val_acc: 0.9471 - val_precision_21: 0.8651 - val_auc_21: 0.9825 - val_recall_21: 0.8867
Epoch 47/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1385 - acc: 0.9403 - precision_21: 0.9076 - auc_21: 0.9802 - recall_21: 0.8273 - val_loss: 0.1189 - val_acc: 0.9532 - val_precision_21: 0.9090 - val_auc_21: 0.9844 - val_recall_21: 0.8686
Epoch 48/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1394 - acc: 0.9406 - precision_21: 0.9051 - auc_21: 0.9798 - recall_21: 0.8329 - val_loss: 0.1311 - val_acc: 0.9479 - val_precision_21: 0.9535 - val_auc_21: 0.9826 - val_recall_21: 0.7902
Epoch 49/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1345 - acc: 0.9423 - precision_21: 0.9128 - auc_21: 0.9813 - recall_21: 0.8314 - val_loss: 0.1560 - val_acc: 0.9363 - val_precision_21: 0.8150 - val_auc_21: 0.9815 - val_recall_21: 0.9058
Epoch 50/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1369 - acc: 0.9402 - precision_21: 0.9052 - auc_21: 0.9810 - recall_21: 0.8289 - val_loss: 0.1409 - val_acc: 0.9421 - val_precision_21: 0.8489 - val_auc_21: 0.9805 - val_recall_21: 0.8860
Epoch 51/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1471 - acc: 0.9378 - precision_21: 0.9044 - auc_21: 0.9778 - recall_21: 0.8187 - val_loss: 0.1454 - val_acc: 0.9417 - val_precision_21: 0.8545 - val_auc_21: 0.9796 - val_recall_21: 0.8709
Epoch 52/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1386 - acc: 0.9400 - precision_21: 0.8992 - auc_21: 0.9803 - recall_21: 0.8362 - val_loss: 0.1563 - val_acc: 0.9479 - val_precision_21: 0.8786 - val_auc_21: 0.9807 - val_recall_21: 0.8772
Epoch 53/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1357 - acc: 0.9412 - precision_21: 0.9090 - auc_21: 0.9811 - recall_21: 0.8302 - val_loss: 0.1386 - val_acc: 0.9443 - val_precision_21: 0.8558 - val_auc_21: 0.9804 - val_recall_21: 0.8852
Epoch 54/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1365 - acc: 0.9415 - precision_21: 0.9068 - auc_21: 0.9809 - recall_21: 0.8359 - val_loss: 0.1436 - val_acc: 0.9416 - val_precision_21: 0.8475 - val_auc_21: 0.9812 - val_recall_21: 0.8860
Epoch 55/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1351 - acc: 0.9412 - precision_21: 0.9074 - auc_21: 0.9815 - recall_21: 0.8316 - val_loss: 0.1133 - val_acc: 0.9554 - val_precision_21: 0.8990 - val_auc_21: 0.9859 - val_recall_21: 0.8861
Epoch 56/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1300 - acc: 0.9428 - precision_21: 0.9130 - auc_21: 0.9828 - recall_21: 0.8336 - val_loss: 0.1349 - val_acc: 0.9442 - val_precision_21: 0.8589 - val_auc_21: 0.9823 - val_recall_21: 0.8893
Epoch 57/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1335 - acc: 0.9422 - precision_21: 0.9072 - auc_21: 0.9817 - recall_21: 0.8378 - val_loss: 0.1347 - val_acc: 0.9459 - val_precision_21: 0.8705 - val_auc_21: 0.9814 - val_recall_21: 0.8717
Epoch 58/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1309 - acc: 0.9431 - precision_21: 0.9092 - auc_21: 0.9827 - recall_21: 0.8412 - val_loss: 0.1360 - val_acc: 0.9498 - val_precision_21: 0.9342 - val_auc_21: 0.9789 - val_recall_21: 0.8119
Epoch 59/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1368 - acc: 0.9410 - precision_21: 0.9094 - auc_21: 0.9808 - recall_21: 0.8282 - val_loss: 0.2174 - val_acc: 0.9164 - val_precision_21: 0.7683 - val_auc_21: 0.9715 - val_recall_21: 0.8956
Epoch 60/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1341 - acc: 0.9419 - precision_21: 0.9072 - auc_21: 0.9817 - recall_21: 0.8362 - val_loss: 0.1191 - val_acc: 0.9518 - val_precision_21: 0.8952 - val_auc_21: 0.9849 - val_recall_21: 0.8757
Epoch 61/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1320 - acc: 0.9428 - precision_21: 0.9093 - auc_21: 0.9823 - recall_21: 0.8387 - val_loss: 0.1294 - val_acc: 0.9505 - val_precision_21: 0.8877 - val_auc_21: 0.9821 - val_recall_21: 0.8783
Epoch 62/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1381 - acc: 0.9414 - precision_21: 0.9087 - auc_21: 0.9804 - recall_21: 0.8319 - val_loss: 0.1245 - val_acc: 0.9474 - val_precision_21: 0.8752 - val_auc_21: 0.9841 - val_recall_21: 0.8728
Epoch 63/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1345 - acc: 0.9417 - precision_21: 0.9072 - auc_21: 0.9813 - recall_21: 0.8341 - val_loss: 0.1189 - val_acc: 0.9522 - val_precision_21: 0.8999 - val_auc_21: 0.9846 - val_recall_21: 0.8684
Epoch 64/100
254/254 [==============================] - 56s 219ms/step - loss: 0.1326 - acc: 0.9429 - precision_21: 0.9075 - auc_21: 0.9822 - recall_21: 0.8406 - val_loss: 0.1355 - val_acc: 0.9483 - val_precision_21: 0.9607 - val_auc_21: 0.9823 - val_recall_21: 0.7977
Epoch 65/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1370 - acc: 0.9418 - precision_21: 0.9124 - auc_21: 0.9801 - recall_21: 0.8295 - val_loss: 0.1319 - val_acc: 0.9498 - val_precision_21: 0.8979 - val_auc_21: 0.9779 - val_recall_21: 0.8441
Epoch 66/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1323 - acc: 0.9430 - precision_21: 0.9099 - auc_21: 0.9816 - recall_21: 0.8395 - val_loss: 0.1214 - val_acc: 0.9505 - val_precision_21: 0.9219 - val_auc_21: 0.9855 - val_recall_21: 0.8496
Epoch 67/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1324 - acc: 0.9420 - precision_21: 0.9057 - auc_21: 0.9823 - recall_21: 0.8383 - val_loss: 0.1502 - val_acc: 0.9430 - val_precision_21: 0.9027 - val_auc_21: 0.9732 - val_recall_21: 0.8098
Epoch 68/100
254/254 [==============================] - 55s 218ms/step - loss: 0.1367 - acc: 0.9418 - precision_21: 0.9079 - auc_21: 0.9805 - recall_21: 0.8341 - val_loss: 0.1229 - val_acc: 0.9501 - val_precision_21: 0.8848 - val_auc_21: 0.9838 - val_recall_21: 0.8751
Epoch 69/100
254/254 [==============================] - ETA: 0s - loss: 0.1286 - acc: 0.9436 - precision_21: 0.9128 - auc_21: 0.9834 - recall_21: 0.8394Restoring model weights from the end of the best epoch.
254/254 [==============================] - 55s 218ms/step - loss: 0.1286 - acc: 0.9436 - precision_21: 0.9128 - auc_21: 0.9834 - recall_21: 0.8394 - val_loss: 0.1531 - val_acc: 0.9378 - val_precision_21: 0.8231 - val_auc_21: 0.9805 - val_recall_21: 0.8986
Epoch 00069: early stopping
36/36 [==============================] - 3s 75ms/step - loss: 0.1370 - acc: 0.9473 - precision_21: 0.8873 - auc_21: 0.9815 - recall_21: 0.8757
---------------TEST METRICS----------------------
jaccard_index 0.78562437348485
test_sensitivity 0.8782135280282972
test_specifitivity 0.9651565200259061
test_accuracy 0.945715938054078
test_precision 0.8789179533268864
test_jaccard_score 0.78562437348485
test_dicecoef 0.8785655994772871
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-050520.h5
[0. 0. 0. 0. 0.] [0.94571594 0.78562437 0.87891795 0.87821353 0.96515652]

-------------------------
Rep: 1
-------------------------

2021-09-25 05:05:20.750998: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:05:20.751131: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_45"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_23 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_418 (Conv2D)             (None, 256, 256, 32) 320         input_23[0][0]
__________________________________________________________________________________________________
conv2d_419 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_418[0][0]
__________________________________________________________________________________________________
max_pooling2d_88 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_420 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_88[0][0]
__________________________________________________________________________________________________
conv2d_421 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_420[0][0]
__________________________________________________________________________________________________
max_pooling2d_89 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_422 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_89[0][0]
__________________________________________________________________________________________________
conv2d_423 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_422[0][0]
__________________________________________________________________________________________________
max_pooling2d_90 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_424 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_90[0][0]
__________________________________________________________________________________________________
conv2d_425 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_424[0][0]
__________________________________________________________________________________________________
max_pooling2d_91 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_426 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_91[0][0]
__________________________________________________________________________________________________
conv2d_427 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_426[0][0]
__________________________________________________________________________________________________
up_sampling2d_88 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_427[0][0]
__________________________________________________________________________________________________
concatenate_88 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_88[0][0]
                                                                 conv2d_425[0][0]
__________________________________________________________________________________________________
conv2d_428 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_88[0][0]
__________________________________________________________________________________________________
conv2d_429 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_428[0][0]
__________________________________________________________________________________________________
up_sampling2d_89 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_429[0][0]
__________________________________________________________________________________________________
concatenate_89 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_89[0][0]
                                                                 conv2d_423[0][0]
__________________________________________________________________________________________________
conv2d_430 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_89[0][0]
__________________________________________________________________________________________________
conv2d_431 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_430[0][0]
__________________________________________________________________________________________________
up_sampling2d_90 (UpSampling2D) (None, 128, 128, 128 0           conv2d_431[0][0]
__________________________________________________________________________________________________
concatenate_90 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_90[0][0]
                                                                 conv2d_421[0][0]
__________________________________________________________________________________________________
conv2d_432 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_90[0][0]
__________________________________________________________________________________________________
conv2d_433 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_432[0][0]
__________________________________________________________________________________________________
up_sampling2d_91 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_433[0][0]
__________________________________________________________________________________________________
concatenate_91 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_91[0][0]
                                                                 conv2d_419[0][0]
__________________________________________________________________________________________________
conv2d_434 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_91[0][0]
__________________________________________________________________________________________________
conv2d_435 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_434[0][0]
__________________________________________________________________________________________________
conv2d_436 (Conv2D)             (None, 256, 256, 1)  33          conv2d_435[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6988 - acc: 0.1748 - precision_22: 0.1705 - auc_22: 0.5494 - recall_22: 0.99912021-09-25 05:05:59.379492: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:05:59.380144: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 05:05:59.899101: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 05:05:59.911883: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59
2021-09-25 05:05:59.915426: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.trace.json.gz
2021-09-25 05:05:59.950378: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59
2021-09-25 05:05:59.959171: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.memory_profile.json.gz
2021-09-25 05:05:59.976089: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59Dumped tool data for xplane.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-050520/train/plugins/profile/2021_09_25_05_05_59/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:15 - loss: 0.6916 - acc: 0.4043 - precision_22: 0.1704 - auc_22: 0.4187 - recall_22: 0.3130WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1993s vs `on_train_batch_end` time: 0.3988s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4697 - acc: 0.8063 - precision_22: 0.7005 - auc_22: 0.7959 - recall_22: 0.1916 - val_loss: 0.3429 - val_acc: 0.8637 - val_precision_22: 0.8986 - val_auc_22: 0.8648 - val_recall_22: 0.4037
Epoch 2/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3461 - acc: 0.8505 - precision_22: 0.8030 - auc_22: 0.8643 - recall_22: 0.4189 - val_loss: 0.3663 - val_acc: 0.8490 - val_precision_22: 0.9187 - val_auc_22: 0.8539 - val_recall_22: 0.3157
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3267 - acc: 0.8603 - precision_22: 0.8399 - auc_22: 0.8745 - recall_22: 0.4454 - val_loss: 0.3338 - val_acc: 0.8700 - val_precision_22: 0.7302 - val_auc_22: 0.8808 - val_recall_22: 0.6224
Epoch 4/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2973 - acc: 0.8804 - precision_22: 0.8439 - auc_22: 0.8922 - recall_22: 0.5623 - val_loss: 0.2687 - val_acc: 0.9027 - val_precision_22: 0.8422 - val_auc_22: 0.9097 - val_recall_22: 0.6639
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2951 - acc: 0.8818 - precision_22: 0.8404 - auc_22: 0.8959 - recall_22: 0.5751 - val_loss: 0.2974 - val_acc: 0.8883 - val_precision_22: 0.7699 - val_auc_22: 0.8945 - val_recall_22: 0.6525
Epoch 6/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2830 - acc: 0.8890 - precision_22: 0.8421 - auc_22: 0.9028 - recall_22: 0.6143 - val_loss: 0.2780 - val_acc: 0.8947 - val_precision_22: 0.7843 - val_auc_22: 0.9173 - val_recall_22: 0.7096
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2536 - acc: 0.9014 - precision_22: 0.8558 - auc_22: 0.9240 - recall_22: 0.6722 - val_loss: 0.2163 - val_acc: 0.9193 - val_precision_22: 0.8407 - val_auc_22: 0.9410 - val_recall_22: 0.7581
Epoch 8/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2315 - acc: 0.9092 - precision_22: 0.8585 - auc_22: 0.9364 - recall_22: 0.7136 - val_loss: 0.2478 - val_acc: 0.9162 - val_precision_22: 0.8613 - val_auc_22: 0.9170 - val_recall_22: 0.7367
Epoch 9/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2143 - acc: 0.9146 - precision_22: 0.8652 - auc_22: 0.9478 - recall_22: 0.7375 - val_loss: 0.2390 - val_acc: 0.9190 - val_precision_22: 0.9303 - val_auc_22: 0.9473 - val_recall_22: 0.6620
Epoch 10/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2025 - acc: 0.9177 - precision_22: 0.8714 - auc_22: 0.9548 - recall_22: 0.7453 - val_loss: 0.1891 - val_acc: 0.9290 - val_precision_22: 0.8654 - val_auc_22: 0.9566 - val_recall_22: 0.7905
Epoch 11/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1895 - acc: 0.9218 - precision_22: 0.8744 - auc_22: 0.9612 - recall_22: 0.7676 - val_loss: 0.1719 - val_acc: 0.9337 - val_precision_22: 0.8968 - val_auc_22: 0.9672 - val_recall_22: 0.7756
Epoch 12/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1866 - acc: 0.9226 - precision_22: 0.8817 - auc_22: 0.9625 - recall_22: 0.7619 - val_loss: 0.1682 - val_acc: 0.9373 - val_precision_22: 0.8414 - val_auc_22: 0.9720 - val_recall_22: 0.8660
Epoch 13/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1763 - acc: 0.9276 - precision_22: 0.8844 - auc_22: 0.9667 - recall_22: 0.7864 - val_loss: 0.1691 - val_acc: 0.9334 - val_precision_22: 0.8143 - val_auc_22: 0.9780 - val_recall_22: 0.8932
Epoch 14/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1735 - acc: 0.9276 - precision_22: 0.8834 - auc_22: 0.9674 - recall_22: 0.7876 - val_loss: 0.1714 - val_acc: 0.9324 - val_precision_22: 0.8906 - val_auc_22: 0.9660 - val_recall_22: 0.7663
Epoch 15/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1699 - acc: 0.9295 - precision_22: 0.8881 - auc_22: 0.9699 - recall_22: 0.7904 - val_loss: 0.1787 - val_acc: 0.9299 - val_precision_22: 0.8153 - val_auc_22: 0.9746 - val_recall_22: 0.8856
Epoch 16/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1673 - acc: 0.9293 - precision_22: 0.8846 - auc_22: 0.9708 - recall_22: 0.7956 - val_loss: 0.1394 - val_acc: 0.9451 - val_precision_22: 0.9329 - val_auc_22: 0.9776 - val_recall_22: 0.7788
Epoch 17/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1634 - acc: 0.9306 - precision_22: 0.8937 - auc_22: 0.9723 - recall_22: 0.7916 - val_loss: 0.1589 - val_acc: 0.9367 - val_precision_22: 0.8623 - val_auc_22: 0.9766 - val_recall_22: 0.8473
Epoch 18/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1584 - acc: 0.9332 - precision_22: 0.8936 - auc_22: 0.9740 - recall_22: 0.8056 - val_loss: 0.1386 - val_acc: 0.9492 - val_precision_22: 0.9235 - val_auc_22: 0.9769 - val_recall_22: 0.8223
Epoch 19/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1605 - acc: 0.9321 - precision_22: 0.8916 - auc_22: 0.9733 - recall_22: 0.8012 - val_loss: 0.1626 - val_acc: 0.9378 - val_precision_22: 0.8784 - val_auc_22: 0.9742 - val_recall_22: 0.8331
Epoch 20/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1574 - acc: 0.9325 - precision_22: 0.8922 - auc_22: 0.9749 - recall_22: 0.8042 - val_loss: 0.1312 - val_acc: 0.9480 - val_precision_22: 0.8563 - val_auc_22: 0.9802 - val_recall_22: 0.8753
Epoch 21/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1546 - acc: 0.9351 - precision_22: 0.8932 - auc_22: 0.9754 - recall_22: 0.8164 - val_loss: 0.1495 - val_acc: 0.9424 - val_precision_22: 0.8745 - val_auc_22: 0.9813 - val_recall_22: 0.8782
Epoch 22/100
254/254 [==============================] - 56s 221ms/step - loss: 0.1473 - acc: 0.9371 - precision_22: 0.9036 - auc_22: 0.9772 - recall_22: 0.8148 - val_loss: 0.1578 - val_acc: 0.9404 - val_precision_22: 0.8441 - val_auc_22: 0.9772 - val_recall_22: 0.8669
Epoch 23/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1509 - acc: 0.9360 - precision_22: 0.8983 - auc_22: 0.9759 - recall_22: 0.8149 - val_loss: 0.1740 - val_acc: 0.9272 - val_precision_22: 0.7899 - val_auc_22: 0.9765 - val_recall_22: 0.8931
Epoch 24/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1485 - acc: 0.9364 - precision_22: 0.8999 - auc_22: 0.9774 - recall_22: 0.8154 - val_loss: 0.1453 - val_acc: 0.9406 - val_precision_22: 0.8443 - val_auc_22: 0.9803 - val_recall_22: 0.8887
Epoch 25/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1512 - acc: 0.9368 - precision_22: 0.8978 - auc_22: 0.9767 - recall_22: 0.8200 - val_loss: 0.1650 - val_acc: 0.9432 - val_precision_22: 0.8338 - val_auc_22: 0.9831 - val_recall_22: 0.9071
Epoch 26/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1419 - acc: 0.9386 - precision_22: 0.9034 - auc_22: 0.9793 - recall_22: 0.8220 - val_loss: 0.1383 - val_acc: 0.9447 - val_precision_22: 0.9071 - val_auc_22: 0.9788 - val_recall_22: 0.8214
Epoch 27/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1460 - acc: 0.9384 - precision_22: 0.9078 - auc_22: 0.9778 - recall_22: 0.8170 - val_loss: 0.1454 - val_acc: 0.9502 - val_precision_22: 0.9323 - val_auc_22: 0.9822 - val_recall_22: 0.8258
Epoch 28/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1456 - acc: 0.9375 - precision_22: 0.9033 - auc_22: 0.9788 - recall_22: 0.8171 - val_loss: 0.2016 - val_acc: 0.9185 - val_precision_22: 0.7512 - val_auc_22: 0.9773 - val_recall_22: 0.9223
Epoch 29/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1449 - acc: 0.9377 - precision_22: 0.9002 - auc_22: 0.9785 - recall_22: 0.8231 - val_loss: 0.1850 - val_acc: 0.9169 - val_precision_22: 0.7589 - val_auc_22: 0.9776 - val_recall_22: 0.9260
Epoch 30/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1437 - acc: 0.9382 - precision_22: 0.8989 - auc_22: 0.9787 - recall_22: 0.8277 - val_loss: 0.1322 - val_acc: 0.9459 - val_precision_22: 0.8897 - val_auc_22: 0.9826 - val_recall_22: 0.8498
Epoch 31/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1393 - acc: 0.9404 - precision_22: 0.9085 - auc_22: 0.9800 - recall_22: 0.8269 - val_loss: 0.1554 - val_acc: 0.9388 - val_precision_22: 0.8241 - val_auc_22: 0.9803 - val_recall_22: 0.9005
Epoch 32/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1443 - acc: 0.9381 - precision_22: 0.9005 - auc_22: 0.9785 - recall_22: 0.8251 - val_loss: 0.1454 - val_acc: 0.9415 - val_precision_22: 0.8342 - val_auc_22: 0.9827 - val_recall_22: 0.9069
Epoch 33/100
254/254 [==============================] - 56s 220ms/step - loss: 0.1432 - acc: 0.9391 - precision_22: 0.9011 - auc_22: 0.9788 - recall_22: 0.8280 - val_loss: 0.1402 - val_acc: 0.9416 - val_precision_22: 0.8414 - val_auc_22: 0.9813 - val_recall_22: 0.8909
Epoch 34/100
254/254 [==============================] - ETA: 0s - loss: 0.1436 - acc: 0.9392 - precision_22: 0.9030 - auc_22: 0.9789 - recall_22: 0.8262Restoring model weights from the end of the best epoch.
254/254 [==============================] - 56s 220ms/step - loss: 0.1436 - acc: 0.9392 - precision_22: 0.9030 - auc_22: 0.9789 - recall_22: 0.8262 - val_loss: 0.1421 - val_acc: 0.9458 - val_precision_22: 0.8543 - val_auc_22: 0.9817 - val_recall_22: 0.8725
Epoch 00034: early stopping
36/36 [==============================] - 3s 76ms/step - loss: 0.1465 - acc: 0.9435 - precision_22: 0.8765 - auc_22: 0.9779 - recall_22: 0.8698
---------------TEST METRICS----------------------
jaccard_index 0.7573034781752033
test_sensitivity 0.8742567599124289
test_specifitivity 0.9604772935099035
test_accuracy 0.9411982543079566
test_precision 0.8643263664052484
test_jaccard_score 0.7573034781752033
test_dicecoef 0.869263203078232
isic_eval_score 0.9964539007092199
---------------TEST METRICS----------------------
Model weights saved: src/models/UNet_model_256x256_25092021-053820.h5
[0.94571594 0.78562437 0.87891795 0.87821353 0.96515652] [0.94119825 0.75730348 0.86432637 0.87425676 0.96047729]

-------------------------
Rep: 2
-------------------------

2021-09-25 05:38:21.741793: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:38:21.742034: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Training set shape:  (2029, 256, 256, 1)
Validation set shape (383, 256, 256, 1)
Test set shape (282, 256, 256, 1)
Data loaded
Applied gaussian blur on all input images
Applied normalization
Data augumentation on
Prep done
Training samples: 2029, channel mean: 0.5810265875815174,
Validation samples: 383, channel mean: 0.5863744105403145
Model built.
Model: "functional_47"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_24 (InputLayer)           [(None, 256, 256, 1) 0
__________________________________________________________________________________________________
conv2d_437 (Conv2D)             (None, 256, 256, 32) 320         input_24[0][0]
__________________________________________________________________________________________________
conv2d_438 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_437[0][0]
__________________________________________________________________________________________________
max_pooling2d_92 (MaxPooling2D) (None, 128, 128, 32) 0           conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_439 (Conv2D)             (None, 128, 128, 64) 18496       max_pooling2d_92[0][0]
__________________________________________________________________________________________________
conv2d_440 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_439[0][0]
__________________________________________________________________________________________________
max_pooling2d_93 (MaxPooling2D) (None, 64, 64, 64)   0           conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_441 (Conv2D)             (None, 64, 64, 128)  73856       max_pooling2d_93[0][0]
__________________________________________________________________________________________________
conv2d_442 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_441[0][0]
__________________________________________________________________________________________________
max_pooling2d_94 (MaxPooling2D) (None, 32, 32, 128)  0           conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_443 (Conv2D)             (None, 32, 32, 256)  295168      max_pooling2d_94[0][0]
__________________________________________________________________________________________________
conv2d_444 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_443[0][0]
__________________________________________________________________________________________________
max_pooling2d_95 (MaxPooling2D) (None, 16, 16, 256)  0           conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_445 (Conv2D)             (None, 16, 16, 512)  1180160     max_pooling2d_95[0][0]
__________________________________________________________________________________________________
conv2d_446 (Conv2D)             (None, 16, 16, 512)  2359808     conv2d_445[0][0]
__________________________________________________________________________________________________
up_sampling2d_92 (UpSampling2D) (None, 32, 32, 512)  0           conv2d_446[0][0]
__________________________________________________________________________________________________
concatenate_92 (Concatenate)    (None, 32, 32, 768)  0           up_sampling2d_92[0][0]
                                                                 conv2d_444[0][0]
__________________________________________________________________________________________________
conv2d_447 (Conv2D)             (None, 32, 32, 256)  1769728     concatenate_92[0][0]
__________________________________________________________________________________________________
conv2d_448 (Conv2D)             (None, 32, 32, 256)  590080      conv2d_447[0][0]
__________________________________________________________________________________________________
up_sampling2d_93 (UpSampling2D) (None, 64, 64, 256)  0           conv2d_448[0][0]
__________________________________________________________________________________________________
concatenate_93 (Concatenate)    (None, 64, 64, 384)  0           up_sampling2d_93[0][0]
                                                                 conv2d_442[0][0]
__________________________________________________________________________________________________
conv2d_449 (Conv2D)             (None, 64, 64, 128)  442496      concatenate_93[0][0]
__________________________________________________________________________________________________
conv2d_450 (Conv2D)             (None, 64, 64, 128)  147584      conv2d_449[0][0]
__________________________________________________________________________________________________
up_sampling2d_94 (UpSampling2D) (None, 128, 128, 128 0           conv2d_450[0][0]
__________________________________________________________________________________________________
concatenate_94 (Concatenate)    (None, 128, 128, 192 0           up_sampling2d_94[0][0]
                                                                 conv2d_440[0][0]
__________________________________________________________________________________________________
conv2d_451 (Conv2D)             (None, 128, 128, 64) 110656      concatenate_94[0][0]
__________________________________________________________________________________________________
conv2d_452 (Conv2D)             (None, 128, 128, 64) 36928       conv2d_451[0][0]
__________________________________________________________________________________________________
up_sampling2d_95 (UpSampling2D) (None, 256, 256, 64) 0           conv2d_452[0][0]
__________________________________________________________________________________________________
concatenate_95 (Concatenate)    (None, 256, 256, 96) 0           up_sampling2d_95[0][0]
                                                                 conv2d_438[0][0]
__________________________________________________________________________________________________
conv2d_453 (Conv2D)             (None, 256, 256, 32) 27680       concatenate_95[0][0]
__________________________________________________________________________________________________
conv2d_454 (Conv2D)             (None, 256, 256, 32) 9248        conv2d_453[0][0]
__________________________________________________________________________________________________
conv2d_455 (Conv2D)             (None, 256, 256, 1)  33          conv2d_454[0][0]
==================================================================================================
Total params: 7,846,081
Trainable params: 7,846,081
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/100
  1/254 [..............................] - ETA: 0s - loss: 0.6898 - acc: 0.8111 - precision_23: 0.1230 - auc_23: 0.6209 - recall_23: 0.02142021-09-25 05:39:00.169328: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2021-09-25 05:39:00.169941: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2021-09-25 05:39:00.678975: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events.
2021-09-25 05:39:00.689556: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00
2021-09-25 05:39:00.695352: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.trace.json.gz
2021-09-25 05:39:00.723380: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00
2021-09-25 05:39:00.731551: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.memory_profile.json.gz
2021-09-25 05:39:00.744915: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00Dumped tool data for xplane.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.xplane.pb
Dumped tool data for overview_page.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.overview_page.pb
Dumped tool data for input_pipeline.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to logs/fit/20210925-053821/train/plugins/profile/2021_09_25_05_39_00/xeon-09.kernel_stats.pb

  2/254 [..............................] - ETA: 1:12 - loss: 0.6784 - acc: 0.7226 - precision_23: 0.1230 - auc_23: 0.4400 - recall_23: 0.0067WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1959s vs `on_train_batch_end` time: 0.3807s). Check your callbacks.
254/254 [==============================] - 57s 224ms/step - loss: 0.4124 - acc: 0.8130 - precision_23: 0.7581 - auc_23: 0.8026 - recall_23: 0.2053 - val_loss: 0.3529 - val_acc: 0.8605 - val_precision_23: 0.7310 - val_auc_23: 0.8533 - val_recall_23: 0.5430
Epoch 2/100
254/254 [==============================] - 56s 221ms/step - loss: 0.3286 - acc: 0.8617 - precision_23: 0.8130 - auc_23: 0.8729 - recall_23: 0.4788 - val_loss: 0.3405 - val_acc: 0.8731 - val_precision_23: 0.7724 - val_auc_23: 0.8719 - val_recall_23: 0.5691
Epoch 3/100
254/254 [==============================] - 56s 220ms/step - loss: 0.3167 - acc: 0.8679 - precision_23: 0.8338 - auc_23: 0.8813 - recall_23: 0.4965 - val_loss: 0.3338 - val_acc: 0.8735 - val_precision_23: 0.7703 - val_auc_23: 0.8781 - val_recall_23: 0.5825
Epoch 4/100
254/254 [==============================] - 56s 221ms/step - loss: 0.2999 - acc: 0.8788 - precision_23: 0.8432 - auc_23: 0.8920 - recall_23: 0.5535 - val_loss: 0.2932 - val_acc: 0.8887 - val_precision_23: 0.8320 - val_auc_23: 0.9017 - val_recall_23: 0.5927
Epoch 5/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2997 - acc: 0.8802 - precision_23: 0.8481 - auc_23: 0.8930 - recall_23: 0.5578 - val_loss: 0.3253 - val_acc: 0.8741 - val_precision_23: 0.8909 - val_auc_23: 0.8772 - val_recall_23: 0.4424
Epoch 6/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2862 - acc: 0.8851 - precision_23: 0.8536 - auc_23: 0.9007 - recall_23: 0.5781 - val_loss: 0.3053 - val_acc: 0.8695 - val_precision_23: 0.7262 - val_auc_23: 0.9010 - val_recall_23: 0.6391
Epoch 7/100
254/254 [==============================] - 56s 220ms/step - loss: 0.2708 - acc: 0.8931 - precision_23: 0.8536 - auc_23: 0.9119 - recall_23: 0.6261 - val_loss: 0.2750 - val_acc: 0.8982 - val_precision_23: 0.8194 - val_auc_23: 0.9049 - val_recall_23: 0.6593
Epoch 8/100
243/254 [===========================>..] - ETA: 2s - loss: 0.2630 - acc: 0.8980 - precision_23: 0.8541 - auc_23: 0.9162 - recall_23: 0.6550client_loop: send disconnect: Connection reset by peer
(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$ ^C
(base)
ignac@DESKTOP-UR509JM MINGW64 ~
$
